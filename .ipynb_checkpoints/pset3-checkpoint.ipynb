{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem Set 3\n",
    "Designed by Sarah Adel Bargal, Ben Usman, and Kun He, with help from Kate Saenko and Brian Kulis.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "- building, training, and testing of neural networks in TensorFlow\n",
    "- using TensorFlow on the SCC cluster\n",
    "- extracting and visualizing neural network features\n",
    "\n",
    "Note: For programming, the use of any additional abstraction of TensorFlow (like `Keras` and `tf.contrib.learn`) is **not permitted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 0: Tutorials\n",
    "The contents of these tutorials are helpful for solving this assignment and upcoming assignments. Please be sure to go through them before proceeding.\n",
    "1. [MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/)\n",
    "2. [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/)\n",
    "3. [TensorFlow Mechanics 101](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/tf/)\n",
    "4. [Sharing Variables](https://www.tensorflow.org/programmers_guide/variable_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: MNIST Softmax Classifier Demo in TensorFlow\n",
    "\n",
    "TensorFlow is already installed on the SCC. Please review the instructions on [connecting to SCC](https://docs.google.com/document/d/1C4XrrTZIRfmJ6-LHmuJ4levTnunezujVrxCz8jJ4rec), and [running jobs on SCC](http://www.bu.edu/tech/support/research/system-usage/running-jobs/).\n",
    "\n",
    "Make sure you are capable of running this demo (using `qsub`) on the SCC cluster: [`mnist_softmax_scope.py`](https://github.com/kunhe/cs591s2/blob/master/mnist_softmax_scope.py). There is no required deliverable, but this exercise is helpful for running jobs on the SCC in the future.\n",
    "\n",
    "The demo is another implementation of `mnist_softmax.py` presented in the MNIST for ML Beginners tutorial, but using scopes. For the purposes of this assignment, the contents of the demo is also replicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-8-afc1dd0c252b>:52: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iteration 0\t accuracy: 0.407\n",
      "iteration 2000\t accuracy: 0.918\n",
      "iteration 4000\t accuracy: 0.921\n",
      "iteration 6000\t accuracy: 0.923\n",
      "iteration 8000\t accuracy: 0.923\n",
      "iteration 10000\t accuracy: 0.922\n",
      "iteration 12000\t accuracy: 0.923\n",
      "iteration 14000\t accuracy: 0.925\n",
      "iteration 16000\t accuracy: 0.924\n",
      "iteration 18000\t accuracy: 0.925\n",
      "iteration 20000\t accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def logistic_regression(x_):\n",
    "    # create the actual model\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=1e-4)}\n",
    "    with tf.variable_scope(\"weights\", **scope_args):\n",
    "        W = tf.get_variable('W', shape=[784, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        y_logits = tf.matmul(x_, W) + b\n",
    "    return y_logits\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "    # import data\n",
    "    mnist = input_data.read_data_sets('./datasets/mnist/', one_hot=True)\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, 784])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_logits)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(y_pred, tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # train\n",
    "        for iter_i in range(20001):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: mnist.test.images, y_: mnist.test.labels}\n",
    "                acc_value = sess.run(accuracy, feed_dict=tf_feed_dict)\n",
    "                print('iteration %d\\t accuracy: %.3f'%(iter_i, acc_value))\n",
    "                \n",
    "test_classification(logistic_regression, learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Building Neural Networks in TensorFlow\n",
    "\n",
    "(45 points)\n",
    "\n",
    "### Q2.1 MLP in TensorFlow\n",
    "\n",
    "Task: \n",
    "\n",
    "- Implement a multi-layer perceptron function **`mlp(x, hidden_sizes, activation_fn)`** in TensorFlow that has the following input and output: \n",
    "\n",
    " **Inputs**\n",
    " - `x`, an input tensor of the images in the current batch `[batch_size, 28x28]`\n",
    " - `hidden_sizes`, a list of the number of hidden units per layer. For example: `[5,2]` means  5 hidden units in the first layer, and 2 hidden units in the second (output) layer.\n",
    " (Note: for MNIST, we need `hidden_sizes[-1]==10` since it has 10 classes.)\n",
    " - `activation_fn`, the activation function to be applied\n",
    "\n",
    " **Output**\n",
    " - a tensor of shape `[batch_size, hidden_sizes[-1]]`. \n",
    "\n",
    "**Note: ** \n",
    "- Make sure the activation function is not applied to the final (output) layer.\n",
    "- It is recommended to use scopes and `tf.get_variable()` (as opposed to `tf.Variable()` which you may see elsewhere), as demonstrated in the sample code, and explained in the \"Sharing Variables\" tutorial in Part 0. Also see [here](http://stackoverflow.com/questions/37098546/difference-between-variable-and-get-variable-in-tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    layer = x\n",
    "    batch_size = 784\n",
    "\n",
    "    def getWeight(shape,ini_value):\n",
    "        weights = tf.get_variable('weights',shape,initializer=tf.random_normal_initializer(stddev=ini_value))\n",
    "        return weights\n",
    "\n",
    "    def getBias(shape,ini_value):\n",
    "        bias = tf.get_variable('bias',shape,initializer=tf.random_normal_initializer(mean=0.1,stddev=ini_value))\n",
    "        return bias\n",
    "\n",
    "    if len(hidden_sizes)>1:\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            with tf.variable_scope('layer_'+str(i+1)):\n",
    "                ini_value = 1.0/(hidden_sizes[i]**0.5)\n",
    "                weights =getWeight([batch_size,hidden_sizes[i]],ini_value)\n",
    "                bias = getBias(hidden_sizes[i],ini_value)\n",
    "                h = tf.matmul(layer,weights)+bias\n",
    "                layer = activation_fn(h)\n",
    "            batch_size = hidden_sizes[i]\n",
    "            \n",
    "    with tf.variable_scope('layer_'+str(i+2)):\n",
    "        ini_value = 1.0/(hidden_sizes[-1]**0.5)\n",
    "        weights =getWeight([batch_size,hidden_sizes[-1]],ini_value)\n",
    "        bias = getBias(hidden_sizes[-1],ini_value)\n",
    "        outlayer = tf.matmul(layer,weights)+bias\n",
    "        \n",
    "    return outlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code tests your  implementation of the **`mlp()`** function. It basically recreates the 2-layer MLP with ReLU activation that you implemented in Problem Set 2. It should give an accuracy of above 0.97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-8-afc1dd0c252b>:52: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iteration 0\t accuracy: 0.184\n",
      "iteration 2000\t accuracy: 0.951\n",
      "iteration 4000\t accuracy: 0.962\n",
      "iteration 6000\t accuracy: 0.968\n",
      "iteration 8000\t accuracy: 0.970\n",
      "iteration 10000\t accuracy: 0.970\n",
      "iteration 12000\t accuracy: 0.969\n",
      "iteration 14000\t accuracy: 0.972\n",
      "iteration 16000\t accuracy: 0.974\n",
      "iteration 18000\t accuracy: 0.973\n",
      "iteration 20000\t accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "test_classification(lambda x: mlp(x, [64, 10], activation_fn=tf.nn.relu), learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q2.2 Siamese Network\n",
    "A siamese network is a network having two identical subnetworks that share parameters. A siamese network gives a *similarity metric* between pairs of inputs, which assigns high values to similar pairs and low values to dissimilar pairs. In MNIST, we can construct similar pairs by taking images from the same digit class, and dissimilar pairs from images from different classes. \n",
    "\n",
    "\n",
    "A sample Siamese network is presented below [Koch et al., Siamese Neural Networks for One-shot Image Recognition, ICML 2015]. Note how the two subnetworks have identical parameters:\n",
    "\n",
    "<img src=\"siamese.png\" style=\"width:480px;\">\n",
    "\n",
    "\n",
    "Given a Siamese network, we can determine the similarity between inputs $x_1$ and $x_2$, by taking the sign of the cosine similarity $\\frac{\\langle r_1,r_2\\rangle}{\\|r_1\\|\\|r_2\\|}$, where $r_1$ and $r_2$ are the hidden representations produced by the network for $x_1$ and $x_2$, respectively. \n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Build a Siamese Network in TensorFlow that receives as input two MNIST images, and decides whether they belong to the same digit category. Each subnetwork is an MLP with 2 hidden layers with 64 units, followed by a \"distance layer\" with 32 units. They will be created using the **`mlp()`** function you implemented earlier, using input argument `hidden_sizes=[64,64,32]`. The network computes the cosine similarity, as defined above, in the final output layer.\n",
    "\n",
    "2. Train and test this model, and  report your test accuracy for this task.\n",
    "\n",
    "You will need to implement the following two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Inputs: arr1 and arr2 have shape [batch_size, hidden_sizes[-1]]\n",
    "# Output: return tensor of shape [batch_size, ], the cosine \n",
    "#         similarity between arr1 and arr2\n",
    "# \n",
    "# Hint: use tf.l2_normalize, tf.mul, tf.reduce_sum\n",
    "#################################################################\n",
    "def cosine_similarity(arr1, arr2):\n",
    "    normed_arr1 = tf.nn.l2_normalize(arr1, dim=1)\n",
    "    normed_arr2 = tf.nn.l2_normalize(arr2, dim=1)\n",
    "    cos_sim = c = tf.reduce_sum(tf.multiply(normed_arr1, normed_arr2), 1, keep_dims=False)\n",
    "    return cos_sim\n",
    "    \n",
    "    \n",
    "#################################################################\n",
    "# Inputs: mlp_args is a dictionary of arguments to the mlp() \n",
    "#         function. \n",
    "#         Example: mlp_args = {'hidden_sizes':[64, 64, 32]}\n",
    "#################################################################\n",
    "def build_model(mlp_args):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x1 = tf.placeholder(tf.float32, [None, 784])\n",
    "            x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            with tf.variable_scope(\"siamese\") as var_scope:\n",
    "                x_repr1 = mlp(x1, **mlp_args)\n",
    "                var_scope.reuse_variables()\n",
    "                x_repr2 = mlp(x2, **mlp_args)\n",
    "                logits = cosine_similarity(x_repr1, x_repr2)\n",
    "            \n",
    "            cross_entropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "            loss = tf.reduce_mean(cross_entropy_loss)\n",
    "            y_prob = tf.sigmoid(logits)\n",
    "            y_pred = tf.cast(tf.sign(logits),tf.float32)/2+0.5\n",
    "            correct_prediction = tf.equal(tf.round(y_pred), y)\n",
    "            accuracy =  tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code segment prepares minibatch training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "def mnist_siamese_dataset_iterator(batch_size, dataset_name):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1 # -1 for entire dataset\n",
    "    mnist = input_data.read_data_sets('./datasets/mnist/', one_hot=True)\n",
    "    dataset = getattr(mnist, dataset_name)\n",
    "    \n",
    "    while True:\n",
    "        if batch_size > 0:\n",
    "            X1, y1 = dataset.next_batch(batch_size)\n",
    "            X2, y2 = dataset.next_batch(batch_size)\n",
    "            y = np.argmax(y1, axis=1) == np.argmax(y2, axis=1)\n",
    "            yield X1, X2, y\n",
    "        else:\n",
    "            X1 = dataset.images\n",
    "            idx = np.arange(len(X1))\n",
    "            np.random.shuffle(idx)\n",
    "            X2 = X1[idx]\n",
    "            y1 = dataset.labels\n",
    "            y2 = y1[idx]\n",
    "            y = np.argmax(y1, axis=1) == np.argmax(y2, axis=1)\n",
    "            yield X1, X2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's run the training.\n",
    "\n",
    "Note: if you have a good GPU, you could change `tf.device(\"/cpu:0\")` to `tf.device(\"/gpu:0\")` in your **`build_model()`** function to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Python 3\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "test iteration 0\t accuracy: 0.119, loss: 0.866\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train iteration 0\t accuracy: 0.121, loss: 0.863\n",
      "test iteration 100\t accuracy: 0.583, loss: 0.674\n",
      "train iteration 100\t accuracy: 0.586, loss: 0.673\n",
      "test iteration 200\t accuracy: 0.605, loss: 0.668\n",
      "train iteration 200\t accuracy: 0.607, loss: 0.668\n",
      "test iteration 300\t accuracy: 0.618, loss: 0.663\n",
      "train iteration 300\t accuracy: 0.615, loss: 0.664\n",
      "test iteration 400\t accuracy: 0.625, loss: 0.660\n",
      "train iteration 400\t accuracy: 0.626, loss: 0.661\n",
      "test iteration 500\t accuracy: 0.630, loss: 0.659\n",
      "train iteration 500\t accuracy: 0.624, loss: 0.661\n",
      "test iteration 600\t accuracy: 0.629, loss: 0.656\n",
      "train iteration 600\t accuracy: 0.632, loss: 0.658\n",
      "test iteration 700\t accuracy: 0.642, loss: 0.652\n",
      "train iteration 700\t accuracy: 0.634, loss: 0.657\n",
      "test iteration 800\t accuracy: 0.646, loss: 0.654\n",
      "train iteration 800\t accuracy: 0.638, loss: 0.656\n",
      "test iteration 900\t accuracy: 0.645, loss: 0.652\n",
      "train iteration 900\t accuracy: 0.648, loss: 0.653\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError:\n",
    "    print('This is Python 3')\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter=1000, print_every=100):\n",
    "    with model_dict['graph'].as_default():\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.3)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['test', 'train'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 32]}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Model Variants\n",
    "\n",
    "(30 points)\n",
    "\n",
    "Tasks:\n",
    " \n",
    " 1. Create an improved version of the model in part 2. You are welcome to use techniques covered in class, including: increasing network depth, varying activation functions, learning rates, optimizers, cost functions, regularization etc. \n",
    " \n",
    " 2. Report the test accuracy.\n",
    "\n",
    "Note: you should **not** use convolutional layers for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Increasing the network depth can increase accurancy apparently, but it require more training, under 5000 iteration and 0.3 learning rate, the accurancy grow to 70 precent. \n",
    "\n",
    "Use Adagrad to repleace gradiance decent will also increace accurancy.\n",
    "\n",
    "Also I implete drop out in the model, it dose not improve the accura\n",
    "\n",
    "softplus or sigmoid function are both not as good as relu as activate function.\n",
    "\n",
    "I dont know the loss function better than cross entropy in classification problem, I add regularzation in loss function, modifying the reg constant will have effect on accurancy, 0.02 is suitable for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test iteration 0\t accuracy: 0.299, loss: 1.039\n",
      "train iteration 0\t accuracy: 0.295, loss: 1.041\n",
      "test iteration 500\t accuracy: 0.550, loss: 0.694\n",
      "train iteration 500\t accuracy: 0.551, loss: 0.694\n",
      "test iteration 1000\t accuracy: 0.591, loss: 0.685\n",
      "train iteration 1000\t accuracy: 0.584, loss: 0.687\n",
      "test iteration 1500\t accuracy: 0.628, loss: 0.679\n",
      "train iteration 1500\t accuracy: 0.623, loss: 0.680\n",
      "test iteration 2000\t accuracy: 0.648, loss: 0.679\n",
      "train iteration 2000\t accuracy: 0.651, loss: 0.677\n",
      "test iteration 2500\t accuracy: 0.683, loss: 0.669\n",
      "train iteration 2500\t accuracy: 0.684, loss: 0.668\n",
      "test iteration 3000\t accuracy: 0.693, loss: 0.668\n",
      "train iteration 3000\t accuracy: 0.692, loss: 0.668\n",
      "test iteration 3500\t accuracy: 0.709, loss: 0.666\n",
      "train iteration 3500\t accuracy: 0.711, loss: 0.666\n",
      "test iteration 4000\t accuracy: 0.729, loss: 0.661\n",
      "train iteration 4000\t accuracy: 0.728, loss: 0.661\n",
      "test iteration 4500\t accuracy: 0.742, loss: 0.659\n",
      "train iteration 4500\t accuracy: 0.741, loss: 0.657\n"
     ]
    }
   ],
   "source": [
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    layer = x\n",
    "    batch_size = 784\n",
    "\n",
    "    def getWeight(shape,ini_value):\n",
    "        weights = tf.get_variable('weights',shape,initializer=tf.random_normal_initializer(stddev=ini_value))\n",
    "        return weights\n",
    "\n",
    "    def getBias(shape,ini_value):\n",
    "        bias = tf.get_variable('bias',shape,initializer=tf.random_normal_initializer(mean=0.1,stddev=ini_value))\n",
    "        return bias\n",
    "\n",
    "    if len(hidden_sizes)>1:\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            with tf.variable_scope('layer_'+str(i+1)):\n",
    "                ini_value = 1.0/(hidden_sizes[i]**0.5)\n",
    "                weights =getWeight([batch_size,hidden_sizes[i]],ini_value)\n",
    "                bias = getBias(hidden_sizes[i],ini_value)\n",
    "                h = tf.matmul(layer,weights)+bias\n",
    "                keep_prob = tf.Variable(0.5)\n",
    "                h_drop = tf.nn.dropout(h, keep_prob)   #drop out\n",
    "                layer = activation_fn(h_drop)\n",
    "            batch_size = hidden_sizes[i]\n",
    "\n",
    "    with tf.variable_scope('layer_'+str(i+2)):\n",
    "        ini_value = 1.0/(hidden_sizes[-1]**0.5)\n",
    "        weights =getWeight([batch_size,hidden_sizes[-1]],ini_value)\n",
    "        bias = getBias(hidden_sizes[-1],ini_value)\n",
    "        outlayer = tf.matmul(layer,weights)+bias\n",
    "        \n",
    "    return outlayer\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter=1000, print_every=100):\n",
    "    with model_dict['graph'].as_default():\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate = 0.3)  #Grad decent accu is 6.53,Adagrad is better\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['test', 'train'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)                     \n",
    "    \n",
    "def my_build_model(mlp_args):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x1 = tf.placeholder(tf.float32, [None, 784])\n",
    "            x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            with tf.variable_scope(\"siamese\") as var_scope:\n",
    "                x_repr1 = mlp(x1, **mlp_args)  \n",
    "                var_scope.reuse_variables()     \n",
    "                x_repr2 = mlp(x2, **mlp_args)  \n",
    "                logits = cosine_similarity(x_repr1, x_repr2)  \n",
    "                \n",
    "            reg_const = 0.02                                          #regularzation\n",
    "            w = tf.get_variable('weights',[64,40,20])\n",
    "            reg = tf.nn.l2_loss(w)           \n",
    "            cross_entropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits)            \n",
    "            loss = (tf.reduce_mean(cross_entropy_loss) + reg_const * reg)\n",
    "            y_prob = tf.sigmoid(logits)\n",
    "            y_pred = tf.cast(tf.sign(logits),tf.float32)/2+0.5\n",
    "            correct_prediction = tf.equal(tf.round(y_pred), y)\n",
    "            accuracy =  tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "    \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "my_mlp_args = {'hidden_sizes':[64, 40, 20], 'activation_fn': tf.nn.relu}\n",
    "model = my_build_model(my_mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter,n_iter=5000, print_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: Visualize learned features\n",
    "(25 points)\n",
    "\n",
    "Once a neural network is trained, we can think of the last layer as performing prediction, and the activations from layer before  become the input \"features\" to the predictor. If the neural network is performing good predictions, then this means that these activations encode useful features that are 1) representative of the input, and 2) discriminative for the prediction task. Activations at a selected layer could then be used as generic feature encodings of the input. \n",
    "\n",
    "We  expect a \"good\" feature encoding scheme to group similar inputs together in the feature encoding space. To check that, we can visualize the features in 2-dimensional space and check if similar examples (in this case, sharing the same labels) are close together. \n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. In TensorFlow, extract 32-dimensional features from the **distance layer** of your trained Siamese network, for the MNIST **test set**.\n",
    "2. Reduce the dimensionality of your deep features to 2 using [t-SNE embedding](https://lvdmaaten.github.io/tsne/). You may use [this fast implementation](https://github.com/lvdmaaten/bhtsne/) with correct attribution. You could alternatively use [sklearn.manifold.TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), but please take note that it might be slow.\n",
    "3. Visualize the 2-dimensional embeddings. If your extracted features are good, data points representing a specific digit should appear within a compact cluster. In the example below, each color corresponds to a digit class.\n",
    "\n",
    " <img src=\"tsne.png\" style=\"width:480px;\">\n",
    " \n",
    "For this part, the starter code is in minimalist fashion. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def my_run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter=1000, print_every=100):\n",
    "    with model_dict['graph'].as_default():\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict = batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([train_full_iter, test_full_iter], ['train', 'test'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss'],model_dict['features']]\n",
    "                        acc_value, loss_val, feature = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "                        \n",
    "            pickle.dump(feature, open('feature.p','wb'))    \n",
    "                        \n",
    "            mnist = input_data.read_data_sets('./datasets/mnist/')\n",
    "            X1, y = mnist.test.images, mnist.test.labels\n",
    "            pickle.dump(y, open('y.p','wb')) \n",
    "            X2 = X1\n",
    "            batch_feed_dict = dict(zip(model_dict['inputs'], [X1, X2, y]))\n",
    "            to_compute = [model_dict['features'], model_dict['accuracy']]\n",
    "            features, acc = sess.run(to_compute, batch_feed_dict)\n",
    "            return features, y\n",
    "\n",
    "def my_build_model2(mlp_args):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x1 = tf.placeholder(tf.float32, [None, 784])\n",
    "            x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "            \n",
    "            with tf.variable_scope(\"siamese\") as var_scope:\n",
    "                x_repr1 = mlp(x1, **mlp_args)  # hidden representation of x1\n",
    "                var_scope.reuse_variables()    # weight sharing! \n",
    "                x_repr2 = mlp(x2, **mlp_args)  # hidden representation of x2\n",
    "                logits = cosine_similarity(x_repr1, x_repr2)  # similarity\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "            y_prob = tf.sigmoid(logits)\n",
    "            y_pred = tf.cast(tf.equal(tf.sign(logits),1),tf.float32)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y), tf.float32))\n",
    "            \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy, 'features' : x_repr1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train iteration 0\t accuracy: 0.231, loss: 0.793\n",
      "test iteration 0\t accuracy: 0.228, loss: 0.793\n",
      "train iteration 500\t accuracy: 0.540, loss: 0.692\n",
      "test iteration 500\t accuracy: 0.547, loss: 0.690\n",
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.75116515, -0.93188113, -0.18354443, ..., -1.40005851,\n",
       "         -0.7876634 ,  0.85547233],\n",
       "        [ 1.14278185,  0.09942205,  1.01749277, ..., -1.10911465,\n",
       "         -3.06502008, -0.73884284],\n",
       "        [ 1.76430988, -0.55046231,  0.33775109, ...,  1.26696765,\n",
       "         -1.54711235, -1.00941133],\n",
       "        ..., \n",
       "        [-2.52664328, -2.37087607,  1.58287179, ...,  1.4481194 ,\n",
       "          3.21246576, -0.05928579],\n",
       "        [ 1.00663376, -1.28487849, -1.44226027, ..., -1.66808057,\n",
       "          0.2716901 , -1.5477761 ],\n",
       "        [-0.30990651,  1.5912782 , -0.30734336, ..., -1.28398085,\n",
       "         -0.44444978,  4.95184135]], dtype=float32),\n",
       " array([7, 2, 1, ..., 4, 5, 6], dtype=uint8))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = my_build_model2(mlp_args)\n",
    "data = my_run_training(model1, train_data_iterator, test_full_iter, train_full_iter, n_iter=1000, print_every=500)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAF+CAYAAAAxyjhHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd33b74d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize(features, labels):\n",
    "    tsne_model = TSNE(n_components=2, perplexity = 10)\n",
    "    X_tsne = tsne_model.fit_transform(features)\n",
    "    colors = (\"b\", \"g\", \"r\", \"c\", \"y\", \"k\", \"m\", \"#f49242\", \"#41f4af\", \"#8e41f4\")\n",
    "    for l in np.unique(labels):\n",
    "        plt.scatter(X_tsne[y==l], X_tsne[y==l], s=20, color=colors[l], alpha=0.5)\n",
    "    plt.xlim(-1e15,2e15)\n",
    "    plt.ylim(-2e15,2e15)\n",
    "    plt.show()\n",
    "\n",
    "features = pickle.load(open('feature.p','rb'))\n",
    "y = pickle.load(open('y.p','rb'))\n",
    "visualize(features, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
