{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem Set 5\n",
    "Designed by Ben Usman, Sarah Adel Bargal, and Brian Kulis, with help from Kun He and Kate Saenko.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "\n",
    "1. implementation of RNN and LSTM\n",
    "2. training your implemented LSTM\n",
    "3. deriving multi-modal Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: RNN Example\n",
    "\n",
    "(10 points)\n",
    "\n",
    "In this example we train an RNN to infer the parameters of a simple dynamical system.\n",
    "First, we simulate a dynamical system with known parameters (random numbers), and use it to generate outputs. Then, we train an RNN on the generated datapoints, attempting to infer the original parameters.\n",
    "You are expected to run and study the provided code, which will be helpful for the second part (implementing LSTM).\n",
    "\n",
    "1. We define a discretized [dynamical system](https://en.wikipedia.org/wiki/Dynamical_system).\n",
    "At each discrete time $t$, the system observes input $x_t$. The system maintains some \"state\" $h_t$, which will be updated over time.\n",
    "Specifically, the states are updated by the following rule: $h_t = \\max(0, 1-(Wx_t+h_{t-1}))$, where $W$ is a parameter matrix.\n",
    "2. In this example, the input data $\\{x_t\\}$ is randomly generated, and the weight matrix $W$ is also drawn randomly. The system starts from an initial hidden state $h_0$, and runs for $t=1,\\ldots,T$.\n",
    "3. Given the sequence of states $\\{h_1,\\ldots,h_T\\}$, we would like to infer the weights $W$.\n",
    "\n",
    "To start with, the following code segment generates and displays the data.\n",
    "Refer to `data_generator.py` for details of data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable name, shape, min, max: \n",
      "h0 (10,) -2.83804700698 0.470849661598\n",
      "w (10, 10) -1.7834638307 1.89404770905\n",
      "x (100, 10) -3.66483744098 2.89223266309\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFyCAYAAAB/b0lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBDMzM6vlJMHMzMxqOUkwMzOzWi8APqMudqj4OZcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36845b7208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we present sample data generated \n",
    "import numpy as np\n",
    "import data_generator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(h_0, w), x, h = data_generator._build_rnn_testdata_matrix()\n",
    "\n",
    "print('variable name, shape, min, max: ')\n",
    "for v, name in zip([h_0, w, x], ['h0', 'w', 'x']):\n",
    "    print(name, v.shape, np.min(v), np.max(v))\n",
    "norm_x_t = np.sum(x**2, axis=1)\n",
    "plt.title('||x(t)||')\n",
    "plt.plot(np.arange(norm_x_t.shape[0]), norm_x_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We model the dynamical system with an RNN. We would like the state generated by the RNN to match the actual observed, and we use L2 loss for this purpose.\n",
    "This RNN is a *regression* model since it outputs real values.\n",
    "\n",
    "Below, `build_rnn_regression_model()` gives the model definition, and `train_rnn_with_noise()` generates a batch of data and then runs training on it. \n",
    "We corrupt the data generation process with noise, and let the dimensionality of the state $h$ be a free parameter. Therefore, the training function takes two input arguments: `noise_level` and `n_hidden_dim`.   You will later see how varying them affects reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "\n",
    "#############################################################################\n",
    "# RNN model graph\n",
    "def build_rnn_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs to the dynamical system\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed state from the dynamical system\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        with tf.variable_scope('weights'):\n",
    "            # weight matrix\n",
    "            w = tf.get_variable('w', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            # initial state\n",
    "            h_0 = tf.get_variable('h_0', [shape['n_hidden_dim']])\n",
    "            \n",
    "        # for t = 1 to T, update state \n",
    "        h_t = h_0\n",
    "        for t in range(shape['n_steps_per_batch']):\n",
    "            x_t = X[:, t, :]\n",
    "            h_t = tf.maximum(0.0, 1 - (tf.matmul(x_t, w) + h_t))\n",
    "        \n",
    "        # loss: L2\n",
    "        loss = tf.nn.l2_loss(h_t - y, name='loss')\n",
    "        train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "        \n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w': w, 'h_0': h_0}, 'graph': g}\n",
    "\n",
    "#############################################################################\n",
    "# Main train loop for an RNN regression model\n",
    "# \n",
    "# This takes synthetic data generated by data_generator.build_dataset()\n",
    "# the weight matrix W is then inferred with back-prop \n",
    "def train_rnn_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    rnn_dataset = data_generator.build_dataset('rnn', noise=noise_level, **shapes)\n",
    "    (h0_true, w_true), batched_data = rnn_dataset  # \"true\" weights\n",
    "    # build RNN model\n",
    "    model = build_rnn_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/rnn_demo'  # if on Windows\n",
    "    logdir = '/tmp/tensorboard/rnn_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    # If you want to see the plots, run tensorboard:\n",
    "    # $ tensorboard --logdir=[your_logdir]\n",
    "    #\n",
    "    # If you use SCC, you can forward the 6006 port from the cluster \n",
    "    # to your local machine via:\n",
    "    # $ ssh [SCC_cluster_name] -L 6006:localhost:6006\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(50):\n",
    "            loss_val, w_dist, h0_dist, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w'], model['weights']['h_0']]\n",
    "                _, summ, loss_val, w_val, h0_val = sess.run(to_compute, train_feed_dict)\n",
    "                \n",
    "                # compute reconstruction error\n",
    "                w_err = np.linalg.norm(w_true-w_val)\n",
    "                h0_err = np.linalg.norm(h0_true-h0_val)\n",
    "                \n",
    "                # for tensorboard\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"w_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=w_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"h_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=h0_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.flush()\n",
    "            if global_step > 200: \n",
    "                break  # just train for 200 iterations\n",
    "            print('epoch %d, loss=%g, w_err=%g'%(epoch_i, loss_val, w_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.\n",
    "Now test the RNN wth varying noise levels and hidden dimensionalities.\n",
    "- For each combination of `n_hidden_dim` and `noise_level`, report the reconstruction error (`w_err`).\n",
    "- Describe how the hidden dimentionality and the noise level influence reconstruction quality. And briefly explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the higher the n_hidden_dim is, the initial loss of model is bigger, it take longer time to train.\n",
    "the bigger the noise is, the initial loss will also get bigger, but the speed the loss decraese drop intensely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "/tmp/tensorboard/rnn_demo/hidden=10_noise=0.00_26-Apr-15-12-30\n",
      "epoch 0, loss=25.6037, w_err=2.27284\n",
      "epoch 1, loss=4.22061, w_err=0.88692\n",
      "epoch 2, loss=0.755555, w_err=0.331385\n",
      "epoch 3, loss=0.103027, w_err=0.138143\n",
      "epoch 4, loss=0.0408573, w_err=0.0846686\n",
      "epoch 5, loss=0.00885994, w_err=0.0428901\n",
      "epoch 6, loss=0.00409427, w_err=0.0358953\n",
      "10 0.1\n",
      "/tmp/tensorboard/rnn_demo/hidden=10_noise=0.10_26-Apr-15-12-34\n",
      "epoch 0, loss=54.498, w_err=2.47387\n",
      "epoch 1, loss=7.72373, w_err=0.990858\n",
      "epoch 2, loss=2.63967, w_err=0.480655\n",
      "epoch 3, loss=2.42119, w_err=0.471707\n",
      "epoch 4, loss=2.53361, w_err=0.501582\n",
      "epoch 5, loss=2.60492, w_err=0.587433\n",
      "epoch 6, loss=3.04657, w_err=0.67672\n",
      "10 0.5\n",
      "/tmp/tensorboard/rnn_demo/hidden=10_noise=0.50_26-Apr-15-12-39\n",
      "epoch 0, loss=46.0535, w_err=2.26858\n",
      "epoch 1, loss=30.5853, w_err=1.52023\n",
      "epoch 2, loss=32.3612, w_err=1.52959\n",
      "epoch 3, loss=32.9018, w_err=1.57825\n",
      "epoch 4, loss=33.6707, w_err=1.56346\n",
      "epoch 5, loss=35.3681, w_err=1.64368\n",
      "epoch 6, loss=36.3581, w_err=1.63208\n",
      "100 0\n",
      "/tmp/tensorboard/rnn_demo/hidden=100_noise=0.00_26-Apr-15-12-44\n",
      "epoch 0, loss=343.274, w_err=7.77055\n",
      "epoch 1, loss=35.7484, w_err=2.56467\n",
      "epoch 2, loss=4.42459, w_err=0.904401\n",
      "epoch 3, loss=0.707427, w_err=0.339904\n",
      "epoch 4, loss=0.199554, w_err=0.157328\n",
      "epoch 5, loss=0.072599, w_err=0.0946103\n",
      "epoch 6, loss=0.0434886, w_err=0.0693622\n",
      "100 0.1\n",
      "/tmp/tensorboard/rnn_demo/hidden=100_noise=0.10_26-Apr-15-12-49\n",
      "epoch 0, loss=376.351, w_err=7.88103\n",
      "epoch 1, loss=58.2206, w_err=3.0252\n",
      "epoch 2, loss=24.9844, w_err=1.63437\n",
      "epoch 3, loss=22.7806, w_err=1.38741\n",
      "epoch 4, loss=26.1233, w_err=1.49609\n",
      "epoch 5, loss=30.8248, w_err=1.64262\n",
      "epoch 6, loss=36.3823, w_err=1.77269\n",
      "100 0.5\n",
      "/tmp/tensorboard/rnn_demo/hidden=100_noise=0.50_26-Apr-15-12-54\n",
      "epoch 0, loss=708.891, w_err=10.2103\n",
      "epoch 1, loss=389.315, w_err=6.01253\n",
      "epoch 2, loss=350.382, w_err=5.98599\n",
      "epoch 3, loss=360.889, w_err=5.96423\n",
      "epoch 4, loss=364.701, w_err=6.06414\n",
      "epoch 5, loss=371.048, w_err=6.15351\n",
      "epoch 6, loss=375.152, w_err=6.23896\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with different data noise levels and hidden dimentionalities\n",
    "# We are lucky to know the true hidden dimentionality in our simultaion\n",
    "for n_hidden_dim in [10,100]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_rnn_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: LSTM implementation\n",
    "\n",
    "(40 points)\n",
    "\n",
    "Now let's attempt to recover the weights in dynamical system simulated with an LSTM.  Although LSTMs are already implemented in TensorFlow ([tutorial here](https://www.tensorflow.org/tutorials/recurrent)) ([source here](https://github.com/tensorflow/tensorflow/blob/efe5376f3dec8fcc2bf3299a4ff4df6ad3591c88/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L264)), you will be implementing a simple LSTM from scratch using Tensorflow in this part. \n",
    "\n",
    "\n",
    "### Q2. Implement an LSTM \n",
    "Implement an LSTM model in an analogous way,  in functions `build_lstm_regression_model()` and `train_lstm_with_noise()` below.\n",
    "The RNN regression implementation above should give you some ideas.\n",
    "We already provided an LSTM version of the dynamical system generator in the dataset generator code.\n",
    "\n",
    "- Specifically, you can follow and implement Eq. 1-6 from [this link](http://deeplearning.net/tutorial/lstm.html) in `build_lstm_regression_model()`. \n",
    "You may simplify your code by concatenating $x$ and $h$.\n",
    "- Afterwards, implement `train_lstm_with_noise()` to train the LSTM and recover the parameters. Compute the reconstruction errors for $W_c$ and $U_c$, which are the parameters used in Eq. 2 in the link.\n",
    "- For each combination of hidden dimension and noise level, report the reconstruction error (`w_err`, `u_err`) you get from the LSTM.\n",
    "\n",
    "Note:\n",
    "- The weights might not get reconstructed correctly in the LSTM case even without noise. (Why?)\n",
    "Nevertheless, the loss should decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed outputs\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        with tf.variable_scope('weights'):\n",
    "            h_0 = tf.get_variable('h_0', [1,shape['n_hidden_dim']])\n",
    "            c_0 = tf.get_variable('c_0', [1,shape['n_hidden_dim']])\n",
    "\n",
    "            w_i = tf.get_variable('w_i', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_c = tf.get_variable('w_c', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_f = tf.get_variable('w_f', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_o = tf.get_variable('w_o', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "\n",
    "            u_i = tf.get_variable('u_i', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_c = tf.get_variable('u_c', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_f = tf.get_variable('u_f', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_o = tf.get_variable('u_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "\n",
    "            v_o = tf.get_variable('v_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "\n",
    "        h_pre = h_0\n",
    "        c_pre = c_0\n",
    "        for t in range(shape['n_steps_per_batch']):\n",
    "            x_t = X[:, t, :]\n",
    "            i_t = tf.sigmoid(tf.matmul(x_t, w_i) + tf.matmul(h_pre, u_i ))\n",
    "            c_bar_t = tf.tanh(tf.matmul(x_t, w_c) + tf.matmul(h_pre, u_c))\n",
    "            f_t = tf.sigmoid(tf.matmul(x_t, w_f) + tf.matmul(h_pre, u_f))\n",
    "            c_t = tf.multiply(i_t, c_bar_t) + tf.multiply(c_pre, f_t)\n",
    "            o_t = tf.sigmoid(tf.matmul(x_t, w_o) + tf.matmul(h_pre, u_o) + tf.matmul(c_t, v_o))\n",
    "            h_t = tf.multiply(o_t, tf.tanh(c_t))\n",
    "            h_pre = h_t\n",
    "            c_pre = c_t\n",
    "        output = h_t\n",
    "        \n",
    "        loss = tf.nn.l2_loss(output - y, name='loss')\n",
    "        train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w_c': w_c, 'u_c': u_c},\n",
    "            'graph': g}\n",
    "    \n",
    "    \n",
    "def train_lstm_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data and random weights\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    weights, batched_data = data_generator.build_dataset('lstm', noise=noise_level, **shapes)\n",
    "    w_c, u_c = weights[3], weights[7]  # the \"true\" weights to recover: Wc & Uc (in Eq.2)\n",
    "    \n",
    "    # this is the function you implemented\n",
    "    model = build_lstm_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/lstm_demo'  # if on Windows\n",
    "    logdir = '/tmp/tensorboard/lstm_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    \n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(10):  # 10 epochs by default, feel free to change\n",
    "            loss_val, w_err, u_err, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w_c'], model['weights']['u_c']]\n",
    "                _, summ, loss_val, w_val, u_val = sess.run(to_compute, train_feed_dict)                \n",
    "                w_err = np.linalg.norm(w_c - w_val)\n",
    "                u_err = np.linalg.norm(u_c - u_val)\n",
    "\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"w_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=w_err),]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"h_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=u_err),]), global_step)\n",
    "                sum_writer.flush()\n",
    "\n",
    "                if global_step > 300:\n",
    "                    break\n",
    "            print('epoch %d, loss=%g, w_err=%g, u_err=%g'%(epoch_i, loss_val, w_err,u_err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "Tensor(\"Mul_299:0\", shape=(?, 10), dtype=float32)\n",
      "/tmp/tensorboard/lstm_demo/hidden=10_noise=0.00_26-Apr-15-13-13\n",
      "epoch 0, loss=11.2598, w_err=10.1051, u_err=12.5752\n",
      "epoch 1, loss=6.95956, w_err=8.60224, u_err=12.2505\n",
      "epoch 2, loss=6.37335, w_err=7.61475, u_err=12.3742\n",
      "epoch 3, loss=5.72636, w_err=7.19063, u_err=13.0638\n",
      "epoch 4, loss=5.50673, w_err=7.24652, u_err=13.4516\n",
      "epoch 5, loss=6.97275, w_err=7.33309, u_err=13.5725\n",
      "epoch 6, loss=6.14768, w_err=7.15017, u_err=13.9518\n",
      "epoch 7, loss=5.31013, w_err=7.07556, u_err=14.798\n",
      "epoch 8, loss=5.91273, w_err=7.33495, u_err=14.9513\n",
      "epoch 9, loss=4.46037, w_err=8.20797, u_err=15.0879\n",
      "10 0.1\n",
      "Tensor(\"Mul_299:0\", shape=(?, 10), dtype=float32)\n",
      "/tmp/tensorboard/lstm_demo/hidden=10_noise=0.10_26-Apr-15-13-30\n",
      "epoch 0, loss=8.32049, w_err=7.70981, u_err=12.2013\n",
      "epoch 1, loss=6.1031, w_err=6.13761, u_err=13.25\n",
      "epoch 2, loss=5.65639, w_err=5.03726, u_err=13.3981\n",
      "epoch 3, loss=5.83069, w_err=4.47336, u_err=13.6781\n",
      "epoch 4, loss=5.26035, w_err=4.68476, u_err=14.0714\n",
      "epoch 5, loss=5.98161, w_err=4.69908, u_err=14.6727\n",
      "epoch 6, loss=4.8776, w_err=5.32463, u_err=14.9658\n",
      "epoch 7, loss=5.75432, w_err=6.12558, u_err=15.7448\n",
      "epoch 8, loss=4.25691, w_err=6.587, u_err=16.2584\n",
      "epoch 9, loss=4.65452, w_err=7.44509, u_err=16.9657\n",
      "10 0.5\n",
      "Tensor(\"Mul_299:0\", shape=(?, 10), dtype=float32)\n",
      "/tmp/tensorboard/lstm_demo/hidden=10_noise=0.50_26-Apr-15-13-47\n",
      "epoch 0, loss=14.5048, w_err=8.64681, u_err=11.7687\n",
      "epoch 1, loss=12.1635, w_err=7.9928, u_err=12.3532\n",
      "epoch 2, loss=9.96297, w_err=7.03496, u_err=13.1138\n",
      "epoch 3, loss=9.8399, w_err=6.24188, u_err=13.7636\n",
      "epoch 4, loss=8.77885, w_err=6.13971, u_err=13.8497\n",
      "epoch 5, loss=9.62276, w_err=6.88602, u_err=14.5517\n",
      "epoch 6, loss=9.37617, w_err=7.48511, u_err=15.5076\n",
      "epoch 7, loss=9.25588, w_err=7.76614, u_err=16.9444\n",
      "epoch 8, loss=8.61525, w_err=8.34572, u_err=17.6619\n",
      "epoch 9, loss=10.4387, w_err=8.72017, u_err=18.1264\n",
      "100 0\n",
      "Tensor(\"Mul_299:0\", shape=(?, 100), dtype=float32)\n",
      "/tmp/tensorboard/lstm_demo/hidden=100_noise=0.00_26-Apr-15-14-08\n",
      "epoch 0, loss=161.108, w_err=37.2262, u_err=115.71\n",
      "epoch 1, loss=156.001, w_err=38.3352, u_err=117.731\n",
      "epoch 2, loss=164.676, w_err=43.2102, u_err=128.266\n",
      "epoch 3, loss=158.23, w_err=46.173, u_err=132.762\n",
      "epoch 4, loss=153.449, w_err=47.6518, u_err=133.646\n",
      "epoch 5, loss=154.18, w_err=48.8453, u_err=134.28\n",
      "epoch 6, loss=153.354, w_err=50.298, u_err=134.874\n",
      "epoch 7, loss=154.893, w_err=52.0408, u_err=135.515\n",
      "epoch 8, loss=153.217, w_err=53.8494, u_err=136.399\n",
      "epoch 9, loss=154.21, w_err=55.7082, u_err=137.222\n",
      "100 0.1\n",
      "Tensor(\"Mul_299:0\", shape=(?, 100), dtype=float32)\n",
      "/tmp/tensorboard/lstm_demo/hidden=100_noise=0.10_26-Apr-15-15-03\n",
      "epoch 0, loss=180.57, w_err=40.604, u_err=114.45\n",
      "epoch 1, loss=157.773, w_err=41.5881, u_err=117.814\n",
      "epoch 2, loss=153.505, w_err=41.6568, u_err=118.168\n",
      "epoch 3, loss=148.917, w_err=41.6552, u_err=118.3\n",
      "epoch 4, loss=146.548, w_err=41.6345, u_err=118.383\n",
      "epoch 5, loss=144.95, w_err=41.656, u_err=118.431\n",
      "epoch 6, loss=144.243, w_err=41.6777, u_err=118.447\n",
      "epoch 7, loss=143.299, w_err=41.6178, u_err=118.484\n",
      "epoch 8, loss=143.739, w_err=41.6695, u_err=118.497\n",
      "epoch 9, loss=148.066, w_err=41.9759, u_err=118.701\n",
      "100 0.5\n",
      "Tensor(\"Mul_299:0\", shape=(?, 100), dtype=float32)\n",
      "/tmp/tensorboard/lstm_demo/hidden=100_noise=0.50_26-Apr-15-15-33\n",
      "epoch 0, loss=161.806, w_err=37.7238, u_err=115.872\n",
      "epoch 1, loss=161.811, w_err=38.816, u_err=118.334\n",
      "epoch 2, loss=161.129, w_err=39.6889, u_err=118.79\n",
      "epoch 3, loss=161.073, w_err=40.8084, u_err=119.27\n",
      "epoch 4, loss=189.371, w_err=52.3421, u_err=147.391\n",
      "epoch 5, loss=nan, w_err=nan, u_err=nan\n",
      "epoch 6, loss=nan, w_err=nan, u_err=nan\n",
      "epoch 7, loss=nan, w_err=nan, u_err=nan\n",
      "epoch 8, loss=nan, w_err=nan, u_err=nan\n",
      "epoch 9, loss=nan, w_err=nan, u_err=nan\n"
     ]
    }
   ],
   "source": [
    "for n_hidden_dim in [10,100]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_lstm_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The parameter is highly nonlinear, thus using the SGD should be hard to find the local min.\n",
    "Also the parameter will affect each other, and it is hard to get the decreasing loss effectively.\n",
    "The training process might have many local min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Neural Tolstoy Model (a character-prediction LSTM)\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this part you will train a character prediction LSTM that predicts the next character given previous characters. \n",
    "The training data is from the book \"War and Peace\" by Leo Tolstoy. \n",
    "\n",
    "### Q3.\n",
    "Compared to the LSTM you implemented in the previous part, the main difference in the character prediction LSTM is that it predicts *discrete* outputs (characters, or their indices in the vocabulary), therefore it is a classification model. As you may recall, the usual choice of loss function for classification is softmax + cross entropy.\n",
    "\n",
    "We have already provided functions for loading the training data. Please define your discrete LSTM in `build_lstm_discrete_prediction_model()`.\n",
    "\n",
    "Hints: \n",
    "- Your LSTM should predict a single character at a time. (That's why it's called a character prediction LSTM.) We are not considering  many-to-many LSTMs here.\n",
    "- Feeding the previous input into the LSTM should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tolstoy_reader\n",
    "\n",
    "def get_default_gpu_session(fraction=0.333):\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = fraction\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "def run_tolstoy_train(n_hid):\n",
    "    # generate data\n",
    "    btg, map_dict, backmap_dict = \\\n",
    "        tolstoy_reader.batch_tolstoy_generator(batch_size=200, seq_size=100)\n",
    "    shape = dict(n_steps_per_batch=100, n_unique_ids=len(map_dict), n_hidden_dim=n_hid)\n",
    "    model = build_lstm_discrete_prediction_model(shape)\n",
    "\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, get_default_gpu_session(0.9) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(10):\n",
    "            for iter_i, data_batch in enumerate(btg):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "\n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['loss']]\n",
    "                _, loss_val = sess.run(to_compute, train_feed_dict)\n",
    "                # test generation\n",
    "                pred_length = 50\n",
    "                data_input = next(iter(btg))[0][[0]]\n",
    "                original_sample = data_input.copy()\n",
    "                pred_seq = []\n",
    "                for _ in range(pred_length):\n",
    "                    pred = sess.run(model['pred'], {model['inputs'][0]: data_input})\n",
    "                    pred_seq.append(pred[0])\n",
    "                    data_input = np.roll(data_input, -1, axis=1)\n",
    "                    data_input[0, -1] = pred[0]\n",
    "                if iter_i % 100 == 0:\n",
    "                    print(loss_val)\n",
    "                    print('epoch %d [%d] Input text:' % (epoch_i, iter_i),\n",
    "                              ''.join([backmap_dict[x] for x in original_sample[0]]))\n",
    "                    print('epoch %d [%d] Generated continuation:' % (epoch_i, iter_i),\n",
    "                              ''.join([backmap_dict[x] for x in pred_seq]))\n",
    "                    print(pred_seq)\n",
    "                    print()\n",
    "                \n",
    "def build_lstm_discrete_prediction_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_unique_ids, n_hidden_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        X = tf.placeholder(tf.int64, [None, shape['n_steps_per_batch']])\n",
    "        y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "        with tf.variable_scope('weights'):\n",
    "            h_0 = tf.get_variable('h_0', [1,shape['n_hidden_dim']])\n",
    "            c_0 = tf.get_variable('c_0', [1,shape['n_hidden_dim']])\n",
    "            w_i = tf.get_variable('w_i', [1, shape['n_hidden_dim']])\n",
    "            w_c = tf.get_variable('w_c', [1, shape['n_hidden_dim']])\n",
    "            w_f = tf.get_variable('w_f', [1, shape['n_hidden_dim']])\n",
    "            w_o = tf.get_variable('w_o', [1, shape['n_hidden_dim']])\n",
    "            w_yh = tf.get_variable('w_yh', [shape['n_hidden_dim'], shape['n_unique_ids']])\n",
    "            v_o = tf.get_variable('v_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_i = tf.get_variable('u_i', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_c = tf.get_variable('u_c', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_f = tf.get_variable('u_f', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_o = tf.get_variable('u_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "\n",
    "        h_pre = h_0\n",
    "        c_pre = c_0\n",
    "        for t in range(shape['n_steps_per_batch']):\n",
    "            x_t = tf.expand_dims(tf.cast(X[:,t],'float32'),1)\n",
    "            i_t = tf.sigmoid(tf.matmul(x_t, w_i) + tf.matmul(h_pre, u_i ))\n",
    "            c_bar_t = tf.tanh(tf.matmul(x_t, w_c) + tf.matmul(h_pre, u_c))\n",
    "            f_t = tf.sigmoid(tf.matmul(x_t, w_f) + tf.matmul(h_pre, u_f))\n",
    "            c_t = tf.multiply(i_t, c_bar_t) + tf.multiply(c_pre, f_t)\n",
    "            o_t = tf.sigmoid(tf.matmul(x_t, w_o) + tf.matmul(h_pre, u_o) + tf.matmul(c_t, v_o))\n",
    "            h_t = tf.multiply(o_t, tf.tanh(c_t))\n",
    "            h_pre = h_t\n",
    "            c_pre = c_t\n",
    "        logits = tf.matmul(h_t, w_yh)\n",
    "        y_onehot = tf.one_hot(indices = y, depth = shape['n_unique_ids'])\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=logits))\n",
    "        pred = tf.argmax(logits, axis=1)\n",
    "        train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op,'graph': g, 'pred': pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the LSTM and see what it says! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58115\n",
      "epoch 0 [0] Input text:  ready? It is nearly ten,\" came the countess' voice. \"Directly! Directly! And you, Mamma?\" \"I have o\n",
      "epoch 0 [0] Generated continuation: DDDDDDDDDDDD]D]D]D]D]D]D]D]D]D]D]D]D]D]D]D]D]D]D]D\n",
      "[25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25, 65, 25]\n",
      "\n",
      "3.29744\n",
      "epoch 0 [100] Input text:  sort of amble, not exactly bowing yet seeming to grow suddenly smaller, and respectfully received t\n",
      "epoch 0 [100] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "3.08162\n",
      "epoch 0 [200] Input text: oul and the veil that had till then concealed the unknown was lifted from his spiritual vision. He f\n",
      "epoch 0 [200] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "3.02326\n",
      "epoch 0 [300] Input text:  close by, who heard him muttering. \"Nothing... only a shell...\" he answered. \"Come along, our Matve\n",
      "epoch 0 [300] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "3.02119\n",
      "epoch 0 [400] Input text: ode up smiling to the Emperor to congratulate him on the victory. Prince Andrew remembered nothing m\n",
      "epoch 0 [400] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "2.98995\n",
      "epoch 0 [500] Input text:  addressing Princess Mary for the first time, \"to forget yourself again before her as you dared to d\n",
      "epoch 0 [500] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "3.2039\n",
      "epoch 0 [600] Input text: oscow. But this intrigue did not now occupy the old man's mind. One terrible question absorbed him a\n",
      "epoch 0 [600] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "3.01142\n",
      "epoch 0 [700] Input text:  of what went on around him. Not only was he indifferent as to whether he got to Petersburg earlier \n",
      "epoch 0 [700] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "2.86786\n",
      "epoch 0 [800] Input text: his battery when two balls, and then four more, fell among our guns, one knocking over two horses an\n",
      "epoch 0 [800] Generated continuation:                                                   \n",
      "[61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "\n",
      "3.01625\n",
      "epoch 0 [900] Input text: TER XXIII Prince Andrew needed his father's consent to his marriage, and to obtain this he started f\n",
      "epoch 0 [900] Generated continuation:  t  t  t  t  t t t t t t t t t t t t t t t t t t t\n",
      "[61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46]\n",
      "\n",
      "2.86103\n",
      "epoch 0 [1000] Input text: exation and placed his hand on the gate as if to leave. He again paused in indecision. \"You see,\" he\n",
      "epoch 0 [1000] Generated continuation:   t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  \n",
      "[61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61]\n",
      "\n",
      "2.88135\n",
      "epoch 0 [1100] Input text: told in Russian, German, and Czech by the crowd of fugitives who understood what was happening as li\n",
      "epoch 0 [1100] Generated continuation: e  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t \n",
      "[41, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61, 61, 46, 61]\n",
      "\n",
      "2.88093\n",
      "epoch 0 [1200] Input text: rink!\" said Anatole, and filled a large glass of Madeira for him. The driver's eyes sparkled at the \n",
      "epoch 0 [1200] Generated continuation: t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "[46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61]\n",
      "\n",
      "2.84867\n",
      "epoch 0 [1300] Input text:  a little, gentlemen,\" said he. \"The battle is won, and there is nothing extraordinary in the captur\n",
      "epoch 0 [1300] Generated continuation: e t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "[41, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61, 46, 61]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2a84ab4cdc22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_tolstoy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c0368a8d6f97>\u001b[0m in \u001b[0;36mrun_tolstoy_train\u001b[0;34m(n_hid)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mtrain_feed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mto_compute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_op'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_compute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# test generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mpred_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden = 50\n",
    "run_tolstoy_train(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result shows that the machine start saying some 't' and space that help it to reduce the cross entropy loss. the result is incomplete because the training take times. I used to run the program on linux shell, it shows that the machine start saying something like 'the sound the sound the sound the sound' and 'concented', the loss was reduce to around 1.81 after 4 epoch train.\n",
    "\n",
    "<img src=\"File_002.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: Multi-modal Restricted Boltzmann Machines\n",
    "\n",
    "(30 points)\n",
    "\n",
    "- Relevant reading: Goodfellow chapter 20 through 20.8, in particular 20.2 and 20.3. Also see the [lecture notes](https://drive.google.com/file/d/0B8xkaMshuaF9dFVRclo3RENKdUk/view?usp=sharing).\n",
    "\n",
    "For this question, we will consider designing an unsupervised probability distribution  over two modalities.  Suppose we have data consisting of images and their associated captions, so that each input data point may be thought of as the pair $(t,v)$, where $t = (t_1, ..., t_n)$ is the text data and $v = (v_1, ..., v_m)$ is the visual data.  For simplicity, we will consider both as vector inputs (so, for instance, we will not be considering any two-dimensional convolutions for the visual data); the main difference between the two inputs is that the text input is discrete-valued, where we assume that each $t_i$ can take on $k$ different values, and each visual input is real-valued.\n",
    "\n",
    "Following our discussion of Restricted Boltzmann Machines (RBMs), we will further consider a hidden layer with **binary** hidden units $(h_1, ..., h_d)$, and define a joint probability distribution over an input data point and a hidden layer.  In particular, let us define two matrices $W_t\\in\\mathbb{R}^{n\\times d}$ and $W_v\\in\\mathbb{R}^{m\\times d}$; these matrices correspond to weights between text input and hidden units, and weights between visual input and hidden units, respectively.  In addition, we have the bias vectors $a$, $b$, and $c$, associated with the text, visual, and hidden units, respectively.\n",
    "The multi-modal RBM is illustrated below.\n",
    "\n",
    "<img src=\"MultimodalRBM.png\" width=\"550\">\n",
    "\n",
    "Next we define the joint probability distribution $p(t,v,h)$ over text inputs, visual inputs, and hidden states.  First we let the energy function be: \n",
    "\n",
    "$E(v,t,h) = -t^T W_t h - v^T W_v h - a^T t + (0.5 b^Tb - b^T v + 0.5 v^T v) - c^T h$.\n",
    "\n",
    "Then let the joint probability be defined as:\n",
    "\n",
    "$p(v,t,h) = \\frac{1}{Z}exp(-E(v,t,h)),$\n",
    "\n",
    "where $Z$ is the normalizing constant that ensures that the probability distribution is properly normalized to sum/integrate to one.\n",
    "\n",
    "Our goal is to perform maximum likelihood estimation for the parameters $W_t$, $W_v$, and $c$ (we will assume $a$ and $b$ are fixed and known), given a set of $N$ data points $D = \\{(t^{(1)},v^{(1)}), ..., (t^{(N)},v^{(N)})\\}$.  As is typical in such settings, we assume that the data in $D$ are i.i.d. samples.\n",
    "\n",
    "Please write your solutions in the cells below, or hand in a hard copy.\n",
    "For each question you must show your work to receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.1 \n",
    "\n",
    "Write down the marginal probability for a single data point $(t^{(1)}, v^{(1)})$, namely $p(t^{(1)}, v^{(1)})$. Treating this probability as the likelihood for a single point, write down the log-likelihood for the entire data set $D$ as a function of the parameters $(W_t, W_v,  c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.2 \n",
    "\n",
    "Write down the partial derivative of $E(t,v,h)$ with respect to each of the parameters, i.e., each entry of $W_t$, each entry of $W_v$, and each entry of $c$. You can also directly write in matrix/vector forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.3 \n",
    "\n",
    "Derive the conditional distributions $p(h | v, t)$, $p(v | h)$ and $p(t | h)$, either in elementwise form or in vector form.\n",
    "Also establish that $v$ and $t$ are conditionally independent given $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"File_000.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.4\n",
    "\n",
    "Write down the partial derivative of the log-likelihood for $D$ with respect to each of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[Put your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.5 \n",
    "\n",
    "Extend the contrastive divergence approach for standard RBMs, as discussed in class, to suggest a gradient ascent procedure for learning the parameters of our multi-modal RBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"File_001.jpeg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
