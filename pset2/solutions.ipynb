{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Cross-Entropy and Softmax\n",
    "\n",
    "\n",
    "**Q1.1**: Show that the maximum likelihood estimate of the parameters $\\theta$ can be obtained by minimizing the multiclass **cross-entropy** loss function \n",
    "<p>\n",
    "$L(\\theta)= - \\frac{1}{N}\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(h_k(x_i,\\theta))$\n",
    "</p>\n",
    "<p>\n",
    "where $N$ is the number of examples $\\{x_i,y_i\\}$. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln**: For a single example $x_i,y_i$, the log-likelihood function can be written as:\n",
    "\\begin{equation}\n",
    "\\log P(y_{i}\\ |\\ x_i, \\theta) = \\log\\prod\\limits_{k=1}^Kh_k(x_i, \\theta)^{y_{ik}} =\\sum_{k=1}^Ky_{ik}\\log h_k(x_i,\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Due to the fact that $y_i$ is one-hot. Then the maximum likelihood solution maximizes\n",
    "\\begin{equation}\n",
    "    \\sum\\limits_{i=1}^N \\log(P(y_{i}\\ |\\ x_i, \\theta)) = \\sum\\limits_{i=1}^N \\sum\\limits_{k=1}^K y_{ik} \\log(h_k(x_i, \\theta))\n",
    "\\end{equation}\n",
    "which is equivalent to minimizing $L(\\theta)$. The constant factor $\\frac{1}{N}$ does not change the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2**: softmax with **temperature parameter $T>0$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial z_k(x, \\theta)}  =\\frac{1}{T}\\left(h_k(x,\\theta) - y_k\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Simple Regularization Methods\n",
    "\n",
    "**Q2.1**:  L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln**: $\\theta_{t+1}\\gets (1-2\\lambda\\eta)\\theta_{t}-\\eta g_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2**:  L1 regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln**: $\\theta_{t+1}\\gets \\theta_t-\\eta\\lambda \\text{sign}(\\theta_t)- \\eta g_t$, where $\\text{sign}(\\theta_{t,i})=\\left\\{\\begin{matrix} 1 & \\theta_{t,i}\\geq 0 \\\\ -1 & \\theta_{t,i}<0 \\end{matrix}\\right.$.\n",
    "\n",
    "Note: it is also acceptable if you define $\\text{sign}(0)=-1$ or $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Backprop in a simple MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down each step of the backward pass explicitly for all layers, i.e. for the loss and $k=2,1$, compute all gradients above, expressing them as a function of variables $x, y, h, W, b$. <i>Hint: you should substitute the updated values for the gradient $g$ in each step and simplify as much as possible.</i>  Specifically, compute the following (we have replace the superscript notation $u^{(i)}$ with $u^i$):\n",
    "<p>\n",
    "**Q3.1**: $\\nabla_{\\hat{y}}L(\\hat{y},y)$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln:** $g \\leftarrow \\nabla_{\\hat{y}}L(\\hat{y},y) =  \\nabla_{\\hat{y}}[-yln(\\hat{y}) + (1-y)ln(1-\\hat{y})] = \\frac{\\hat{y}-y}{(1-\\hat{y})\\hat{y}} = \\frac{h^2-y}{(1-h^2)h^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2**: $\\nabla_{a^2}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** for $k=2$, $g \\leftarrow \\nabla_{a^2} J = g \\odot f'(a^2) = g \\odot h^2(1-h^2) = h^2 - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3**: $\\nabla_{b^2}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln:** for $k=2$, $\\nabla_{b^2} J = g  = h^2 - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.4**: $\\nabla_{W^2}J$ <br><i>Hint: this should be a vector, since $W^2$ is a vector. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** for $k=2$, $\\nabla_{W^2}J = g (h^1)^T = (h^2 - y)(h^1)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.5**: $\\nabla_{h^1}J$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** for $k=2$, $\\nabla_{h^1}J = (W^2)^T g = (h^2 - y)(W^2)^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.6**: $\\nabla_{b^1}J$, $\\nabla_{W^1}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** $\\nabla_{b^1}J= (h^{2} - y)((h^{1})^2 - h^{1}) \\odot W^{2}$, $\\nabla_{W^1}J = (h^{2} - y) (h^{0}) [((h^{1})^2 - h^{1}) \\odot W^{2}]  ^ T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.7** Briefly, explain how would the computational speed of backpropagation be affected if it did not include a forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln** It would increase significantly, as we'd need to re-compute activations h many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: XOR problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** Write the XOR function in terms of the logical functions (gates) $OR(x_1,x_2)$, $AND(x_1,x_2)$, $NAND(x_1,x_2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln** $XOR(x_1, x_2)= AND(OR(x_1,x_2),NAND(x_1,x_2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q4.2**: What are the missing weights $a,b,c,d,e$ of the OR, NAND, AND and CONVERT subnetworks, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln** $a=-2, b=-4, c=+2, d=+1, e=+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (Programming): Implementing a simple MLP\n",
    "\n",
    "See example implementation in [solutions_mlp.py](https://github.com/kunhe/cs591s2/blob/master/pset2/solutions_mlp.py).\n",
    "\n",
    "When you compare the three nets using different activations, you should see that Sigmoid underperforms (~86% test accuracy), and that Softplus and ReLU have similar performance (~96% test accuracy). Due to the simplicity of ReLU (easy coding and no need to worry about numerical issues), it is preferred in practice.\n",
    "\n",
    "Example output from the ReLU net on MNIST is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"sol_relu_curves.png\" style=\"width:800px;\">\n",
    "<img src=\"sol_relu_filters.png\" style=\"width:600px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
