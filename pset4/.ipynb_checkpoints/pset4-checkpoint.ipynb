{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem Set 4\n",
    "Designed by Ben Usman, Kun He, and Sarah Adel Bargal, with help from Kate Saenko and Brian Kulis.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "1. Building and training a convolutional network\n",
    "2. Saving snapshots of your trained model\n",
    "3. Reloading weights from a saved model\n",
    "4. Fine-tuning a pre-trained network\n",
    "5. Visualizations using Tensorboard\n",
    "\n",
    "This code has been tested and should for Python 3.5 and 2.7 with tensorflow 1.0.*. Since recently, you can update to recent tensorflow version just by doing `pip install tensorflow`,  or `pip install tensorflow-gpu` if you want to use GPU.\n",
    "\n",
    "**Note:** This notebook contains problem descriptions and demo/starter code. However, you're welcome to implement and submit .py files directly, if that's easier for you. Starter .py files are provided in the same `pset4/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 0: Tutorials\n",
    "\n",
    "You will find these TensorFlow tutorials on CNNs useful:\n",
    " - [Deep MNIST for experts](https://www.tensorflow.org/get_started/mnist/pros)\n",
    " - [Convolutional Neural Networks](https://www.tensorflow.org/tutorials/deep_cnn)\n",
    " \n",
    "Note that there are many ways to implement the same thing in TensorFlow, for example, both tf.nn and tf.layers provide convolutional layers but with slightly different interfaces. You will need to read the documentation of the functions provided below to understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: Building and Training a ConvNet on SVHN\n",
    "(25 points)\n",
    "\n",
    "First we provide demo code that trains a convolutional network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).. \n",
    "\n",
    "You will need to download   __Format 2__ from the link above.\n",
    "- Create a directory named `svhn_mat/` in the working directory. Or, you can create it anywhere you want, but change the path in `svhn_dataset_generator` to match it.\n",
    "- Download `train_32x32.mat` and `test_32x32.mat` to this directory.\n",
    "- `extra_32x32.mat` is NOT needed.\n",
    "- You may find the `wget` command useful for downloading on linux. \n",
    "\n",
    "\n",
    "\n",
    "The following defines a generator for the SVHN Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import read_data\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following defines the CovNet Model. It has two identical conv layers with 32 5x5 convlution filters, followed by a fully-connected layer to output the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cnn_map(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss(model_function):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.2 Training SVHN Net\n",
    "Now we train a `cnn_map` net on Format 2 of the SVHN Dataset. We will call this \"SVHN net\". \n",
    "\n",
    "**Note:** training will take a while, so you might want to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_dict, dataset_generators, epoch_n, print_every):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('epoch {:d} iter {:d}, loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "    \n",
    "model_dict = apply_classification_loss(cnn_map)\n",
    "train_model(model_dict, dataset_generators, epoch_n=50, print_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.3 SVHN Net Variations\n",
    "Now we vary the structure of the network. To keep things simple, we still use  two identical conv layers, but vary their parameters. \n",
    "\n",
    "Report the final test accuracy on 3 different number of filters, and 3 different number of strides. Each time when you vary one parameter, keep the other fixed at the original value.\n",
    "\n",
    "|Stride|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| / | / |\n",
    "| / | / |\n",
    "| / | / |\n",
    "\n",
    "|Filters|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| / | / |\n",
    "| / | / |\n",
    "| / | / |\n",
    "\n",
    "A template for one sample modification is given below. \n",
    "\n",
    "**Note:** you're welcome to decide how many training epochs to use, if that gets you the same results but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cnn_modification_s44(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=4)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=4)\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "#stride2\n",
    "def cnn_modification_s24(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=4)\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "#stride3\n",
    "def cnn_modification_s11(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=1)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=1)\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "#stride 1\n",
    "def cnn_modification_f12(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=12,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=12, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "#stride2\n",
    "def cnn_modification_f24(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=24,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=24, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "#filter3\n",
    "def cnn_modification_f48(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=48,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=48,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "\n",
    "print(\"map\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_map)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)\n",
    "print(\"stride 44\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_modification_s44)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)\n",
    "print(\"stride 24\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_modification_s24)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)\n",
    "print(\"stride11\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_modification_s11)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)\n",
    "print(\"filternum=12\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_modification_f12)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)\n",
    "print(\"filternum=24\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_modification_f24)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)\n",
    "print(\"filternum=48\")\n",
    "model_dict = model.apply_classification_loss(model.cnn_modification_f48)\n",
    "train_model(model_dict, dataset_generators, epoch_n=20, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Saving and Reloading Model Weights\n",
    "(25 points)\n",
    "\n",
    "In this section you learn to save the weights of a trained model, and to load the weights of a saved model. This is really useful when we would like to load an already trained model in order to continue training or to fine-tune it. Often times we save “snapshots” of the trained model as training progresses in case the training is interrupted, or in case we would like to fall back to an earlier model, this is called snapshot saving.\n",
    "\n",
    "### Q2.1 Defining another network\n",
    "Define a network with a slightly different structure in `def cnn_expanded(x_)` below. `cnn_expanded` is an expanded version of `cnn_model`. \n",
    "It should have: \n",
    "- a different size of kernel for the last convolutional layer, \n",
    "- followed by one additional convolutional layer, and \n",
    "- followed by one additional pooling layer.\n",
    "\n",
    "The last fully-connected layer will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the new model (see cnn_map(x_) above for an example)\n",
    "def cnn_expanded(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)  # convolution stride\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[3, 3],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)  # convolution stride\n",
    "\n",
    "    conv3 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[3, 3],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                    pool_size=[2, 2],\n",
    "                                    strides=2)  # convolution stride\n",
    "\n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q2.2 Saving and Loading Weights\n",
    "`new_train_model()` below has two additional parameters `save_model=False, load_model=False` than `train_model` defined previously. Modify `new_train_model()` such that it would \n",
    "- save weights after the training is complete if `save_model` is `True`, and\n",
    "- load weights on start-up before training if `load_model` is `True`.\n",
    "\n",
    "*Hint:*  `tf.train.Saver()`.\n",
    "\n",
    "Note: if you are unable to load weights into `cnn_expanded` network, use `cnn_map` in order to continue the assingment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 0\t loss: 46.746, accuracy: 0.109\n",
      "iteration 0 10\t loss: 2.335, accuracy: 0.114\n",
      "iteration 0 20\t loss: 2.245, accuracy: 0.196\n",
      "iteration 0 30\t loss: 2.248, accuracy: 0.172\n",
      "iteration 0 40\t loss: 2.234, accuracy: 0.198\n",
      "iteration 0 50\t loss: 2.233, accuracy: 0.202\n",
      "iteration 0 60\t loss: 2.261, accuracy: 0.168\n",
      "iteration 0 70\t loss: 2.214, accuracy: 0.210\n",
      "iteration 0 80\t loss: 2.220, accuracy: 0.207\n",
      "iteration 0 90\t loss: 2.191, accuracy: 0.218\n",
      "iteration 0 100\t loss: 2.165, accuracy: 0.243\n",
      "iteration 0 110\t loss: 2.027, accuracy: 0.320\n",
      "iteration 0 120\t loss: 2.283, accuracy: 0.342\n",
      "iteration 0 130\t loss: 1.840, accuracy: 0.391\n",
      "iteration 0 140\t loss: 1.721, accuracy: 0.457\n",
      "iteration 0 150\t loss: 1.578, accuracy: 0.502\n",
      "iteration 0 160\t loss: 1.431, accuracy: 0.554\n",
      "iteration 0 170\t loss: 1.396, accuracy: 0.562\n",
      "iteration 0 180\t loss: 1.333, accuracy: 0.588\n",
      "iteration 0 190\t loss: 1.267, accuracy: 0.612\n",
      "iteration 0 200\t loss: 1.260, accuracy: 0.612\n",
      "iteration 0 210\t loss: 1.257, accuracy: 0.614\n",
      "iteration 0 220\t loss: 1.165, accuracy: 0.647\n",
      "iteration 0 230\t loss: 1.160, accuracy: 0.651\n",
      "iteration 0 240\t loss: 1.113, accuracy: 0.665\n",
      "iteration 0 250\t loss: 1.099, accuracy: 0.668\n",
      "iteration 0 260\t loss: 1.095, accuracy: 0.672\n",
      "iteration 0 270\t loss: 1.069, accuracy: 0.680\n",
      "iteration 0 280\t loss: 1.113, accuracy: 0.668\n",
      "iteration 1 0\t loss: 1.061, accuracy: 0.680\n",
      "iteration 1 10\t loss: 1.060, accuracy: 0.683\n",
      "iteration 1 20\t loss: 1.058, accuracy: 0.685\n",
      "iteration 1 30\t loss: 1.023, accuracy: 0.698\n",
      "iteration 1 40\t loss: 1.062, accuracy: 0.680\n",
      "iteration 1 50\t loss: 1.004, accuracy: 0.704\n",
      "iteration 1 60\t loss: 1.019, accuracy: 0.698\n",
      "iteration 1 70\t loss: 0.978, accuracy: 0.711\n",
      "iteration 1 80\t loss: 1.006, accuracy: 0.705\n",
      "iteration 1 90\t loss: 0.981, accuracy: 0.711\n",
      "iteration 1 100\t loss: 0.950, accuracy: 0.719\n",
      "iteration 1 110\t loss: 0.976, accuracy: 0.710\n",
      "iteration 1 120\t loss: 0.972, accuracy: 0.712\n",
      "iteration 1 130\t loss: 0.941, accuracy: 0.722\n",
      "iteration 1 140\t loss: 0.953, accuracy: 0.719\n",
      "iteration 1 150\t loss: 0.938, accuracy: 0.722\n",
      "iteration 1 160\t loss: 0.950, accuracy: 0.716\n",
      "iteration 1 170\t loss: 0.920, accuracy: 0.733\n",
      "iteration 1 180\t loss: 0.914, accuracy: 0.730\n",
      "iteration 1 190\t loss: 0.918, accuracy: 0.729\n",
      "iteration 1 200\t loss: 0.960, accuracy: 0.711\n",
      "iteration 1 210\t loss: 0.919, accuracy: 0.733\n",
      "iteration 1 220\t loss: 0.920, accuracy: 0.731\n",
      "iteration 1 230\t loss: 0.897, accuracy: 0.735\n",
      "iteration 1 240\t loss: 0.889, accuracy: 0.732\n",
      "iteration 1 250\t loss: 0.883, accuracy: 0.741\n",
      "iteration 1 260\t loss: 0.904, accuracy: 0.737\n",
      "iteration 1 270\t loss: 0.913, accuracy: 0.731\n",
      "iteration 1 280\t loss: 0.913, accuracy: 0.729\n",
      "iteration 2 0\t loss: 0.881, accuracy: 0.739\n",
      "iteration 2 10\t loss: 0.909, accuracy: 0.732\n",
      "iteration 2 20\t loss: 0.874, accuracy: 0.742\n",
      "iteration 2 30\t loss: 0.886, accuracy: 0.736\n",
      "iteration 2 40\t loss: 0.942, accuracy: 0.726\n",
      "iteration 2 50\t loss: 0.861, accuracy: 0.746\n",
      "iteration 2 60\t loss: 0.891, accuracy: 0.740\n",
      "iteration 2 70\t loss: 0.852, accuracy: 0.751\n",
      "iteration 2 80\t loss: 0.844, accuracy: 0.752\n",
      "iteration 2 90\t loss: 0.886, accuracy: 0.740\n",
      "iteration 2 100\t loss: 0.876, accuracy: 0.741\n",
      "iteration 2 110\t loss: 0.937, accuracy: 0.722\n",
      "iteration 2 120\t loss: 0.854, accuracy: 0.752\n",
      "iteration 2 130\t loss: 0.854, accuracy: 0.749\n",
      "iteration 2 140\t loss: 0.857, accuracy: 0.752\n",
      "iteration 2 150\t loss: 0.844, accuracy: 0.755\n",
      "iteration 2 160\t loss: 0.837, accuracy: 0.756\n",
      "iteration 2 170\t loss: 0.871, accuracy: 0.743\n",
      "iteration 2 180\t loss: 0.887, accuracy: 0.740\n",
      "iteration 2 190\t loss: 0.867, accuracy: 0.751\n",
      "iteration 2 200\t loss: 0.837, accuracy: 0.752\n",
      "iteration 2 210\t loss: 0.876, accuracy: 0.749\n",
      "iteration 2 220\t loss: 0.866, accuracy: 0.750\n",
      "iteration 2 230\t loss: 0.845, accuracy: 0.757\n",
      "iteration 2 240\t loss: 0.839, accuracy: 0.756\n",
      "iteration 2 250\t loss: 0.840, accuracy: 0.754\n",
      "iteration 2 260\t loss: 0.820, accuracy: 0.763\n",
      "iteration 2 270\t loss: 0.879, accuracy: 0.744\n",
      "iteration 2 280\t loss: 0.853, accuracy: 0.749\n",
      "iteration 3 0\t loss: 0.839, accuracy: 0.755\n",
      "iteration 3 10\t loss: 0.888, accuracy: 0.739\n",
      "iteration 3 20\t loss: 0.814, accuracy: 0.762\n",
      "iteration 3 30\t loss: 0.825, accuracy: 0.759\n",
      "iteration 3 40\t loss: 0.850, accuracy: 0.753\n",
      "iteration 3 50\t loss: 0.799, accuracy: 0.768\n",
      "iteration 3 60\t loss: 0.821, accuracy: 0.762\n",
      "iteration 3 70\t loss: 0.828, accuracy: 0.758\n",
      "iteration 3 80\t loss: 0.820, accuracy: 0.759\n",
      "iteration 3 90\t loss: 0.801, accuracy: 0.767\n",
      "iteration 3 100\t loss: 0.791, accuracy: 0.765\n",
      "iteration 3 110\t loss: 0.849, accuracy: 0.749\n",
      "iteration 3 120\t loss: 0.803, accuracy: 0.766\n",
      "iteration 3 130\t loss: 0.827, accuracy: 0.759\n",
      "iteration 3 140\t loss: 0.877, accuracy: 0.749\n",
      "iteration 3 150\t loss: 0.839, accuracy: 0.764\n",
      "iteration 3 160\t loss: 0.839, accuracy: 0.755\n",
      "iteration 3 170\t loss: 0.820, accuracy: 0.761\n",
      "iteration 3 180\t loss: 0.818, accuracy: 0.762\n",
      "iteration 3 190\t loss: 0.804, accuracy: 0.768\n",
      "iteration 3 200\t loss: 0.834, accuracy: 0.756\n",
      "iteration 3 210\t loss: 0.844, accuracy: 0.760\n",
      "iteration 3 220\t loss: 0.845, accuracy: 0.762\n",
      "iteration 3 230\t loss: 0.865, accuracy: 0.750\n",
      "iteration 3 240\t loss: 0.821, accuracy: 0.764\n",
      "iteration 3 250\t loss: 0.807, accuracy: 0.767\n",
      "iteration 3 260\t loss: 0.811, accuracy: 0.767\n",
      "iteration 3 270\t loss: 0.825, accuracy: 0.764\n",
      "iteration 3 280\t loss: 0.802, accuracy: 0.766\n",
      "iteration 4 0\t loss: 0.831, accuracy: 0.761\n",
      "iteration 4 10\t loss: 0.880, accuracy: 0.749\n",
      "iteration 4 20\t loss: 0.841, accuracy: 0.755\n",
      "iteration 4 30\t loss: 0.824, accuracy: 0.762\n",
      "iteration 4 40\t loss: 0.840, accuracy: 0.764\n",
      "iteration 4 50\t loss: 0.814, accuracy: 0.764\n",
      "iteration 4 60\t loss: 0.803, accuracy: 0.769\n",
      "iteration 4 70\t loss: 0.774, accuracy: 0.773\n",
      "iteration 4 80\t loss: 0.777, accuracy: 0.774\n",
      "iteration 4 90\t loss: 0.768, accuracy: 0.778\n",
      "iteration 4 100\t loss: 0.811, accuracy: 0.762\n",
      "iteration 4 110\t loss: 0.826, accuracy: 0.761\n",
      "iteration 4 120\t loss: 0.782, accuracy: 0.774\n",
      "iteration 4 130\t loss: 0.788, accuracy: 0.773\n",
      "iteration 4 140\t loss: 0.828, accuracy: 0.765\n",
      "iteration 4 150\t loss: 0.819, accuracy: 0.771\n",
      "iteration 4 160\t loss: 0.808, accuracy: 0.769\n",
      "iteration 4 170\t loss: 0.823, accuracy: 0.764\n",
      "iteration 4 180\t loss: 0.798, accuracy: 0.772\n",
      "iteration 4 190\t loss: 0.793, accuracy: 0.776\n",
      "iteration 4 200\t loss: 0.840, accuracy: 0.760\n",
      "iteration 4 210\t loss: 0.863, accuracy: 0.756\n",
      "iteration 4 220\t loss: 0.801, accuracy: 0.774\n",
      "iteration 4 230\t loss: 0.830, accuracy: 0.764\n",
      "iteration 4 240\t loss: 0.784, accuracy: 0.776\n",
      "iteration 4 250\t loss: 0.768, accuracy: 0.779\n",
      "iteration 4 260\t loss: 0.796, accuracy: 0.775\n",
      "iteration 4 270\t loss: 0.820, accuracy: 0.772\n",
      "iteration 4 280\t loss: 0.790, accuracy: 0.773\n",
      "iteration 5 0\t loss: 0.855, accuracy: 0.759\n",
      "iteration 5 10\t loss: 0.881, accuracy: 0.755\n",
      "iteration 5 20\t loss: 0.813, accuracy: 0.767\n",
      "iteration 5 30\t loss: 0.845, accuracy: 0.758\n",
      "iteration 5 40\t loss: 0.820, accuracy: 0.768\n",
      "iteration 5 50\t loss: 0.776, accuracy: 0.776\n",
      "iteration 5 60\t loss: 0.774, accuracy: 0.779\n",
      "iteration 5 70\t loss: 0.775, accuracy: 0.777\n",
      "iteration 5 80\t loss: 0.771, accuracy: 0.779\n",
      "iteration 5 90\t loss: 0.759, accuracy: 0.779\n",
      "iteration 5 100\t loss: 0.774, accuracy: 0.779\n",
      "iteration 5 110\t loss: 0.785, accuracy: 0.773\n",
      "iteration 5 120\t loss: 0.775, accuracy: 0.778\n",
      "iteration 5 130\t loss: 0.826, accuracy: 0.764\n",
      "iteration 5 140\t loss: 0.814, accuracy: 0.770\n",
      "iteration 5 150\t loss: 0.798, accuracy: 0.776\n",
      "iteration 5 160\t loss: 0.770, accuracy: 0.779\n",
      "iteration 5 170\t loss: 0.797, accuracy: 0.775\n",
      "iteration 5 180\t loss: 0.777, accuracy: 0.777\n",
      "iteration 5 190\t loss: 0.776, accuracy: 0.783\n",
      "iteration 5 200\t loss: 0.802, accuracy: 0.775\n",
      "iteration 5 210\t loss: 0.828, accuracy: 0.769\n",
      "iteration 5 220\t loss: 0.780, accuracy: 0.781\n",
      "iteration 5 230\t loss: 0.836, accuracy: 0.765\n",
      "iteration 5 240\t loss: 0.792, accuracy: 0.777\n",
      "iteration 5 250\t loss: 0.774, accuracy: 0.781\n",
      "iteration 5 260\t loss: 0.811, accuracy: 0.772\n",
      "iteration 5 270\t loss: 0.861, accuracy: 0.764\n",
      "iteration 5 280\t loss: 0.794, accuracy: 0.775\n",
      "iteration 6 0\t loss: 0.912, accuracy: 0.748\n",
      "iteration 6 10\t loss: 0.804, accuracy: 0.777\n",
      "iteration 6 20\t loss: 0.794, accuracy: 0.772\n",
      "iteration 6 30\t loss: 0.786, accuracy: 0.774\n",
      "iteration 6 40\t loss: 0.803, accuracy: 0.772\n",
      "iteration 6 50\t loss: 0.772, accuracy: 0.778\n",
      "iteration 6 60\t loss: 0.752, accuracy: 0.785\n",
      "iteration 6 70\t loss: 0.776, accuracy: 0.775\n",
      "iteration 6 80\t loss: 0.784, accuracy: 0.776\n",
      "iteration 6 90\t loss: 0.757, accuracy: 0.782\n",
      "iteration 6 100\t loss: 0.758, accuracy: 0.787\n",
      "iteration 6 110\t loss: 0.792, accuracy: 0.777\n",
      "iteration 6 120\t loss: 0.772, accuracy: 0.782\n",
      "iteration 6 130\t loss: 0.815, accuracy: 0.771\n",
      "iteration 6 140\t loss: 0.781, accuracy: 0.782\n",
      "iteration 6 150\t loss: 0.836, accuracy: 0.765\n",
      "iteration 6 160\t loss: 0.792, accuracy: 0.778\n",
      "iteration 6 170\t loss: 0.810, accuracy: 0.776\n",
      "iteration 6 180\t loss: 0.794, accuracy: 0.775\n",
      "iteration 6 190\t loss: 0.768, accuracy: 0.787\n",
      "iteration 6 200\t loss: 0.804, accuracy: 0.778\n",
      "iteration 6 210\t loss: 0.832, accuracy: 0.772\n",
      "iteration 6 220\t loss: 0.789, accuracy: 0.782\n",
      "iteration 6 230\t loss: 0.835, accuracy: 0.769\n",
      "iteration 6 240\t loss: 0.769, accuracy: 0.784\n",
      "iteration 6 250\t loss: 0.785, accuracy: 0.777\n",
      "iteration 6 260\t loss: 0.801, accuracy: 0.780\n",
      "iteration 6 270\t loss: 0.811, accuracy: 0.779\n",
      "iteration 6 280\t loss: 0.791, accuracy: 0.778\n",
      "iteration 7 0\t loss: 0.890, accuracy: 0.757\n",
      "iteration 7 10\t loss: 0.795, accuracy: 0.778\n",
      "iteration 7 20\t loss: 0.799, accuracy: 0.769\n",
      "iteration 7 30\t loss: 0.801, accuracy: 0.773\n",
      "iteration 7 40\t loss: 0.833, accuracy: 0.770\n",
      "iteration 7 50\t loss: 0.799, accuracy: 0.777\n",
      "iteration 7 60\t loss: 0.782, accuracy: 0.781\n",
      "iteration 7 70\t loss: 0.747, accuracy: 0.788\n",
      "iteration 7 80\t loss: 0.781, accuracy: 0.784\n",
      "iteration 7 90\t loss: 0.762, accuracy: 0.783\n",
      "iteration 7 100\t loss: 0.774, accuracy: 0.786\n",
      "iteration 7 110\t loss: 0.780, accuracy: 0.782\n",
      "iteration 7 120\t loss: 0.768, accuracy: 0.785\n",
      "iteration 7 130\t loss: 0.785, accuracy: 0.779\n",
      "iteration 7 140\t loss: 0.839, accuracy: 0.763\n",
      "iteration 7 150\t loss: 0.805, accuracy: 0.776\n",
      "iteration 7 160\t loss: 0.804, accuracy: 0.778\n",
      "iteration 7 170\t loss: 0.845, accuracy: 0.772\n",
      "iteration 7 180\t loss: 0.783, accuracy: 0.780\n",
      "iteration 7 190\t loss: 0.768, accuracy: 0.784\n",
      "iteration 7 200\t loss: 0.779, accuracy: 0.781\n",
      "iteration 7 210\t loss: 0.847, accuracy: 0.771\n",
      "iteration 7 220\t loss: 0.783, accuracy: 0.785\n",
      "iteration 7 230\t loss: 0.834, accuracy: 0.772\n",
      "iteration 7 240\t loss: 0.800, accuracy: 0.777\n",
      "iteration 7 250\t loss: 0.784, accuracy: 0.779\n",
      "iteration 7 260\t loss: 0.844, accuracy: 0.771\n",
      "iteration 7 270\t loss: 0.821, accuracy: 0.780\n",
      "iteration 7 280\t loss: 0.823, accuracy: 0.775\n",
      "iteration 8 0\t loss: 0.899, accuracy: 0.762\n",
      "iteration 8 10\t loss: 0.785, accuracy: 0.782\n",
      "iteration 8 20\t loss: 0.803, accuracy: 0.772\n",
      "iteration 8 30\t loss: 0.810, accuracy: 0.776\n",
      "iteration 8 40\t loss: 0.804, accuracy: 0.781\n",
      "iteration 8 50\t loss: 0.810, accuracy: 0.777\n",
      "iteration 8 60\t loss: 0.803, accuracy: 0.784\n",
      "iteration 8 70\t loss: 0.757, accuracy: 0.783\n",
      "iteration 8 80\t loss: 0.780, accuracy: 0.785\n",
      "iteration 8 90\t loss: 0.775, accuracy: 0.784\n",
      "iteration 8 100\t loss: 0.774, accuracy: 0.787\n",
      "iteration 8 110\t loss: 0.791, accuracy: 0.782\n",
      "iteration 8 120\t loss: 0.806, accuracy: 0.773\n",
      "iteration 8 130\t loss: 0.800, accuracy: 0.773\n",
      "iteration 8 140\t loss: 0.822, accuracy: 0.769\n",
      "iteration 8 150\t loss: 0.828, accuracy: 0.779\n",
      "iteration 8 160\t loss: 0.870, accuracy: 0.766\n",
      "iteration 8 170\t loss: 0.837, accuracy: 0.778\n",
      "iteration 8 180\t loss: 0.814, accuracy: 0.772\n",
      "iteration 8 190\t loss: 0.783, accuracy: 0.783\n",
      "iteration 8 200\t loss: 0.815, accuracy: 0.779\n",
      "iteration 8 210\t loss: 0.824, accuracy: 0.775\n",
      "iteration 8 220\t loss: 0.795, accuracy: 0.781\n",
      "iteration 8 230\t loss: 0.844, accuracy: 0.771\n",
      "iteration 8 240\t loss: 0.835, accuracy: 0.767\n",
      "iteration 8 250\t loss: 0.794, accuracy: 0.786\n",
      "iteration 8 260\t loss: 0.835, accuracy: 0.779\n",
      "iteration 8 270\t loss: 0.819, accuracy: 0.776\n",
      "iteration 8 280\t loss: 0.834, accuracy: 0.778\n",
      "iteration 9 0\t loss: 0.944, accuracy: 0.754\n",
      "iteration 9 10\t loss: 0.849, accuracy: 0.768\n",
      "iteration 9 20\t loss: 0.835, accuracy: 0.768\n",
      "iteration 9 30\t loss: 0.839, accuracy: 0.774\n",
      "iteration 9 40\t loss: 0.797, accuracy: 0.780\n",
      "iteration 9 50\t loss: 0.830, accuracy: 0.777\n",
      "iteration 9 60\t loss: 0.811, accuracy: 0.784\n",
      "iteration 9 70\t loss: 0.767, accuracy: 0.785\n",
      "iteration 9 80\t loss: 0.802, accuracy: 0.784\n",
      "iteration 9 90\t loss: 0.795, accuracy: 0.782\n",
      "iteration 9 100\t loss: 0.844, accuracy: 0.776\n",
      "iteration 9 110\t loss: 0.882, accuracy: 0.764\n",
      "iteration 9 120\t loss: 0.850, accuracy: 0.772\n",
      "iteration 9 130\t loss: 0.792, accuracy: 0.780\n",
      "iteration 9 140\t loss: 0.803, accuracy: 0.780\n",
      "iteration 9 150\t loss: 0.831, accuracy: 0.774\n",
      "iteration 9 160\t loss: 0.832, accuracy: 0.779\n",
      "iteration 9 170\t loss: 0.843, accuracy: 0.777\n",
      "iteration 9 180\t loss: 0.805, accuracy: 0.778\n",
      "iteration 9 190\t loss: 0.778, accuracy: 0.787\n",
      "iteration 9 200\t loss: 0.806, accuracy: 0.781\n",
      "iteration 9 210\t loss: 0.828, accuracy: 0.775\n",
      "iteration 9 220\t loss: 0.787, accuracy: 0.785\n",
      "iteration 9 230\t loss: 0.836, accuracy: 0.776\n",
      "iteration 9 240\t loss: 0.831, accuracy: 0.773\n",
      "iteration 9 250\t loss: 0.800, accuracy: 0.783\n",
      "iteration 9 260\t loss: 0.826, accuracy: 0.781\n",
      "iteration 9 270\t loss: 0.820, accuracy: 0.783\n",
      "iteration 9 280\t loss: 0.858, accuracy: 0.775\n",
      "iteration 10 0\t loss: 0.894, accuracy: 0.769\n",
      "iteration 10 10\t loss: 0.901, accuracy: 0.761\n",
      "iteration 10 20\t loss: 0.841, accuracy: 0.774\n",
      "iteration 10 30\t loss: 0.815, accuracy: 0.775\n",
      "iteration 10 40\t loss: 0.822, accuracy: 0.776\n",
      "iteration 10 50\t loss: 0.850, accuracy: 0.777\n",
      "iteration 10 60\t loss: 0.824, accuracy: 0.783\n",
      "iteration 10 70\t loss: 0.786, accuracy: 0.779\n",
      "iteration 10 80\t loss: 0.806, accuracy: 0.782\n",
      "iteration 10 90\t loss: 0.874, accuracy: 0.769\n",
      "iteration 10 100\t loss: 0.873, accuracy: 0.771\n",
      "iteration 10 110\t loss: 1.134, accuracy: 0.717\n",
      "iteration 10 120\t loss: 0.838, accuracy: 0.776\n",
      "iteration 10 130\t loss: 0.793, accuracy: 0.790\n",
      "iteration 10 140\t loss: 0.875, accuracy: 0.762\n",
      "iteration 10 150\t loss: 0.867, accuracy: 0.772\n",
      "iteration 10 160\t loss: 0.833, accuracy: 0.776\n",
      "iteration 10 170\t loss: 0.839, accuracy: 0.774\n",
      "iteration 10 180\t loss: 0.822, accuracy: 0.776\n",
      "iteration 10 190\t loss: 0.796, accuracy: 0.784\n",
      "iteration 10 200\t loss: 0.848, accuracy: 0.772\n",
      "iteration 10 210\t loss: 0.839, accuracy: 0.776\n",
      "iteration 10 220\t loss: 0.819, accuracy: 0.779\n",
      "iteration 10 230\t loss: 0.831, accuracy: 0.780\n",
      "iteration 10 240\t loss: 0.817, accuracy: 0.780\n",
      "iteration 10 250\t loss: 0.818, accuracy: 0.784\n",
      "iteration 10 260\t loss: 0.892, accuracy: 0.777\n",
      "iteration 10 270\t loss: 0.862, accuracy: 0.770\n",
      "iteration 10 280\t loss: 0.864, accuracy: 0.779\n",
      "iteration 11 0\t loss: 0.885, accuracy: 0.773\n",
      "iteration 11 10\t loss: 0.900, accuracy: 0.767\n",
      "iteration 11 20\t loss: 0.891, accuracy: 0.772\n",
      "iteration 11 30\t loss: 0.844, accuracy: 0.772\n",
      "iteration 11 40\t loss: 0.868, accuracy: 0.773\n",
      "iteration 11 50\t loss: 0.871, accuracy: 0.773\n",
      "iteration 11 60\t loss: 0.861, accuracy: 0.777\n",
      "iteration 11 70\t loss: 0.803, accuracy: 0.782\n",
      "iteration 11 80\t loss: 0.835, accuracy: 0.783\n",
      "iteration 11 90\t loss: 0.867, accuracy: 0.776\n",
      "iteration 11 100\t loss: 0.907, accuracy: 0.769\n",
      "iteration 11 110\t loss: 1.088, accuracy: 0.742\n",
      "iteration 11 120\t loss: 0.863, accuracy: 0.780\n",
      "iteration 11 130\t loss: 0.813, accuracy: 0.783\n",
      "iteration 11 140\t loss: 0.849, accuracy: 0.776\n",
      "iteration 11 150\t loss: 0.900, accuracy: 0.775\n",
      "iteration 11 160\t loss: 0.852, accuracy: 0.777\n",
      "iteration 11 170\t loss: 0.846, accuracy: 0.779\n",
      "iteration 11 180\t loss: 0.849, accuracy: 0.776\n",
      "iteration 11 190\t loss: 0.811, accuracy: 0.787\n",
      "iteration 11 200\t loss: 0.904, accuracy: 0.773\n",
      "iteration 11 210\t loss: 0.834, accuracy: 0.778\n",
      "iteration 11 220\t loss: 0.880, accuracy: 0.770\n",
      "iteration 11 230\t loss: 0.879, accuracy: 0.770\n",
      "iteration 11 240\t loss: 0.838, accuracy: 0.784\n",
      "iteration 11 250\t loss: 0.840, accuracy: 0.786\n",
      "iteration 11 260\t loss: 1.009, accuracy: 0.765\n",
      "iteration 11 270\t loss: 0.892, accuracy: 0.774\n",
      "iteration 11 280\t loss: 0.927, accuracy: 0.770\n",
      "iteration 12 0\t loss: 0.938, accuracy: 0.769\n",
      "iteration 12 10\t loss: 0.940, accuracy: 0.770\n",
      "iteration 12 20\t loss: 0.924, accuracy: 0.776\n",
      "iteration 12 30\t loss: 0.870, accuracy: 0.766\n",
      "iteration 12 40\t loss: 0.939, accuracy: 0.766\n",
      "iteration 12 50\t loss: 0.918, accuracy: 0.772\n",
      "iteration 12 60\t loss: 0.881, accuracy: 0.782\n",
      "iteration 12 70\t loss: 0.853, accuracy: 0.774\n",
      "iteration 12 80\t loss: 0.864, accuracy: 0.780\n",
      "iteration 12 90\t loss: 0.949, accuracy: 0.761\n",
      "iteration 12 100\t loss: 0.902, accuracy: 0.776\n",
      "iteration 12 110\t loss: 0.980, accuracy: 0.760\n",
      "iteration 12 120\t loss: 0.927, accuracy: 0.779\n",
      "iteration 12 130\t loss: 0.874, accuracy: 0.773\n",
      "iteration 12 140\t loss: 0.855, accuracy: 0.775\n",
      "iteration 12 150\t loss: 0.852, accuracy: 0.782\n",
      "iteration 12 160\t loss: 0.855, accuracy: 0.782\n",
      "iteration 12 170\t loss: 0.838, accuracy: 0.785\n",
      "iteration 12 180\t loss: 0.830, accuracy: 0.779\n",
      "iteration 12 190\t loss: 0.827, accuracy: 0.782\n",
      "iteration 12 200\t loss: 0.927, accuracy: 0.770\n",
      "iteration 12 210\t loss: 0.861, accuracy: 0.780\n",
      "iteration 12 220\t loss: 0.869, accuracy: 0.780\n",
      "iteration 12 230\t loss: 0.877, accuracy: 0.784\n",
      "iteration 12 240\t loss: 0.855, accuracy: 0.783\n",
      "iteration 12 250\t loss: 0.848, accuracy: 0.786\n",
      "iteration 12 260\t loss: 0.895, accuracy: 0.777\n",
      "iteration 12 270\t loss: 0.958, accuracy: 0.773\n",
      "iteration 12 280\t loss: 0.912, accuracy: 0.777\n",
      "iteration 13 0\t loss: 0.973, accuracy: 0.768\n",
      "iteration 13 10\t loss: 0.891, accuracy: 0.779\n",
      "iteration 13 20\t loss: 0.898, accuracy: 0.772\n",
      "iteration 13 30\t loss: 0.882, accuracy: 0.775\n",
      "iteration 13 40\t loss: 0.893, accuracy: 0.770\n",
      "iteration 13 50\t loss: 0.927, accuracy: 0.772\n",
      "iteration 13 60\t loss: 0.869, accuracy: 0.782\n",
      "iteration 13 70\t loss: 0.842, accuracy: 0.783\n",
      "iteration 13 80\t loss: 0.909, accuracy: 0.775\n",
      "iteration 13 90\t loss: 0.878, accuracy: 0.780\n",
      "iteration 13 100\t loss: 0.873, accuracy: 0.786\n",
      "iteration 13 110\t loss: 1.024, accuracy: 0.749\n",
      "iteration 13 120\t loss: 1.002, accuracy: 0.763\n",
      "iteration 13 130\t loss: 0.971, accuracy: 0.757\n",
      "iteration 13 140\t loss: 0.863, accuracy: 0.784\n",
      "iteration 13 150\t loss: 0.839, accuracy: 0.786\n",
      "iteration 13 160\t loss: 0.864, accuracy: 0.783\n",
      "iteration 13 170\t loss: 0.880, accuracy: 0.779\n",
      "iteration 13 180\t loss: 0.864, accuracy: 0.773\n",
      "iteration 13 190\t loss: 0.841, accuracy: 0.783\n",
      "iteration 13 200\t loss: 0.974, accuracy: 0.766\n",
      "iteration 13 210\t loss: 0.911, accuracy: 0.767\n",
      "iteration 13 220\t loss: 0.918, accuracy: 0.776\n",
      "iteration 13 230\t loss: 0.862, accuracy: 0.789\n",
      "iteration 13 240\t loss: 0.874, accuracy: 0.783\n",
      "iteration 13 250\t loss: 0.857, accuracy: 0.787\n",
      "iteration 13 260\t loss: 0.883, accuracy: 0.780\n",
      "iteration 13 270\t loss: 1.060, accuracy: 0.743\n",
      "iteration 13 280\t loss: 0.930, accuracy: 0.778\n",
      "iteration 14 0\t loss: 0.971, accuracy: 0.766\n",
      "iteration 14 10\t loss: 0.932, accuracy: 0.782\n",
      "iteration 14 20\t loss: 0.901, accuracy: 0.781\n",
      "iteration 14 30\t loss: 0.909, accuracy: 0.776\n",
      "iteration 14 40\t loss: 0.892, accuracy: 0.781\n",
      "iteration 14 50\t loss: 0.916, accuracy: 0.782\n",
      "iteration 14 60\t loss: 0.942, accuracy: 0.775\n",
      "iteration 14 70\t loss: 0.871, accuracy: 0.777\n",
      "iteration 14 80\t loss: 0.952, accuracy: 0.774\n",
      "iteration 14 90\t loss: 0.998, accuracy: 0.759\n",
      "iteration 14 100\t loss: 0.843, accuracy: 0.788\n",
      "iteration 14 110\t loss: 1.076, accuracy: 0.755\n",
      "iteration 14 120\t loss: 1.032, accuracy: 0.740\n",
      "iteration 14 130\t loss: 1.027, accuracy: 0.761\n",
      "iteration 14 140\t loss: 0.938, accuracy: 0.766\n",
      "iteration 14 150\t loss: 0.899, accuracy: 0.784\n",
      "iteration 14 160\t loss: 0.896, accuracy: 0.785\n",
      "iteration 14 170\t loss: 0.890, accuracy: 0.787\n",
      "iteration 14 180\t loss: 0.883, accuracy: 0.780\n",
      "iteration 14 190\t loss: 0.875, accuracy: 0.783\n",
      "iteration 14 200\t loss: 0.956, accuracy: 0.777\n",
      "iteration 14 210\t loss: 0.907, accuracy: 0.768\n",
      "iteration 14 220\t loss: 0.960, accuracy: 0.773\n",
      "iteration 14 230\t loss: 0.921, accuracy: 0.780\n",
      "iteration 14 240\t loss: 0.920, accuracy: 0.777\n",
      "iteration 14 250\t loss: 0.873, accuracy: 0.793\n",
      "iteration 14 260\t loss: 0.898, accuracy: 0.789\n",
      "iteration 14 270\t loss: 1.046, accuracy: 0.765\n",
      "iteration 14 280\t loss: 1.049, accuracy: 0.769\n",
      "iteration 15 0\t loss: 1.013, accuracy: 0.766\n",
      "iteration 15 10\t loss: 1.061, accuracy: 0.770\n",
      "iteration 15 20\t loss: 0.915, accuracy: 0.777\n",
      "iteration 15 30\t loss: 0.976, accuracy: 0.774\n",
      "iteration 15 40\t loss: 0.991, accuracy: 0.775\n",
      "iteration 15 50\t loss: 0.917, accuracy: 0.784\n",
      "iteration 15 60\t loss: 0.988, accuracy: 0.777\n",
      "iteration 15 70\t loss: 0.976, accuracy: 0.765\n",
      "iteration 15 80\t loss: 1.007, accuracy: 0.771\n",
      "iteration 15 90\t loss: 0.933, accuracy: 0.778\n",
      "iteration 15 100\t loss: 0.897, accuracy: 0.781\n",
      "iteration 15 110\t loss: 1.049, accuracy: 0.764\n",
      "iteration 15 120\t loss: 1.046, accuracy: 0.728\n",
      "iteration 15 130\t loss: 0.970, accuracy: 0.772\n",
      "iteration 15 140\t loss: 0.927, accuracy: 0.763\n",
      "iteration 15 150\t loss: 0.942, accuracy: 0.776\n",
      "iteration 15 160\t loss: 0.982, accuracy: 0.768\n",
      "iteration 15 170\t loss: 0.889, accuracy: 0.783\n",
      "iteration 15 180\t loss: 0.856, accuracy: 0.784\n",
      "iteration 15 190\t loss: 0.907, accuracy: 0.788\n",
      "iteration 15 200\t loss: 0.957, accuracy: 0.783\n",
      "iteration 15 210\t loss: 0.902, accuracy: 0.783\n",
      "iteration 15 220\t loss: 0.969, accuracy: 0.774\n",
      "iteration 15 230\t loss: 0.871, accuracy: 0.790\n",
      "iteration 15 240\t loss: 0.952, accuracy: 0.784\n",
      "iteration 15 250\t loss: 0.925, accuracy: 0.793\n",
      "iteration 15 260\t loss: 1.155, accuracy: 0.763\n",
      "iteration 15 270\t loss: 1.231, accuracy: 0.757\n",
      "iteration 15 280\t loss: 1.053, accuracy: 0.778\n",
      "iteration 16 0\t loss: 1.104, accuracy: 0.761\n",
      "iteration 16 10\t loss: 1.079, accuracy: 0.774\n",
      "iteration 16 20\t loss: 0.953, accuracy: 0.772\n",
      "iteration 16 30\t loss: 0.977, accuracy: 0.772\n",
      "iteration 16 40\t loss: 1.031, accuracy: 0.767\n",
      "iteration 16 50\t loss: 0.972, accuracy: 0.777\n",
      "iteration 16 60\t loss: 0.976, accuracy: 0.778\n",
      "iteration 16 70\t loss: 0.997, accuracy: 0.770\n",
      "iteration 16 80\t loss: 1.053, accuracy: 0.764\n",
      "iteration 16 90\t loss: 0.925, accuracy: 0.781\n",
      "iteration 16 100\t loss: 0.974, accuracy: 0.772\n",
      "iteration 16 110\t loss: 1.197, accuracy: 0.752\n",
      "iteration 16 120\t loss: 1.084, accuracy: 0.735\n",
      "iteration 16 130\t loss: 1.078, accuracy: 0.765\n",
      "iteration 16 140\t loss: 1.017, accuracy: 0.759\n",
      "iteration 16 150\t loss: 0.973, accuracy: 0.774\n",
      "iteration 16 160\t loss: 0.975, accuracy: 0.775\n",
      "iteration 16 170\t loss: 0.874, accuracy: 0.787\n",
      "iteration 16 180\t loss: 0.944, accuracy: 0.786\n",
      "iteration 16 190\t loss: 0.950, accuracy: 0.780\n",
      "iteration 16 200\t loss: 0.963, accuracy: 0.781\n",
      "iteration 16 210\t loss: 0.976, accuracy: 0.784\n",
      "iteration 16 220\t loss: 0.964, accuracy: 0.778\n",
      "iteration 16 230\t loss: 0.898, accuracy: 0.793\n",
      "iteration 16 240\t loss: 0.987, accuracy: 0.785\n",
      "iteration 16 250\t loss: 0.963, accuracy: 0.791\n",
      "iteration 16 260\t loss: 1.060, accuracy: 0.773\n",
      "iteration 16 270\t loss: 1.170, accuracy: 0.774\n",
      "iteration 16 280\t loss: 1.081, accuracy: 0.778\n",
      "iteration 17 0\t loss: 1.136, accuracy: 0.770\n",
      "iteration 17 10\t loss: 1.098, accuracy: 0.772\n",
      "iteration 17 20\t loss: 1.039, accuracy: 0.764\n",
      "iteration 17 30\t loss: 1.009, accuracy: 0.774\n",
      "iteration 17 40\t loss: 1.101, accuracy: 0.766\n",
      "iteration 17 50\t loss: 1.049, accuracy: 0.774\n",
      "iteration 17 60\t loss: 0.992, accuracy: 0.782\n",
      "iteration 17 70\t loss: 1.108, accuracy: 0.761\n",
      "iteration 17 80\t loss: 1.034, accuracy: 0.769\n",
      "iteration 17 90\t loss: 1.027, accuracy: 0.775\n",
      "iteration 17 100\t loss: 0.984, accuracy: 0.769\n",
      "iteration 17 110\t loss: 1.162, accuracy: 0.765\n",
      "iteration 17 120\t loss: 1.246, accuracy: 0.744\n",
      "iteration 17 130\t loss: 1.230, accuracy: 0.760\n",
      "iteration 17 140\t loss: 0.940, accuracy: 0.771\n",
      "iteration 17 150\t loss: 0.969, accuracy: 0.781\n",
      "iteration 17 160\t loss: 0.972, accuracy: 0.782\n",
      "iteration 17 170\t loss: 0.909, accuracy: 0.790\n",
      "iteration 17 180\t loss: 0.947, accuracy: 0.791\n",
      "iteration 17 190\t loss: 0.882, accuracy: 0.795\n",
      "iteration 17 200\t loss: 0.914, accuracy: 0.793\n",
      "iteration 17 210\t loss: 0.972, accuracy: 0.790\n",
      "iteration 17 220\t loss: 1.012, accuracy: 0.783\n",
      "iteration 17 230\t loss: 0.951, accuracy: 0.786\n",
      "iteration 17 240\t loss: 1.013, accuracy: 0.794\n",
      "iteration 17 250\t loss: 1.047, accuracy: 0.789\n",
      "iteration 17 260\t loss: 1.078, accuracy: 0.781\n",
      "iteration 17 270\t loss: 1.108, accuracy: 0.769\n",
      "iteration 17 280\t loss: 1.153, accuracy: 0.761\n",
      "iteration 18 0\t loss: 1.249, accuracy: 0.750\n",
      "iteration 18 10\t loss: 1.102, accuracy: 0.765\n",
      "iteration 18 20\t loss: 1.082, accuracy: 0.761\n",
      "iteration 18 30\t loss: 0.998, accuracy: 0.775\n",
      "iteration 18 40\t loss: 1.064, accuracy: 0.777\n",
      "iteration 18 50\t loss: 1.023, accuracy: 0.777\n",
      "iteration 18 60\t loss: 0.967, accuracy: 0.788\n",
      "iteration 18 70\t loss: 0.989, accuracy: 0.792\n",
      "iteration 18 80\t loss: 0.982, accuracy: 0.786\n",
      "iteration 18 90\t loss: 0.993, accuracy: 0.786\n",
      "iteration 18 100\t loss: 1.033, accuracy: 0.780\n",
      "iteration 18 110\t loss: 1.085, accuracy: 0.779\n",
      "iteration 18 120\t loss: 1.262, accuracy: 0.761\n",
      "iteration 18 130\t loss: 1.082, accuracy: 0.788\n",
      "iteration 18 140\t loss: 1.001, accuracy: 0.764\n",
      "iteration 18 150\t loss: 0.960, accuracy: 0.787\n",
      "iteration 18 160\t loss: 0.961, accuracy: 0.775\n",
      "iteration 18 170\t loss: 0.923, accuracy: 0.790\n",
      "iteration 18 180\t loss: 0.966, accuracy: 0.790\n",
      "iteration 18 190\t loss: 0.905, accuracy: 0.804\n",
      "iteration 18 200\t loss: 0.917, accuracy: 0.795\n",
      "iteration 18 210\t loss: 0.953, accuracy: 0.803\n",
      "iteration 18 220\t loss: 1.021, accuracy: 0.790\n",
      "iteration 18 230\t loss: 0.979, accuracy: 0.789\n",
      "iteration 18 240\t loss: 0.933, accuracy: 0.797\n",
      "iteration 18 250\t loss: 1.013, accuracy: 0.793\n",
      "iteration 18 260\t loss: 1.208, accuracy: 0.769\n",
      "iteration 18 270\t loss: 1.081, accuracy: 0.775\n",
      "iteration 18 280\t loss: 1.202, accuracy: 0.769\n",
      "iteration 19 0\t loss: 1.299, accuracy: 0.752\n",
      "iteration 19 10\t loss: 1.142, accuracy: 0.771\n",
      "iteration 19 20\t loss: 1.060, accuracy: 0.771\n",
      "iteration 19 30\t loss: 1.013, accuracy: 0.787\n",
      "iteration 19 40\t loss: 1.115, accuracy: 0.779\n",
      "iteration 19 50\t loss: 1.028, accuracy: 0.788\n",
      "iteration 19 60\t loss: 0.964, accuracy: 0.781\n",
      "iteration 19 70\t loss: 1.025, accuracy: 0.793\n",
      "iteration 19 80\t loss: 1.092, accuracy: 0.781\n",
      "iteration 19 90\t loss: 1.024, accuracy: 0.786\n",
      "iteration 19 100\t loss: 1.122, accuracy: 0.776\n",
      "iteration 19 110\t loss: 1.054, accuracy: 0.792\n",
      "iteration 19 120\t loss: 1.176, accuracy: 0.758\n",
      "iteration 19 130\t loss: 1.055, accuracy: 0.781\n",
      "iteration 19 140\t loss: 1.042, accuracy: 0.778\n",
      "iteration 19 150\t loss: 1.003, accuracy: 0.793\n",
      "iteration 19 160\t loss: 0.904, accuracy: 0.783\n",
      "iteration 19 170\t loss: 0.934, accuracy: 0.796\n",
      "iteration 19 180\t loss: 0.956, accuracy: 0.794\n",
      "iteration 19 190\t loss: 0.901, accuracy: 0.801\n",
      "iteration 19 200\t loss: 0.887, accuracy: 0.804\n",
      "iteration 19 210\t loss: 0.952, accuracy: 0.806\n",
      "iteration 19 220\t loss: 0.955, accuracy: 0.800\n",
      "iteration 19 230\t loss: 0.969, accuracy: 0.789\n",
      "iteration 19 240\t loss: 0.978, accuracy: 0.798\n",
      "iteration 19 250\t loss: 1.009, accuracy: 0.799\n",
      "iteration 19 260\t loss: 1.121, accuracy: 0.786\n",
      "iteration 19 270\t loss: 1.090, accuracy: 0.779\n",
      "iteration 19 280\t loss: 1.376, accuracy: 0.766\n",
      "iteration 20 0\t loss: 1.179, accuracy: 0.768\n",
      "iteration 20 10\t loss: 1.122, accuracy: 0.779\n",
      "iteration 20 20\t loss: 1.123, accuracy: 0.778\n",
      "iteration 20 30\t loss: 1.089, accuracy: 0.778\n",
      "iteration 20 40\t loss: 1.208, accuracy: 0.773\n",
      "iteration 20 50\t loss: 1.099, accuracy: 0.772\n",
      "iteration 20 60\t loss: 1.045, accuracy: 0.766\n",
      "iteration 20 70\t loss: 1.169, accuracy: 0.789\n",
      "iteration 20 80\t loss: 1.102, accuracy: 0.778\n",
      "iteration 20 90\t loss: 1.139, accuracy: 0.789\n",
      "iteration 20 100\t loss: 1.189, accuracy: 0.779\n",
      "iteration 20 110\t loss: 1.136, accuracy: 0.788\n",
      "iteration 20 120\t loss: 1.206, accuracy: 0.761\n",
      "iteration 20 130\t loss: 1.130, accuracy: 0.789\n",
      "iteration 20 140\t loss: 1.106, accuracy: 0.776\n",
      "iteration 20 150\t loss: 1.003, accuracy: 0.798\n",
      "iteration 20 160\t loss: 0.963, accuracy: 0.787\n",
      "iteration 20 170\t loss: 0.953, accuracy: 0.800\n",
      "iteration 20 180\t loss: 1.019, accuracy: 0.796\n",
      "iteration 20 190\t loss: 0.950, accuracy: 0.806\n",
      "iteration 20 200\t loss: 0.911, accuracy: 0.805\n",
      "iteration 20 210\t loss: 1.018, accuracy: 0.804\n",
      "iteration 20 220\t loss: 0.995, accuracy: 0.793\n",
      "iteration 20 230\t loss: 0.964, accuracy: 0.787\n",
      "iteration 20 240\t loss: 1.038, accuracy: 0.805\n",
      "iteration 20 250\t loss: 1.041, accuracy: 0.800\n",
      "iteration 20 260\t loss: 0.984, accuracy: 0.805\n",
      "iteration 20 270\t loss: 1.236, accuracy: 0.782\n",
      "iteration 20 280\t loss: 1.224, accuracy: 0.773\n",
      "iteration 21 0\t loss: 1.228, accuracy: 0.780\n",
      "iteration 21 10\t loss: 1.195, accuracy: 0.787\n",
      "iteration 21 20\t loss: 1.080, accuracy: 0.779\n",
      "iteration 21 30\t loss: 1.040, accuracy: 0.794\n",
      "iteration 21 40\t loss: 1.071, accuracy: 0.795\n",
      "iteration 21 50\t loss: 1.129, accuracy: 0.785\n",
      "iteration 21 60\t loss: 1.022, accuracy: 0.793\n",
      "iteration 21 70\t loss: 1.162, accuracy: 0.793\n",
      "iteration 21 80\t loss: 1.085, accuracy: 0.797\n",
      "iteration 21 90\t loss: 1.144, accuracy: 0.785\n",
      "iteration 21 100\t loss: 1.299, accuracy: 0.767\n",
      "iteration 21 110\t loss: 1.138, accuracy: 0.790\n",
      "iteration 21 120\t loss: 1.253, accuracy: 0.775\n",
      "iteration 21 130\t loss: 1.358, accuracy: 0.789\n",
      "iteration 21 140\t loss: 1.192, accuracy: 0.796\n",
      "iteration 21 150\t loss: 1.091, accuracy: 0.792\n",
      "iteration 21 160\t loss: 0.974, accuracy: 0.793\n",
      "iteration 21 170\t loss: 1.005, accuracy: 0.807\n",
      "iteration 21 180\t loss: 0.994, accuracy: 0.803\n",
      "iteration 21 190\t loss: 1.066, accuracy: 0.802\n",
      "iteration 21 200\t loss: 0.952, accuracy: 0.811\n",
      "iteration 21 210\t loss: 1.079, accuracy: 0.803\n",
      "iteration 21 220\t loss: 1.038, accuracy: 0.792\n",
      "iteration 21 230\t loss: 0.963, accuracy: 0.803\n",
      "iteration 21 240\t loss: 1.016, accuracy: 0.806\n",
      "iteration 21 250\t loss: 1.013, accuracy: 0.816\n",
      "iteration 21 260\t loss: 1.045, accuracy: 0.798\n",
      "iteration 21 270\t loss: 1.299, accuracy: 0.797\n",
      "iteration 21 280\t loss: 1.391, accuracy: 0.774\n",
      "iteration 22 0\t loss: 1.250, accuracy: 0.776\n",
      "iteration 22 10\t loss: 1.287, accuracy: 0.781\n",
      "iteration 22 20\t loss: 1.096, accuracy: 0.782\n",
      "iteration 22 30\t loss: 1.129, accuracy: 0.795\n",
      "iteration 22 40\t loss: 1.137, accuracy: 0.790\n",
      "iteration 22 50\t loss: 1.191, accuracy: 0.794\n",
      "iteration 22 60\t loss: 1.161, accuracy: 0.771\n",
      "iteration 22 70\t loss: 1.236, accuracy: 0.803\n",
      "iteration 22 80\t loss: 1.161, accuracy: 0.793\n",
      "iteration 22 90\t loss: 1.182, accuracy: 0.791\n",
      "iteration 22 100\t loss: 1.246, accuracy: 0.786\n",
      "iteration 22 110\t loss: 1.262, accuracy: 0.789\n",
      "iteration 22 120\t loss: 1.544, accuracy: 0.748\n",
      "iteration 22 130\t loss: 1.382, accuracy: 0.788\n",
      "iteration 22 140\t loss: 1.194, accuracy: 0.782\n",
      "iteration 22 150\t loss: 1.180, accuracy: 0.792\n",
      "iteration 22 160\t loss: 1.144, accuracy: 0.785\n",
      "iteration 22 170\t loss: 1.133, accuracy: 0.803\n",
      "iteration 22 180\t loss: 1.076, accuracy: 0.793\n",
      "iteration 22 190\t loss: 1.130, accuracy: 0.806\n",
      "iteration 22 200\t loss: 1.062, accuracy: 0.811\n",
      "iteration 22 210\t loss: 1.089, accuracy: 0.802\n",
      "iteration 22 220\t loss: 1.081, accuracy: 0.786\n",
      "iteration 22 230\t loss: 1.064, accuracy: 0.805\n",
      "iteration 22 240\t loss: 1.127, accuracy: 0.802\n",
      "iteration 22 250\t loss: 1.045, accuracy: 0.811\n",
      "iteration 22 260\t loss: 1.131, accuracy: 0.795\n",
      "iteration 22 270\t loss: 1.422, accuracy: 0.791\n",
      "iteration 22 280\t loss: 1.431, accuracy: 0.767\n",
      "iteration 23 0\t loss: 1.393, accuracy: 0.762\n",
      "iteration 23 10\t loss: 1.309, accuracy: 0.775\n",
      "iteration 23 20\t loss: 1.192, accuracy: 0.782\n",
      "iteration 23 30\t loss: 1.200, accuracy: 0.798\n",
      "iteration 23 40\t loss: 1.147, accuracy: 0.793\n",
      "iteration 23 50\t loss: 1.227, accuracy: 0.792\n",
      "iteration 23 60\t loss: 1.265, accuracy: 0.770\n",
      "iteration 23 70\t loss: 1.266, accuracy: 0.802\n",
      "iteration 23 80\t loss: 1.237, accuracy: 0.790\n",
      "iteration 23 90\t loss: 1.222, accuracy: 0.785\n",
      "iteration 23 100\t loss: 1.120, accuracy: 0.782\n",
      "iteration 23 110\t loss: 1.407, accuracy: 0.787\n",
      "iteration 23 120\t loss: 1.600, accuracy: 0.757\n",
      "iteration 23 130\t loss: 1.295, accuracy: 0.783\n",
      "iteration 23 140\t loss: 1.274, accuracy: 0.800\n",
      "iteration 23 150\t loss: 1.221, accuracy: 0.781\n",
      "iteration 23 160\t loss: 1.203, accuracy: 0.791\n",
      "iteration 23 170\t loss: 1.168, accuracy: 0.804\n",
      "iteration 23 180\t loss: 1.088, accuracy: 0.799\n",
      "iteration 23 190\t loss: 1.217, accuracy: 0.803\n",
      "iteration 23 200\t loss: 1.152, accuracy: 0.807\n",
      "iteration 23 210\t loss: 1.134, accuracy: 0.803\n",
      "iteration 23 220\t loss: 1.166, accuracy: 0.789\n",
      "iteration 23 230\t loss: 1.122, accuracy: 0.804\n",
      "iteration 23 240\t loss: 1.075, accuracy: 0.801\n",
      "iteration 23 250\t loss: 1.102, accuracy: 0.807\n",
      "iteration 23 260\t loss: 1.144, accuracy: 0.802\n",
      "iteration 23 270\t loss: 1.449, accuracy: 0.785\n",
      "iteration 23 280\t loss: 1.234, accuracy: 0.786\n",
      "iteration 24 0\t loss: 1.410, accuracy: 0.777\n",
      "iteration 24 10\t loss: 1.345, accuracy: 0.772\n",
      "iteration 24 20\t loss: 1.285, accuracy: 0.790\n",
      "iteration 24 30\t loss: 1.310, accuracy: 0.794\n",
      "iteration 24 40\t loss: 1.180, accuracy: 0.796\n",
      "iteration 24 50\t loss: 1.274, accuracy: 0.808\n",
      "iteration 24 60\t loss: 1.222, accuracy: 0.798\n",
      "iteration 24 70\t loss: 1.231, accuracy: 0.804\n",
      "iteration 24 80\t loss: 1.437, accuracy: 0.776\n",
      "iteration 24 90\t loss: 1.446, accuracy: 0.786\n",
      "iteration 24 100\t loss: 1.130, accuracy: 0.784\n",
      "iteration 24 110\t loss: 1.275, accuracy: 0.800\n",
      "iteration 24 120\t loss: 1.832, accuracy: 0.761\n",
      "iteration 24 130\t loss: 1.273, accuracy: 0.792\n",
      "iteration 24 140\t loss: 1.244, accuracy: 0.786\n",
      "iteration 24 150\t loss: 1.278, accuracy: 0.792\n",
      "iteration 24 160\t loss: 1.052, accuracy: 0.805\n",
      "iteration 24 170\t loss: 1.053, accuracy: 0.814\n",
      "iteration 24 180\t loss: 1.140, accuracy: 0.804\n",
      "iteration 24 190\t loss: 1.098, accuracy: 0.810\n",
      "iteration 24 200\t loss: 1.049, accuracy: 0.811\n",
      "iteration 24 210\t loss: 1.120, accuracy: 0.808\n",
      "iteration 24 220\t loss: 1.139, accuracy: 0.798\n",
      "iteration 24 230\t loss: 1.190, accuracy: 0.797\n",
      "iteration 24 240\t loss: 1.164, accuracy: 0.795\n",
      "iteration 24 250\t loss: 1.159, accuracy: 0.813\n",
      "iteration 24 260\t loss: 1.121, accuracy: 0.802\n",
      "iteration 24 270\t loss: 1.539, accuracy: 0.790\n",
      "iteration 24 280\t loss: 1.275, accuracy: 0.803\n",
      "iteration 25 0\t loss: 1.697, accuracy: 0.771\n",
      "iteration 25 10\t loss: 1.321, accuracy: 0.788\n",
      "iteration 25 20\t loss: 1.247, accuracy: 0.790\n",
      "iteration 25 30\t loss: 1.290, accuracy: 0.797\n",
      "iteration 25 40\t loss: 1.239, accuracy: 0.798\n",
      "iteration 25 50\t loss: 1.350, accuracy: 0.797\n",
      "iteration 25 60\t loss: 1.305, accuracy: 0.794\n",
      "iteration 25 70\t loss: 1.227, accuracy: 0.803\n",
      "iteration 25 80\t loss: 1.450, accuracy: 0.781\n",
      "iteration 25 90\t loss: 1.556, accuracy: 0.769\n",
      "iteration 25 100\t loss: 1.328, accuracy: 0.775\n",
      "iteration 25 110\t loss: 1.334, accuracy: 0.800\n",
      "iteration 25 120\t loss: 1.718, accuracy: 0.755\n",
      "iteration 25 130\t loss: 1.237, accuracy: 0.751\n",
      "iteration 25 140\t loss: 1.319, accuracy: 0.794\n",
      "iteration 25 150\t loss: 1.276, accuracy: 0.795\n",
      "iteration 25 160\t loss: 1.097, accuracy: 0.799\n",
      "iteration 25 170\t loss: 1.136, accuracy: 0.808\n",
      "iteration 25 180\t loss: 1.169, accuracy: 0.805\n",
      "iteration 25 190\t loss: 1.213, accuracy: 0.808\n",
      "iteration 25 200\t loss: 1.098, accuracy: 0.809\n",
      "iteration 25 210\t loss: 1.169, accuracy: 0.812\n",
      "iteration 25 220\t loss: 1.131, accuracy: 0.798\n",
      "iteration 25 230\t loss: 1.139, accuracy: 0.805\n",
      "iteration 25 240\t loss: 1.165, accuracy: 0.806\n",
      "iteration 25 250\t loss: 1.232, accuracy: 0.812\n",
      "iteration 25 260\t loss: 1.072, accuracy: 0.803\n",
      "iteration 25 270\t loss: 1.560, accuracy: 0.792\n",
      "iteration 25 280\t loss: 1.317, accuracy: 0.809\n",
      "iteration 26 0\t loss: 2.116, accuracy: 0.757\n",
      "iteration 26 10\t loss: 1.264, accuracy: 0.785\n",
      "iteration 26 20\t loss: 1.283, accuracy: 0.804\n",
      "iteration 26 30\t loss: 1.220, accuracy: 0.797\n",
      "iteration 26 40\t loss: 1.412, accuracy: 0.799\n",
      "iteration 26 50\t loss: 1.520, accuracy: 0.789\n",
      "iteration 26 60\t loss: 1.275, accuracy: 0.789\n",
      "iteration 26 70\t loss: 1.280, accuracy: 0.805\n",
      "iteration 26 80\t loss: 1.514, accuracy: 0.773\n",
      "iteration 26 90\t loss: 1.583, accuracy: 0.783\n",
      "iteration 26 100\t loss: 1.370, accuracy: 0.787\n",
      "iteration 26 110\t loss: 1.460, accuracy: 0.782\n",
      "iteration 26 120\t loss: 1.612, accuracy: 0.777\n",
      "iteration 26 130\t loss: 1.220, accuracy: 0.774\n",
      "iteration 26 140\t loss: 1.407, accuracy: 0.803\n",
      "iteration 26 150\t loss: 1.314, accuracy: 0.799\n",
      "iteration 26 160\t loss: 1.168, accuracy: 0.806\n",
      "iteration 26 170\t loss: 1.164, accuracy: 0.811\n",
      "iteration 26 180\t loss: 1.222, accuracy: 0.809\n",
      "iteration 26 190\t loss: 1.296, accuracy: 0.808\n",
      "iteration 26 200\t loss: 1.234, accuracy: 0.806\n",
      "iteration 26 210\t loss: 1.305, accuracy: 0.810\n",
      "iteration 26 220\t loss: 1.265, accuracy: 0.800\n",
      "iteration 26 230\t loss: 1.218, accuracy: 0.812\n",
      "iteration 26 240\t loss: 1.292, accuracy: 0.800\n",
      "iteration 26 250\t loss: 1.315, accuracy: 0.814\n",
      "iteration 26 260\t loss: 1.194, accuracy: 0.804\n",
      "iteration 26 270\t loss: 1.619, accuracy: 0.789\n",
      "iteration 26 280\t loss: 1.320, accuracy: 0.809\n",
      "iteration 27 0\t loss: 1.898, accuracy: 0.767\n",
      "iteration 27 10\t loss: 1.402, accuracy: 0.780\n",
      "iteration 27 20\t loss: 1.449, accuracy: 0.800\n",
      "iteration 27 30\t loss: 1.236, accuracy: 0.794\n",
      "iteration 27 40\t loss: 1.380, accuracy: 0.799\n",
      "iteration 27 50\t loss: 1.550, accuracy: 0.791\n",
      "iteration 27 60\t loss: 1.356, accuracy: 0.760\n",
      "iteration 27 70\t loss: 1.407, accuracy: 0.805\n",
      "iteration 27 80\t loss: 1.555, accuracy: 0.772\n",
      "iteration 27 90\t loss: 1.581, accuracy: 0.782\n",
      "iteration 27 100\t loss: 1.591, accuracy: 0.784\n",
      "iteration 27 110\t loss: 1.432, accuracy: 0.775\n",
      "iteration 27 120\t loss: 1.840, accuracy: 0.771\n",
      "iteration 27 130\t loss: 1.387, accuracy: 0.797\n",
      "iteration 27 140\t loss: 1.547, accuracy: 0.798\n",
      "iteration 27 150\t loss: 1.316, accuracy: 0.794\n",
      "iteration 27 160\t loss: 1.315, accuracy: 0.808\n",
      "iteration 27 170\t loss: 1.224, accuracy: 0.808\n",
      "iteration 27 180\t loss: 1.372, accuracy: 0.802\n",
      "iteration 27 190\t loss: 1.309, accuracy: 0.806\n",
      "iteration 27 200\t loss: 1.261, accuracy: 0.811\n",
      "iteration 27 210\t loss: 1.375, accuracy: 0.805\n",
      "iteration 27 220\t loss: 1.328, accuracy: 0.797\n",
      "iteration 27 230\t loss: 1.277, accuracy: 0.807\n",
      "iteration 27 240\t loss: 1.330, accuracy: 0.800\n",
      "iteration 27 250\t loss: 1.377, accuracy: 0.805\n",
      "iteration 27 260\t loss: 1.289, accuracy: 0.797\n",
      "iteration 27 270\t loss: 1.627, accuracy: 0.796\n",
      "iteration 27 280\t loss: 1.533, accuracy: 0.803\n",
      "iteration 28 0\t loss: 1.602, accuracy: 0.784\n",
      "iteration 28 10\t loss: 1.566, accuracy: 0.776\n",
      "iteration 28 20\t loss: 1.698, accuracy: 0.790\n",
      "iteration 28 30\t loss: 1.368, accuracy: 0.781\n",
      "iteration 28 40\t loss: 1.471, accuracy: 0.794\n",
      "iteration 28 50\t loss: 1.509, accuracy: 0.797\n",
      "iteration 28 60\t loss: 1.392, accuracy: 0.786\n",
      "iteration 28 70\t loss: 1.575, accuracy: 0.805\n",
      "iteration 28 80\t loss: 1.431, accuracy: 0.791\n",
      "iteration 28 90\t loss: 1.538, accuracy: 0.794\n",
      "iteration 28 100\t loss: 1.659, accuracy: 0.790\n",
      "iteration 28 110\t loss: 1.306, accuracy: 0.792\n",
      "iteration 28 120\t loss: 1.642, accuracy: 0.778\n",
      "iteration 28 130\t loss: 1.460, accuracy: 0.795\n",
      "iteration 28 140\t loss: 1.651, accuracy: 0.799\n",
      "iteration 28 150\t loss: 1.261, accuracy: 0.796\n",
      "iteration 28 160\t loss: 1.299, accuracy: 0.812\n",
      "iteration 28 170\t loss: 1.245, accuracy: 0.813\n",
      "iteration 28 180\t loss: 1.389, accuracy: 0.812\n",
      "iteration 28 190\t loss: 1.364, accuracy: 0.802\n",
      "iteration 28 200\t loss: 1.418, accuracy: 0.799\n",
      "iteration 28 210\t loss: 1.515, accuracy: 0.808\n",
      "iteration 28 220\t loss: 1.406, accuracy: 0.805\n",
      "iteration 28 230\t loss: 1.330, accuracy: 0.805\n",
      "iteration 28 240\t loss: 1.387, accuracy: 0.791\n",
      "iteration 28 250\t loss: 1.445, accuracy: 0.809\n",
      "iteration 28 260\t loss: 1.349, accuracy: 0.794\n",
      "iteration 28 270\t loss: 1.816, accuracy: 0.791\n",
      "iteration 28 280\t loss: 1.807, accuracy: 0.796\n",
      "iteration 29 0\t loss: 1.489, accuracy: 0.783\n",
      "iteration 29 10\t loss: 1.506, accuracy: 0.793\n",
      "iteration 29 20\t loss: 1.585, accuracy: 0.799\n",
      "iteration 29 30\t loss: 1.401, accuracy: 0.789\n",
      "iteration 29 40\t loss: 1.478, accuracy: 0.803\n",
      "iteration 29 50\t loss: 1.824, accuracy: 0.792\n",
      "iteration 29 60\t loss: 1.423, accuracy: 0.793\n",
      "iteration 29 70\t loss: 1.823, accuracy: 0.792\n",
      "iteration 29 80\t loss: 1.453, accuracy: 0.807\n",
      "iteration 29 90\t loss: 1.465, accuracy: 0.798\n",
      "iteration 29 100\t loss: 1.558, accuracy: 0.795\n",
      "iteration 29 110\t loss: 1.573, accuracy: 0.798\n",
      "iteration 29 120\t loss: 1.863, accuracy: 0.778\n",
      "iteration 29 130\t loss: 1.519, accuracy: 0.793\n",
      "iteration 29 140\t loss: 1.595, accuracy: 0.800\n",
      "iteration 29 150\t loss: 1.476, accuracy: 0.802\n",
      "iteration 29 160\t loss: 1.472, accuracy: 0.807\n",
      "iteration 29 170\t loss: 1.312, accuracy: 0.810\n",
      "iteration 29 180\t loss: 1.506, accuracy: 0.808\n",
      "iteration 29 190\t loss: 1.387, accuracy: 0.803\n",
      "iteration 29 200\t loss: 1.454, accuracy: 0.810\n",
      "iteration 29 210\t loss: 1.548, accuracy: 0.815\n",
      "iteration 29 220\t loss: 1.492, accuracy: 0.803\n",
      "iteration 29 230\t loss: 1.392, accuracy: 0.808\n",
      "iteration 29 240\t loss: 1.478, accuracy: 0.797\n",
      "iteration 29 250\t loss: 1.541, accuracy: 0.810\n",
      "iteration 29 260\t loss: 1.393, accuracy: 0.800\n",
      "iteration 29 270\t loss: 1.738, accuracy: 0.807\n",
      "iteration 29 280\t loss: 1.893, accuracy: 0.795\n",
      "iteration 30 0\t loss: 1.584, accuracy: 0.795\n",
      "iteration 30 10\t loss: 1.727, accuracy: 0.779\n",
      "iteration 30 20\t loss: 1.724, accuracy: 0.801\n",
      "iteration 30 30\t loss: 1.484, accuracy: 0.781\n",
      "iteration 30 40\t loss: 1.707, accuracy: 0.798\n",
      "iteration 30 50\t loss: 1.796, accuracy: 0.803\n",
      "iteration 30 60\t loss: 1.565, accuracy: 0.787\n",
      "iteration 30 70\t loss: 1.685, accuracy: 0.799\n",
      "iteration 30 80\t loss: 1.471, accuracy: 0.806\n",
      "iteration 30 90\t loss: 1.603, accuracy: 0.791\n",
      "iteration 30 100\t loss: 1.612, accuracy: 0.794\n",
      "iteration 30 110\t loss: 1.815, accuracy: 0.795\n",
      "iteration 30 120\t loss: 1.906, accuracy: 0.786\n",
      "iteration 30 130\t loss: 1.525, accuracy: 0.801\n",
      "iteration 30 140\t loss: 1.640, accuracy: 0.802\n",
      "iteration 30 150\t loss: 1.521, accuracy: 0.804\n",
      "iteration 30 160\t loss: 1.516, accuracy: 0.804\n",
      "iteration 30 170\t loss: 1.417, accuracy: 0.816\n",
      "iteration 30 180\t loss: 1.461, accuracy: 0.816\n",
      "iteration 30 190\t loss: 1.473, accuracy: 0.804\n",
      "iteration 30 200\t loss: 1.522, accuracy: 0.795\n",
      "iteration 30 210\t loss: 1.630, accuracy: 0.811\n",
      "iteration 30 220\t loss: 1.515, accuracy: 0.804\n",
      "iteration 30 230\t loss: 1.505, accuracy: 0.807\n",
      "iteration 30 240\t loss: 1.403, accuracy: 0.811\n",
      "iteration 30 250\t loss: 1.468, accuracy: 0.810\n",
      "iteration 30 260\t loss: 1.469, accuracy: 0.788\n",
      "iteration 30 270\t loss: 1.684, accuracy: 0.800\n",
      "iteration 30 280\t loss: 1.989, accuracy: 0.797\n",
      "iteration 31 0\t loss: 1.578, accuracy: 0.794\n",
      "iteration 31 10\t loss: 1.846, accuracy: 0.782\n",
      "iteration 31 20\t loss: 1.790, accuracy: 0.802\n",
      "iteration 31 30\t loss: 1.508, accuracy: 0.809\n",
      "iteration 31 40\t loss: 1.758, accuracy: 0.797\n",
      "iteration 31 50\t loss: 1.758, accuracy: 0.799\n",
      "iteration 31 60\t loss: 1.572, accuracy: 0.799\n",
      "iteration 31 70\t loss: 1.729, accuracy: 0.804\n",
      "iteration 31 80\t loss: 1.531, accuracy: 0.806\n",
      "iteration 31 90\t loss: 1.624, accuracy: 0.796\n",
      "iteration 31 100\t loss: 1.557, accuracy: 0.803\n",
      "iteration 31 110\t loss: 1.828, accuracy: 0.800\n",
      "iteration 31 120\t loss: 1.606, accuracy: 0.811\n",
      "iteration 31 130\t loss: 1.567, accuracy: 0.797\n",
      "iteration 31 140\t loss: 1.569, accuracy: 0.804\n",
      "iteration 31 150\t loss: 1.508, accuracy: 0.795\n",
      "iteration 31 160\t loss: 1.615, accuracy: 0.808\n",
      "iteration 31 170\t loss: 1.462, accuracy: 0.808\n",
      "iteration 31 180\t loss: 1.510, accuracy: 0.815\n",
      "iteration 31 190\t loss: 1.670, accuracy: 0.798\n",
      "iteration 31 200\t loss: 1.442, accuracy: 0.811\n",
      "iteration 31 210\t loss: 1.588, accuracy: 0.811\n",
      "iteration 31 220\t loss: 1.595, accuracy: 0.805\n",
      "iteration 31 230\t loss: 1.496, accuracy: 0.811\n",
      "iteration 31 240\t loss: 1.513, accuracy: 0.819\n",
      "iteration 31 250\t loss: 1.513, accuracy: 0.815\n",
      "iteration 31 260\t loss: 1.432, accuracy: 0.807\n",
      "iteration 31 270\t loss: 1.563, accuracy: 0.803\n",
      "iteration 31 280\t loss: 1.924, accuracy: 0.792\n",
      "iteration 32 0\t loss: 1.761, accuracy: 0.771\n",
      "iteration 32 10\t loss: 1.781, accuracy: 0.806\n",
      "iteration 32 20\t loss: 1.680, accuracy: 0.797\n",
      "iteration 32 30\t loss: 1.690, accuracy: 0.798\n",
      "iteration 32 40\t loss: 1.665, accuracy: 0.811\n",
      "iteration 32 50\t loss: 1.569, accuracy: 0.809\n",
      "iteration 32 60\t loss: 1.616, accuracy: 0.798\n",
      "iteration 32 70\t loss: 1.703, accuracy: 0.796\n",
      "iteration 32 80\t loss: 1.701, accuracy: 0.800\n",
      "iteration 32 90\t loss: 1.649, accuracy: 0.797\n",
      "iteration 32 100\t loss: 1.478, accuracy: 0.796\n",
      "iteration 32 110\t loss: 1.792, accuracy: 0.794\n",
      "iteration 32 120\t loss: 1.495, accuracy: 0.809\n",
      "iteration 32 130\t loss: 1.965, accuracy: 0.782\n",
      "iteration 32 140\t loss: 1.521, accuracy: 0.788\n",
      "iteration 32 150\t loss: 1.582, accuracy: 0.807\n",
      "iteration 32 160\t loss: 1.576, accuracy: 0.805\n",
      "iteration 32 170\t loss: 1.469, accuracy: 0.817\n",
      "iteration 32 180\t loss: 1.604, accuracy: 0.807\n",
      "iteration 32 190\t loss: 1.562, accuracy: 0.810\n",
      "iteration 32 200\t loss: 1.482, accuracy: 0.811\n",
      "iteration 32 210\t loss: 1.456, accuracy: 0.818\n",
      "iteration 32 220\t loss: 1.619, accuracy: 0.811\n",
      "iteration 32 230\t loss: 1.525, accuracy: 0.813\n",
      "iteration 32 240\t loss: 1.464, accuracy: 0.816\n",
      "iteration 32 250\t loss: 1.512, accuracy: 0.812\n",
      "iteration 32 260\t loss: 1.477, accuracy: 0.817\n",
      "iteration 32 270\t loss: 1.785, accuracy: 0.800\n",
      "iteration 32 280\t loss: 2.007, accuracy: 0.787\n",
      "iteration 33 0\t loss: 1.565, accuracy: 0.797\n",
      "iteration 33 10\t loss: 1.659, accuracy: 0.793\n",
      "iteration 33 20\t loss: 1.592, accuracy: 0.792\n",
      "iteration 33 30\t loss: 1.730, accuracy: 0.803\n",
      "iteration 33 40\t loss: 1.820, accuracy: 0.802\n",
      "iteration 33 50\t loss: 1.621, accuracy: 0.801\n",
      "iteration 33 60\t loss: 1.788, accuracy: 0.809\n",
      "iteration 33 70\t loss: 1.738, accuracy: 0.806\n",
      "iteration 33 80\t loss: 1.553, accuracy: 0.814\n",
      "iteration 33 90\t loss: 1.693, accuracy: 0.802\n",
      "iteration 33 100\t loss: 1.673, accuracy: 0.809\n",
      "iteration 33 110\t loss: 1.819, accuracy: 0.805\n",
      "iteration 33 120\t loss: 1.699, accuracy: 0.812\n",
      "iteration 33 130\t loss: 1.760, accuracy: 0.784\n",
      "iteration 33 140\t loss: 1.511, accuracy: 0.790\n",
      "iteration 33 150\t loss: 1.547, accuracy: 0.810\n",
      "iteration 33 160\t loss: 1.531, accuracy: 0.809\n",
      "iteration 33 170\t loss: 1.561, accuracy: 0.816\n",
      "iteration 33 180\t loss: 1.605, accuracy: 0.816\n",
      "iteration 33 190\t loss: 1.788, accuracy: 0.807\n",
      "iteration 33 200\t loss: 1.552, accuracy: 0.804\n",
      "iteration 33 210\t loss: 1.614, accuracy: 0.819\n",
      "iteration 33 220\t loss: 1.649, accuracy: 0.809\n",
      "iteration 33 230\t loss: 1.602, accuracy: 0.820\n",
      "iteration 33 240\t loss: 1.643, accuracy: 0.804\n",
      "iteration 33 250\t loss: 1.637, accuracy: 0.812\n",
      "iteration 33 260\t loss: 1.704, accuracy: 0.808\n",
      "iteration 33 270\t loss: 1.937, accuracy: 0.769\n",
      "iteration 33 280\t loss: 1.993, accuracy: 0.787\n",
      "iteration 34 0\t loss: 2.005, accuracy: 0.775\n",
      "iteration 34 10\t loss: 1.635, accuracy: 0.793\n",
      "iteration 34 20\t loss: 1.766, accuracy: 0.791\n",
      "iteration 34 30\t loss: 1.777, accuracy: 0.799\n",
      "iteration 34 40\t loss: 1.928, accuracy: 0.808\n",
      "iteration 34 50\t loss: 1.838, accuracy: 0.803\n",
      "iteration 34 60\t loss: 1.845, accuracy: 0.810\n",
      "iteration 34 70\t loss: 1.755, accuracy: 0.800\n",
      "iteration 34 80\t loss: 1.768, accuracy: 0.809\n",
      "iteration 34 90\t loss: 1.756, accuracy: 0.804\n",
      "iteration 34 100\t loss: 1.706, accuracy: 0.807\n",
      "iteration 34 110\t loss: 1.799, accuracy: 0.802\n",
      "iteration 34 120\t loss: 1.965, accuracy: 0.805\n",
      "iteration 34 130\t loss: 1.759, accuracy: 0.784\n",
      "iteration 34 140\t loss: 1.448, accuracy: 0.786\n",
      "iteration 34 150\t loss: 1.685, accuracy: 0.800\n",
      "iteration 34 160\t loss: 1.507, accuracy: 0.802\n",
      "iteration 34 170\t loss: 1.574, accuracy: 0.804\n",
      "iteration 34 180\t loss: 1.685, accuracy: 0.814\n",
      "iteration 34 190\t loss: 1.613, accuracy: 0.818\n",
      "iteration 34 200\t loss: 1.603, accuracy: 0.811\n",
      "iteration 34 210\t loss: 1.782, accuracy: 0.813\n",
      "iteration 34 220\t loss: 1.851, accuracy: 0.804\n",
      "iteration 34 230\t loss: 1.685, accuracy: 0.811\n",
      "iteration 34 240\t loss: 1.545, accuracy: 0.800\n",
      "iteration 34 250\t loss: 1.825, accuracy: 0.810\n",
      "iteration 34 260\t loss: 1.699, accuracy: 0.807\n",
      "iteration 34 270\t loss: 1.780, accuracy: 0.791\n",
      "iteration 34 280\t loss: 1.872, accuracy: 0.804\n",
      "iteration 35 0\t loss: 1.849, accuracy: 0.799\n",
      "iteration 35 10\t loss: 2.025, accuracy: 0.800\n",
      "iteration 35 20\t loss: 1.801, accuracy: 0.796\n",
      "iteration 35 30\t loss: 1.884, accuracy: 0.796\n",
      "iteration 35 40\t loss: 1.785, accuracy: 0.810\n",
      "iteration 35 50\t loss: 1.851, accuracy: 0.808\n",
      "iteration 35 60\t loss: 1.941, accuracy: 0.803\n",
      "iteration 35 70\t loss: 1.802, accuracy: 0.790\n",
      "iteration 35 80\t loss: 1.954, accuracy: 0.810\n",
      "iteration 35 90\t loss: 1.986, accuracy: 0.770\n",
      "iteration 35 100\t loss: 1.951, accuracy: 0.802\n",
      "iteration 35 110\t loss: 1.849, accuracy: 0.802\n",
      "iteration 35 120\t loss: 1.982, accuracy: 0.813\n",
      "iteration 35 130\t loss: 1.913, accuracy: 0.798\n",
      "iteration 35 140\t loss: 1.590, accuracy: 0.799\n",
      "iteration 35 150\t loss: 1.919, accuracy: 0.813\n",
      "iteration 35 160\t loss: 1.573, accuracy: 0.789\n",
      "iteration 35 170\t loss: 1.816, accuracy: 0.810\n",
      "iteration 35 180\t loss: 1.795, accuracy: 0.807\n",
      "iteration 35 190\t loss: 1.943, accuracy: 0.798\n",
      "iteration 35 200\t loss: 1.850, accuracy: 0.802\n",
      "iteration 35 210\t loss: 1.718, accuracy: 0.813\n",
      "iteration 35 220\t loss: 1.910, accuracy: 0.812\n",
      "iteration 35 230\t loss: 1.732, accuracy: 0.816\n",
      "iteration 35 240\t loss: 1.582, accuracy: 0.793\n",
      "iteration 35 250\t loss: 1.886, accuracy: 0.823\n",
      "iteration 35 260\t loss: 1.665, accuracy: 0.811\n",
      "iteration 35 270\t loss: 1.843, accuracy: 0.810\n",
      "iteration 35 280\t loss: 1.920, accuracy: 0.812\n",
      "iteration 36 0\t loss: 2.058, accuracy: 0.800\n",
      "iteration 36 10\t loss: 1.938, accuracy: 0.796\n",
      "iteration 36 20\t loss: 1.853, accuracy: 0.801\n",
      "iteration 36 30\t loss: 1.872, accuracy: 0.812\n",
      "iteration 36 40\t loss: 1.817, accuracy: 0.810\n",
      "iteration 36 50\t loss: 1.941, accuracy: 0.806\n",
      "iteration 36 60\t loss: 1.962, accuracy: 0.807\n",
      "iteration 36 70\t loss: 1.815, accuracy: 0.797\n",
      "iteration 36 80\t loss: 1.798, accuracy: 0.812\n",
      "iteration 36 90\t loss: 1.949, accuracy: 0.765\n",
      "iteration 36 100\t loss: 1.777, accuracy: 0.803\n",
      "iteration 36 110\t loss: 2.005, accuracy: 0.802\n",
      "iteration 36 120\t loss: 1.998, accuracy: 0.814\n",
      "iteration 36 130\t loss: 1.933, accuracy: 0.801\n",
      "iteration 36 140\t loss: 1.593, accuracy: 0.801\n",
      "iteration 36 150\t loss: 1.960, accuracy: 0.808\n",
      "iteration 36 160\t loss: 1.612, accuracy: 0.793\n",
      "iteration 36 170\t loss: 1.736, accuracy: 0.820\n",
      "iteration 36 180\t loss: 1.825, accuracy: 0.817\n",
      "iteration 36 190\t loss: 1.768, accuracy: 0.818\n",
      "iteration 36 200\t loss: 1.830, accuracy: 0.812\n",
      "iteration 36 210\t loss: 1.671, accuracy: 0.812\n",
      "iteration 36 220\t loss: 1.984, accuracy: 0.817\n",
      "iteration 36 230\t loss: 1.763, accuracy: 0.812\n",
      "iteration 36 240\t loss: 1.646, accuracy: 0.804\n",
      "iteration 36 250\t loss: 1.833, accuracy: 0.820\n",
      "iteration 36 260\t loss: 1.640, accuracy: 0.821\n",
      "iteration 36 270\t loss: 1.558, accuracy: 0.820\n",
      "iteration 36 280\t loss: 1.679, accuracy: 0.821\n",
      "iteration 37 0\t loss: 1.918, accuracy: 0.817\n",
      "iteration 37 10\t loss: 1.996, accuracy: 0.800\n",
      "iteration 37 20\t loss: 1.936, accuracy: 0.802\n",
      "iteration 37 30\t loss: 1.932, accuracy: 0.814\n",
      "iteration 37 40\t loss: 2.049, accuracy: 0.811\n",
      "iteration 37 50\t loss: 1.928, accuracy: 0.815\n",
      "iteration 37 60\t loss: 1.951, accuracy: 0.821\n",
      "iteration 37 70\t loss: 1.797, accuracy: 0.813\n",
      "iteration 37 80\t loss: 1.949, accuracy: 0.823\n",
      "iteration 37 90\t loss: 1.858, accuracy: 0.806\n",
      "iteration 37 100\t loss: 1.812, accuracy: 0.797\n",
      "iteration 37 110\t loss: 1.907, accuracy: 0.810\n",
      "iteration 37 120\t loss: 2.291, accuracy: 0.808\n",
      "iteration 37 130\t loss: 1.987, accuracy: 0.796\n",
      "iteration 37 140\t loss: 1.712, accuracy: 0.808\n",
      "iteration 37 150\t loss: 2.103, accuracy: 0.799\n",
      "iteration 37 160\t loss: 1.538, accuracy: 0.804\n",
      "iteration 37 170\t loss: 1.731, accuracy: 0.818\n",
      "iteration 37 180\t loss: 1.917, accuracy: 0.817\n",
      "iteration 37 190\t loss: 1.974, accuracy: 0.810\n",
      "iteration 37 200\t loss: 1.871, accuracy: 0.810\n",
      "iteration 37 210\t loss: 1.778, accuracy: 0.821\n",
      "iteration 37 220\t loss: 2.069, accuracy: 0.819\n",
      "iteration 37 230\t loss: 1.818, accuracy: 0.819\n",
      "iteration 37 240\t loss: 1.694, accuracy: 0.817\n",
      "iteration 37 250\t loss: 1.864, accuracy: 0.828\n",
      "iteration 37 260\t loss: 1.844, accuracy: 0.821\n",
      "iteration 37 270\t loss: 1.817, accuracy: 0.806\n",
      "iteration 37 280\t loss: 1.973, accuracy: 0.815\n",
      "iteration 38 0\t loss: 2.006, accuracy: 0.812\n",
      "iteration 38 10\t loss: 2.038, accuracy: 0.805\n",
      "iteration 38 20\t loss: 2.004, accuracy: 0.801\n",
      "iteration 38 30\t loss: 2.108, accuracy: 0.808\n",
      "iteration 38 40\t loss: 2.196, accuracy: 0.806\n",
      "iteration 38 50\t loss: 2.082, accuracy: 0.815\n",
      "iteration 38 60\t loss: 2.097, accuracy: 0.811\n",
      "iteration 38 70\t loss: 1.986, accuracy: 0.813\n",
      "iteration 38 80\t loss: 2.080, accuracy: 0.822\n",
      "iteration 38 90\t loss: 2.290, accuracy: 0.811\n",
      "iteration 38 100\t loss: 1.856, accuracy: 0.789\n",
      "iteration 38 110\t loss: 2.163, accuracy: 0.812\n",
      "iteration 38 120\t loss: 2.386, accuracy: 0.808\n",
      "iteration 38 130\t loss: 2.163, accuracy: 0.812\n",
      "iteration 38 140\t loss: 1.909, accuracy: 0.789\n",
      "iteration 38 150\t loss: 1.930, accuracy: 0.803\n",
      "iteration 38 160\t loss: 1.695, accuracy: 0.789\n",
      "iteration 38 170\t loss: 1.793, accuracy: 0.815\n",
      "iteration 38 180\t loss: 1.698, accuracy: 0.821\n",
      "iteration 38 190\t loss: 1.943, accuracy: 0.817\n",
      "iteration 38 200\t loss: 1.986, accuracy: 0.809\n",
      "iteration 38 210\t loss: 1.880, accuracy: 0.813\n",
      "iteration 38 220\t loss: 2.190, accuracy: 0.817\n",
      "iteration 38 230\t loss: 1.785, accuracy: 0.815\n",
      "iteration 38 240\t loss: 1.655, accuracy: 0.804\n",
      "iteration 38 250\t loss: 1.783, accuracy: 0.824\n",
      "iteration 38 260\t loss: 1.907, accuracy: 0.826\n",
      "iteration 38 270\t loss: 1.807, accuracy: 0.804\n",
      "iteration 38 280\t loss: 1.934, accuracy: 0.827\n",
      "iteration 39 0\t loss: 1.988, accuracy: 0.821\n",
      "iteration 39 10\t loss: 2.136, accuracy: 0.802\n",
      "iteration 39 20\t loss: 2.073, accuracy: 0.804\n",
      "iteration 39 30\t loss: 2.207, accuracy: 0.800\n",
      "iteration 39 40\t loss: 2.162, accuracy: 0.818\n",
      "iteration 39 50\t loss: 2.297, accuracy: 0.816\n",
      "iteration 39 60\t loss: 2.096, accuracy: 0.821\n",
      "iteration 39 70\t loss: 2.025, accuracy: 0.811\n",
      "iteration 39 80\t loss: 2.148, accuracy: 0.822\n",
      "iteration 39 90\t loss: 2.234, accuracy: 0.816\n",
      "iteration 39 100\t loss: 1.911, accuracy: 0.791\n",
      "iteration 39 110\t loss: 2.172, accuracy: 0.798\n",
      "iteration 39 120\t loss: 2.410, accuracy: 0.806\n",
      "iteration 39 130\t loss: 2.133, accuracy: 0.812\n",
      "iteration 39 140\t loss: 2.081, accuracy: 0.796\n",
      "iteration 39 150\t loss: 2.026, accuracy: 0.799\n",
      "iteration 39 160\t loss: 1.852, accuracy: 0.811\n",
      "iteration 39 170\t loss: 1.768, accuracy: 0.806\n",
      "iteration 39 180\t loss: 1.862, accuracy: 0.814\n",
      "iteration 39 190\t loss: 1.928, accuracy: 0.812\n",
      "iteration 39 200\t loss: 1.947, accuracy: 0.809\n",
      "iteration 39 210\t loss: 2.089, accuracy: 0.813\n",
      "iteration 39 220\t loss: 2.188, accuracy: 0.820\n",
      "iteration 39 230\t loss: 1.995, accuracy: 0.819\n",
      "iteration 39 240\t loss: 1.825, accuracy: 0.813\n",
      "iteration 39 250\t loss: 1.936, accuracy: 0.820\n",
      "iteration 39 260\t loss: 2.077, accuracy: 0.817\n",
      "iteration 39 270\t loss: 2.027, accuracy: 0.816\n",
      "iteration 39 280\t loss: 2.052, accuracy: 0.807\n",
      "iteration 40 0\t loss: 2.013, accuracy: 0.822\n",
      "iteration 40 10\t loss: 2.046, accuracy: 0.810\n",
      "iteration 40 20\t loss: 1.984, accuracy: 0.806\n",
      "iteration 40 30\t loss: 2.361, accuracy: 0.809\n",
      "iteration 40 40\t loss: 2.166, accuracy: 0.811\n",
      "iteration 40 50\t loss: 2.288, accuracy: 0.814\n",
      "iteration 40 60\t loss: 2.359, accuracy: 0.815\n",
      "iteration 40 70\t loss: 2.078, accuracy: 0.815\n",
      "iteration 40 80\t loss: 2.164, accuracy: 0.824\n",
      "iteration 40 90\t loss: 2.241, accuracy: 0.806\n",
      "iteration 40 100\t loss: 1.900, accuracy: 0.796\n",
      "iteration 40 110\t loss: 2.567, accuracy: 0.809\n",
      "iteration 40 120\t loss: 2.530, accuracy: 0.808\n",
      "iteration 40 130\t loss: 1.999, accuracy: 0.819\n",
      "iteration 40 140\t loss: 1.960, accuracy: 0.802\n",
      "iteration 40 150\t loss: 1.931, accuracy: 0.800\n",
      "iteration 40 160\t loss: 1.847, accuracy: 0.812\n",
      "iteration 40 170\t loss: 1.702, accuracy: 0.808\n",
      "iteration 40 180\t loss: 2.091, accuracy: 0.814\n",
      "iteration 40 190\t loss: 2.047, accuracy: 0.817\n",
      "iteration 40 200\t loss: 2.072, accuracy: 0.807\n",
      "iteration 40 210\t loss: 2.140, accuracy: 0.813\n",
      "iteration 40 220\t loss: 2.055, accuracy: 0.825\n",
      "iteration 40 230\t loss: 2.207, accuracy: 0.824\n",
      "iteration 40 240\t loss: 1.873, accuracy: 0.809\n",
      "iteration 40 250\t loss: 1.943, accuracy: 0.823\n",
      "iteration 40 260\t loss: 2.155, accuracy: 0.824\n",
      "iteration 40 270\t loss: 2.052, accuracy: 0.816\n",
      "iteration 40 280\t loss: 2.544, accuracy: 0.805\n",
      "iteration 41 0\t loss: 2.109, accuracy: 0.813\n",
      "iteration 41 10\t loss: 2.438, accuracy: 0.811\n",
      "iteration 41 20\t loss: 2.227, accuracy: 0.803\n",
      "iteration 41 30\t loss: 2.268, accuracy: 0.816\n",
      "iteration 41 40\t loss: 2.222, accuracy: 0.810\n",
      "iteration 41 50\t loss: 2.388, accuracy: 0.808\n",
      "iteration 41 60\t loss: 2.501, accuracy: 0.810\n",
      "iteration 41 70\t loss: 2.238, accuracy: 0.809\n",
      "iteration 41 80\t loss: 2.270, accuracy: 0.815\n",
      "iteration 41 90\t loss: 2.537, accuracy: 0.817\n",
      "iteration 41 100\t loss: 2.179, accuracy: 0.783\n",
      "iteration 41 110\t loss: 2.283, accuracy: 0.810\n",
      "iteration 41 120\t loss: 2.565, accuracy: 0.815\n",
      "iteration 41 130\t loss: 2.393, accuracy: 0.814\n",
      "iteration 41 140\t loss: 2.205, accuracy: 0.812\n",
      "iteration 41 150\t loss: 2.130, accuracy: 0.809\n",
      "iteration 41 160\t loss: 2.157, accuracy: 0.809\n",
      "iteration 41 170\t loss: 1.899, accuracy: 0.805\n",
      "iteration 41 180\t loss: 2.114, accuracy: 0.812\n",
      "iteration 41 190\t loss: 2.075, accuracy: 0.812\n",
      "iteration 41 200\t loss: 2.147, accuracy: 0.813\n",
      "iteration 41 210\t loss: 2.309, accuracy: 0.808\n",
      "iteration 41 220\t loss: 2.414, accuracy: 0.814\n",
      "iteration 41 230\t loss: 2.383, accuracy: 0.814\n",
      "iteration 41 240\t loss: 2.106, accuracy: 0.808\n",
      "iteration 41 250\t loss: 2.005, accuracy: 0.813\n",
      "iteration 41 260\t loss: 2.277, accuracy: 0.822\n",
      "iteration 41 270\t loss: 2.079, accuracy: 0.814\n",
      "iteration 41 280\t loss: 2.404, accuracy: 0.814\n",
      "iteration 42 0\t loss: 2.187, accuracy: 0.816\n",
      "iteration 42 10\t loss: 2.412, accuracy: 0.801\n",
      "iteration 42 20\t loss: 2.071, accuracy: 0.814\n",
      "iteration 42 30\t loss: 2.732, accuracy: 0.790\n",
      "iteration 42 40\t loss: 2.315, accuracy: 0.812\n",
      "iteration 42 50\t loss: 2.498, accuracy: 0.806\n",
      "iteration 42 60\t loss: 2.610, accuracy: 0.813\n",
      "iteration 42 70\t loss: 2.301, accuracy: 0.811\n",
      "iteration 42 80\t loss: 2.164, accuracy: 0.819\n",
      "iteration 42 90\t loss: 2.619, accuracy: 0.818\n",
      "iteration 42 100\t loss: 2.176, accuracy: 0.804\n",
      "iteration 42 110\t loss: 2.388, accuracy: 0.811\n",
      "iteration 42 120\t loss: 2.338, accuracy: 0.819\n",
      "iteration 42 130\t loss: 2.489, accuracy: 0.815\n",
      "iteration 42 140\t loss: 2.261, accuracy: 0.800\n",
      "iteration 42 150\t loss: 2.044, accuracy: 0.816\n",
      "iteration 42 160\t loss: 2.051, accuracy: 0.816\n",
      "iteration 42 170\t loss: 1.933, accuracy: 0.816\n",
      "iteration 42 180\t loss: 2.055, accuracy: 0.821\n",
      "iteration 42 190\t loss: 2.104, accuracy: 0.816\n",
      "iteration 42 200\t loss: 2.020, accuracy: 0.814\n",
      "iteration 42 210\t loss: 2.168, accuracy: 0.808\n",
      "iteration 42 220\t loss: 2.357, accuracy: 0.811\n",
      "iteration 42 230\t loss: 2.305, accuracy: 0.819\n",
      "iteration 42 240\t loss: 1.938, accuracy: 0.818\n",
      "iteration 42 250\t loss: 2.104, accuracy: 0.804\n",
      "iteration 42 260\t loss: 2.159, accuracy: 0.824\n",
      "iteration 42 270\t loss: 2.532, accuracy: 0.813\n",
      "iteration 42 280\t loss: 2.234, accuracy: 0.819\n",
      "iteration 43 0\t loss: 2.266, accuracy: 0.814\n",
      "iteration 43 10\t loss: 2.363, accuracy: 0.810\n",
      "iteration 43 20\t loss: 2.251, accuracy: 0.804\n",
      "iteration 43 30\t loss: 2.760, accuracy: 0.801\n",
      "iteration 43 40\t loss: 2.376, accuracy: 0.809\n",
      "iteration 43 50\t loss: 2.572, accuracy: 0.814\n",
      "iteration 43 60\t loss: 2.385, accuracy: 0.823\n",
      "iteration 43 70\t loss: 2.337, accuracy: 0.818\n",
      "iteration 43 80\t loss: 2.344, accuracy: 0.817\n",
      "iteration 43 90\t loss: 2.468, accuracy: 0.823\n",
      "iteration 43 100\t loss: 2.442, accuracy: 0.809\n",
      "iteration 43 110\t loss: 2.340, accuracy: 0.810\n",
      "iteration 43 120\t loss: 2.672, accuracy: 0.816\n",
      "iteration 43 130\t loss: 2.500, accuracy: 0.819\n",
      "iteration 43 140\t loss: 2.162, accuracy: 0.812\n",
      "iteration 43 150\t loss: 2.211, accuracy: 0.808\n",
      "iteration 43 160\t loss: 2.085, accuracy: 0.816\n",
      "iteration 43 170\t loss: 1.892, accuracy: 0.808\n",
      "iteration 43 180\t loss: 2.306, accuracy: 0.818\n",
      "iteration 43 190\t loss: 2.164, accuracy: 0.817\n",
      "iteration 43 200\t loss: 2.265, accuracy: 0.821\n",
      "iteration 43 210\t loss: 2.338, accuracy: 0.813\n",
      "iteration 43 220\t loss: 2.491, accuracy: 0.809\n",
      "iteration 43 230\t loss: 2.463, accuracy: 0.821\n",
      "iteration 43 240\t loss: 2.153, accuracy: 0.823\n",
      "iteration 43 250\t loss: 2.244, accuracy: 0.806\n",
      "iteration 43 260\t loss: 2.204, accuracy: 0.816\n",
      "iteration 43 270\t loss: 2.370, accuracy: 0.811\n",
      "iteration 43 280\t loss: 2.417, accuracy: 0.817\n",
      "iteration 44 0\t loss: 2.078, accuracy: 0.823\n",
      "iteration 44 10\t loss: 2.532, accuracy: 0.795\n",
      "iteration 44 20\t loss: 1.962, accuracy: 0.801\n",
      "iteration 44 30\t loss: 2.493, accuracy: 0.805\n",
      "iteration 44 40\t loss: 2.184, accuracy: 0.824\n",
      "iteration 44 50\t loss: 2.402, accuracy: 0.812\n",
      "iteration 44 60\t loss: 2.438, accuracy: 0.810\n",
      "iteration 44 70\t loss: 2.421, accuracy: 0.820\n",
      "iteration 44 80\t loss: 2.413, accuracy: 0.822\n",
      "iteration 44 90\t loss: 2.630, accuracy: 0.813\n",
      "iteration 44 100\t loss: 2.519, accuracy: 0.812\n",
      "iteration 44 110\t loss: 2.260, accuracy: 0.807\n",
      "iteration 44 120\t loss: 2.771, accuracy: 0.812\n",
      "iteration 44 130\t loss: 2.652, accuracy: 0.818\n",
      "iteration 44 140\t loss: 2.215, accuracy: 0.808\n",
      "iteration 44 150\t loss: 2.165, accuracy: 0.810\n",
      "iteration 44 160\t loss: 2.451, accuracy: 0.809\n",
      "iteration 44 170\t loss: 2.029, accuracy: 0.812\n",
      "iteration 44 180\t loss: 2.164, accuracy: 0.817\n",
      "iteration 44 190\t loss: 2.278, accuracy: 0.824\n",
      "iteration 44 200\t loss: 2.205, accuracy: 0.815\n",
      "iteration 44 210\t loss: 2.224, accuracy: 0.821\n",
      "iteration 44 220\t loss: 2.412, accuracy: 0.821\n",
      "iteration 44 230\t loss: 2.329, accuracy: 0.824\n",
      "iteration 44 240\t loss: 2.253, accuracy: 0.829\n",
      "iteration 44 250\t loss: 2.068, accuracy: 0.811\n",
      "iteration 44 260\t loss: 2.355, accuracy: 0.826\n",
      "iteration 44 270\t loss: 2.432, accuracy: 0.821\n",
      "iteration 44 280\t loss: 2.362, accuracy: 0.823\n",
      "iteration 45 0\t loss: 2.510, accuracy: 0.823\n",
      "iteration 45 10\t loss: 2.594, accuracy: 0.816\n",
      "iteration 45 20\t loss: 2.233, accuracy: 0.801\n",
      "iteration 45 30\t loss: 2.811, accuracy: 0.803\n",
      "iteration 45 40\t loss: 2.458, accuracy: 0.821\n",
      "iteration 45 50\t loss: 2.566, accuracy: 0.814\n",
      "iteration 45 60\t loss: 2.772, accuracy: 0.813\n",
      "iteration 45 70\t loss: 2.642, accuracy: 0.810\n",
      "iteration 45 80\t loss: 2.591, accuracy: 0.823\n",
      "iteration 45 90\t loss: 2.671, accuracy: 0.825\n",
      "iteration 45 100\t loss: 2.462, accuracy: 0.813\n",
      "iteration 45 110\t loss: 2.591, accuracy: 0.800\n",
      "iteration 45 120\t loss: 2.794, accuracy: 0.815\n",
      "iteration 45 130\t loss: 2.778, accuracy: 0.812\n",
      "iteration 45 140\t loss: 2.456, accuracy: 0.811\n",
      "iteration 45 150\t loss: 2.159, accuracy: 0.812\n",
      "iteration 45 160\t loss: 2.538, accuracy: 0.809\n",
      "iteration 45 170\t loss: 2.118, accuracy: 0.808\n",
      "iteration 45 180\t loss: 2.177, accuracy: 0.815\n",
      "iteration 45 190\t loss: 2.483, accuracy: 0.818\n",
      "iteration 45 200\t loss: 2.364, accuracy: 0.813\n",
      "iteration 45 210\t loss: 2.152, accuracy: 0.815\n",
      "iteration 45 220\t loss: 2.559, accuracy: 0.817\n",
      "iteration 45 230\t loss: 2.502, accuracy: 0.818\n",
      "iteration 45 240\t loss: 2.396, accuracy: 0.824\n",
      "iteration 45 250\t loss: 2.178, accuracy: 0.812\n",
      "iteration 45 260\t loss: 2.246, accuracy: 0.825\n",
      "iteration 45 270\t loss: 2.489, accuracy: 0.822\n",
      "iteration 45 280\t loss: 2.561, accuracy: 0.814\n",
      "iteration 46 0\t loss: 2.565, accuracy: 0.824\n",
      "iteration 46 10\t loss: 2.457, accuracy: 0.818\n",
      "iteration 46 20\t loss: 2.153, accuracy: 0.795\n",
      "iteration 46 30\t loss: 2.404, accuracy: 0.823\n",
      "iteration 46 40\t loss: 2.358, accuracy: 0.812\n",
      "iteration 46 50\t loss: 2.778, accuracy: 0.807\n",
      "iteration 46 60\t loss: 2.641, accuracy: 0.814\n",
      "iteration 46 70\t loss: 2.569, accuracy: 0.823\n",
      "iteration 46 80\t loss: 2.413, accuracy: 0.820\n",
      "iteration 46 90\t loss: 2.675, accuracy: 0.825\n",
      "iteration 46 100\t loss: 2.642, accuracy: 0.808\n",
      "iteration 46 110\t loss: 2.871, accuracy: 0.784\n",
      "iteration 46 120\t loss: 2.882, accuracy: 0.820\n",
      "iteration 46 130\t loss: 2.876, accuracy: 0.817\n",
      "iteration 46 140\t loss: 2.413, accuracy: 0.813\n",
      "iteration 46 150\t loss: 2.349, accuracy: 0.826\n",
      "iteration 46 160\t loss: 2.303, accuracy: 0.814\n",
      "iteration 46 170\t loss: 2.262, accuracy: 0.818\n",
      "iteration 46 180\t loss: 2.324, accuracy: 0.824\n",
      "iteration 46 190\t loss: 2.650, accuracy: 0.815\n",
      "iteration 46 200\t loss: 2.420, accuracy: 0.809\n",
      "iteration 46 210\t loss: 2.252, accuracy: 0.818\n",
      "iteration 46 220\t loss: 2.673, accuracy: 0.818\n",
      "iteration 46 230\t loss: 2.510, accuracy: 0.815\n",
      "iteration 46 240\t loss: 2.356, accuracy: 0.817\n",
      "iteration 46 250\t loss: 2.299, accuracy: 0.815\n",
      "iteration 46 260\t loss: 2.327, accuracy: 0.828\n",
      "iteration 46 270\t loss: 2.814, accuracy: 0.806\n",
      "iteration 46 280\t loss: 2.586, accuracy: 0.816\n",
      "iteration 47 0\t loss: 2.465, accuracy: 0.815\n",
      "iteration 47 10\t loss: 2.514, accuracy: 0.815\n",
      "iteration 47 20\t loss: 2.475, accuracy: 0.809\n",
      "iteration 47 30\t loss: 2.517, accuracy: 0.814\n",
      "iteration 47 40\t loss: 2.459, accuracy: 0.824\n",
      "iteration 47 50\t loss: 2.685, accuracy: 0.815\n",
      "iteration 47 60\t loss: 2.540, accuracy: 0.820\n",
      "iteration 47 70\t loss: 2.794, accuracy: 0.824\n",
      "iteration 47 80\t loss: 2.387, accuracy: 0.829\n",
      "iteration 47 90\t loss: 2.480, accuracy: 0.830\n",
      "iteration 47 100\t loss: 2.711, accuracy: 0.814\n",
      "iteration 47 110\t loss: 2.615, accuracy: 0.798\n",
      "iteration 47 120\t loss: 2.861, accuracy: 0.822\n",
      "iteration 47 130\t loss: 2.708, accuracy: 0.819\n",
      "iteration 47 140\t loss: 2.361, accuracy: 0.819\n",
      "iteration 47 150\t loss: 2.285, accuracy: 0.823\n",
      "iteration 47 160\t loss: 2.333, accuracy: 0.815\n",
      "iteration 47 170\t loss: 2.377, accuracy: 0.814\n",
      "iteration 47 180\t loss: 2.374, accuracy: 0.813\n",
      "iteration 47 190\t loss: 2.535, accuracy: 0.820\n",
      "iteration 47 200\t loss: 2.348, accuracy: 0.813\n",
      "iteration 47 210\t loss: 2.374, accuracy: 0.823\n",
      "iteration 47 220\t loss: 2.522, accuracy: 0.831\n",
      "iteration 47 230\t loss: 2.484, accuracy: 0.820\n",
      "iteration 47 240\t loss: 2.457, accuracy: 0.819\n",
      "iteration 47 250\t loss: 2.144, accuracy: 0.810\n",
      "iteration 47 260\t loss: 2.276, accuracy: 0.829\n",
      "iteration 47 270\t loss: 2.581, accuracy: 0.826\n",
      "iteration 47 280\t loss: 2.817, accuracy: 0.813\n",
      "iteration 48 0\t loss: 2.453, accuracy: 0.817\n",
      "iteration 48 10\t loss: 2.582, accuracy: 0.814\n",
      "iteration 48 20\t loss: 2.583, accuracy: 0.805\n",
      "iteration 48 30\t loss: 2.799, accuracy: 0.814\n",
      "iteration 48 40\t loss: 2.571, accuracy: 0.814\n",
      "iteration 48 50\t loss: 2.545, accuracy: 0.817\n",
      "iteration 48 60\t loss: 2.597, accuracy: 0.809\n",
      "iteration 48 70\t loss: 2.898, accuracy: 0.818\n",
      "iteration 48 80\t loss: 2.565, accuracy: 0.819\n",
      "iteration 48 90\t loss: 2.895, accuracy: 0.829\n",
      "iteration 48 100\t loss: 2.825, accuracy: 0.816\n",
      "iteration 48 110\t loss: 2.553, accuracy: 0.799\n",
      "iteration 48 120\t loss: 3.118, accuracy: 0.820\n",
      "iteration 48 130\t loss: 2.904, accuracy: 0.821\n",
      "iteration 48 140\t loss: 2.597, accuracy: 0.813\n",
      "iteration 48 150\t loss: 2.211, accuracy: 0.812\n",
      "iteration 48 160\t loss: 2.453, accuracy: 0.821\n",
      "iteration 48 170\t loss: 2.290, accuracy: 0.821\n",
      "iteration 48 180\t loss: 2.588, accuracy: 0.820\n",
      "iteration 48 190\t loss: 2.470, accuracy: 0.820\n",
      "iteration 48 200\t loss: 2.539, accuracy: 0.808\n",
      "iteration 48 210\t loss: 2.374, accuracy: 0.814\n",
      "iteration 48 220\t loss: 2.420, accuracy: 0.818\n",
      "iteration 48 230\t loss: 2.696, accuracy: 0.825\n",
      "iteration 48 240\t loss: 2.497, accuracy: 0.826\n",
      "iteration 48 250\t loss: 2.509, accuracy: 0.815\n",
      "iteration 48 260\t loss: 2.394, accuracy: 0.828\n",
      "iteration 48 270\t loss: 2.700, accuracy: 0.820\n",
      "iteration 48 280\t loss: 2.624, accuracy: 0.817\n",
      "iteration 49 0\t loss: 2.496, accuracy: 0.816\n",
      "iteration 49 10\t loss: 2.678, accuracy: 0.816\n",
      "iteration 49 20\t loss: 2.634, accuracy: 0.812\n",
      "iteration 49 30\t loss: 2.719, accuracy: 0.813\n",
      "iteration 49 40\t loss: 2.865, accuracy: 0.819\n",
      "iteration 49 50\t loss: 2.752, accuracy: 0.827\n",
      "iteration 49 60\t loss: 2.887, accuracy: 0.822\n",
      "iteration 49 70\t loss: 2.949, accuracy: 0.821\n",
      "iteration 49 80\t loss: 2.700, accuracy: 0.822\n",
      "iteration 49 90\t loss: 2.736, accuracy: 0.827\n",
      "iteration 49 100\t loss: 3.027, accuracy: 0.819\n",
      "iteration 49 110\t loss: 2.667, accuracy: 0.793\n",
      "iteration 49 120\t loss: 3.446, accuracy: 0.813\n",
      "iteration 49 130\t loss: 2.874, accuracy: 0.823\n",
      "iteration 49 140\t loss: 2.722, accuracy: 0.826\n",
      "iteration 49 150\t loss: 2.587, accuracy: 0.817\n",
      "iteration 49 160\t loss: 2.542, accuracy: 0.815\n",
      "iteration 49 170\t loss: 2.482, accuracy: 0.824\n",
      "iteration 49 180\t loss: 2.566, accuracy: 0.826\n",
      "iteration 49 190\t loss: 2.962, accuracy: 0.818\n",
      "iteration 49 200\t loss: 2.812, accuracy: 0.807\n",
      "iteration 49 210\t loss: 2.674, accuracy: 0.817\n",
      "iteration 49 220\t loss: 2.856, accuracy: 0.817\n",
      "iteration 49 230\t loss: 2.995, accuracy: 0.817\n",
      "iteration 49 240\t loss: 2.876, accuracy: 0.822\n",
      "iteration 49 250\t loss: 2.526, accuracy: 0.818\n",
      "iteration 49 260\t loss: 2.817, accuracy: 0.824\n",
      "iteration 49 270\t loss: 2.765, accuracy: 0.829\n",
      "iteration 49 280\t loss: 3.105, accuracy: 0.812\n",
      "iteration 50 0\t loss: 2.620, accuracy: 0.819\n",
      "iteration 50 10\t loss: 2.647, accuracy: 0.825\n",
      "iteration 50 20\t loss: 2.712, accuracy: 0.814\n",
      "iteration 50 30\t loss: 2.699, accuracy: 0.805\n",
      "iteration 50 40\t loss: 2.856, accuracy: 0.824\n",
      "iteration 50 50\t loss: 2.926, accuracy: 0.824\n",
      "iteration 50 60\t loss: 2.825, accuracy: 0.821\n",
      "iteration 50 70\t loss: 2.793, accuracy: 0.823\n",
      "iteration 50 80\t loss: 2.674, accuracy: 0.825\n",
      "iteration 50 90\t loss: 2.621, accuracy: 0.821\n",
      "iteration 50 100\t loss: 2.799, accuracy: 0.827\n",
      "iteration 50 110\t loss: 2.881, accuracy: 0.799\n",
      "iteration 50 120\t loss: 3.325, accuracy: 0.820\n",
      "iteration 50 130\t loss: 2.976, accuracy: 0.816\n",
      "iteration 50 140\t loss: 2.793, accuracy: 0.819\n",
      "iteration 50 150\t loss: 2.553, accuracy: 0.822\n",
      "iteration 50 160\t loss: 2.469, accuracy: 0.819\n",
      "iteration 50 170\t loss: 2.435, accuracy: 0.824\n",
      "iteration 50 180\t loss: 2.667, accuracy: 0.823\n",
      "iteration 50 190\t loss: 2.730, accuracy: 0.818\n",
      "iteration 50 200\t loss: 2.712, accuracy: 0.819\n",
      "iteration 50 210\t loss: 2.536, accuracy: 0.822\n",
      "iteration 50 220\t loss: 2.793, accuracy: 0.822\n",
      "iteration 50 230\t loss: 2.799, accuracy: 0.819\n",
      "iteration 50 240\t loss: 2.690, accuracy: 0.823\n",
      "iteration 50 250\t loss: 2.585, accuracy: 0.815\n",
      "iteration 50 260\t loss: 2.768, accuracy: 0.823\n",
      "iteration 50 270\t loss: 2.959, accuracy: 0.820\n",
      "iteration 50 280\t loss: 2.997, accuracy: 0.820\n",
      "iteration 51 0\t loss: 2.625, accuracy: 0.825\n",
      "iteration 51 10\t loss: 2.872, accuracy: 0.825\n",
      "iteration 51 20\t loss: 2.729, accuracy: 0.811\n",
      "iteration 51 30\t loss: 2.982, accuracy: 0.814\n",
      "iteration 51 40\t loss: 2.592, accuracy: 0.833\n",
      "iteration 51 50\t loss: 2.770, accuracy: 0.827\n",
      "iteration 51 60\t loss: 3.004, accuracy: 0.826\n",
      "iteration 51 70\t loss: 3.065, accuracy: 0.824\n",
      "iteration 51 80\t loss: 2.860, accuracy: 0.822\n",
      "iteration 51 90\t loss: 2.767, accuracy: 0.830\n",
      "iteration 51 100\t loss: 3.229, accuracy: 0.816\n",
      "iteration 51 110\t loss: 2.625, accuracy: 0.812\n",
      "iteration 51 120\t loss: 3.633, accuracy: 0.811\n",
      "iteration 51 130\t loss: 3.509, accuracy: 0.810\n",
      "iteration 51 140\t loss: 2.761, accuracy: 0.813\n",
      "iteration 51 150\t loss: 2.677, accuracy: 0.811\n",
      "iteration 51 160\t loss: 2.523, accuracy: 0.814\n",
      "iteration 51 170\t loss: 2.568, accuracy: 0.812\n",
      "iteration 51 180\t loss: 2.737, accuracy: 0.829\n",
      "iteration 51 190\t loss: 2.892, accuracy: 0.814\n",
      "iteration 51 200\t loss: 2.636, accuracy: 0.809\n",
      "iteration 51 210\t loss: 2.557, accuracy: 0.818\n",
      "iteration 51 220\t loss: 2.914, accuracy: 0.822\n",
      "iteration 51 230\t loss: 2.717, accuracy: 0.823\n",
      "iteration 51 240\t loss: 2.586, accuracy: 0.821\n",
      "iteration 51 250\t loss: 2.528, accuracy: 0.820\n",
      "iteration 51 260\t loss: 2.659, accuracy: 0.827\n",
      "iteration 51 270\t loss: 2.748, accuracy: 0.824\n",
      "iteration 51 280\t loss: 2.874, accuracy: 0.823\n",
      "iteration 52 0\t loss: 2.786, accuracy: 0.814\n",
      "iteration 52 10\t loss: 2.796, accuracy: 0.822\n",
      "iteration 52 20\t loss: 2.582, accuracy: 0.816\n",
      "iteration 52 30\t loss: 3.089, accuracy: 0.793\n",
      "iteration 52 40\t loss: 3.016, accuracy: 0.820\n",
      "iteration 52 50\t loss: 2.783, accuracy: 0.825\n",
      "iteration 52 60\t loss: 3.088, accuracy: 0.822\n",
      "iteration 52 70\t loss: 3.025, accuracy: 0.819\n",
      "iteration 52 80\t loss: 3.200, accuracy: 0.823\n",
      "iteration 52 90\t loss: 3.015, accuracy: 0.829\n",
      "iteration 52 100\t loss: 3.204, accuracy: 0.816\n",
      "iteration 52 110\t loss: 2.933, accuracy: 0.820\n",
      "iteration 52 120\t loss: 3.910, accuracy: 0.805\n",
      "iteration 52 130\t loss: 3.219, accuracy: 0.813\n",
      "iteration 52 140\t loss: 2.873, accuracy: 0.822\n",
      "iteration 52 150\t loss: 2.642, accuracy: 0.822\n",
      "iteration 52 160\t loss: 2.602, accuracy: 0.829\n",
      "iteration 52 170\t loss: 2.639, accuracy: 0.820\n",
      "iteration 52 180\t loss: 2.634, accuracy: 0.813\n",
      "iteration 52 190\t loss: 2.957, accuracy: 0.818\n",
      "iteration 52 200\t loss: 2.946, accuracy: 0.815\n",
      "iteration 52 210\t loss: 2.387, accuracy: 0.806\n",
      "iteration 52 220\t loss: 2.940, accuracy: 0.827\n",
      "iteration 52 230\t loss: 2.977, accuracy: 0.822\n",
      "iteration 52 240\t loss: 2.810, accuracy: 0.819\n",
      "iteration 52 250\t loss: 2.688, accuracy: 0.821\n",
      "iteration 52 260\t loss: 2.830, accuracy: 0.826\n",
      "iteration 52 270\t loss: 2.937, accuracy: 0.828\n",
      "iteration 52 280\t loss: 3.062, accuracy: 0.818\n",
      "iteration 53 0\t loss: 2.988, accuracy: 0.821\n",
      "iteration 53 10\t loss: 2.653, accuracy: 0.815\n",
      "iteration 53 20\t loss: 2.842, accuracy: 0.823\n",
      "iteration 53 30\t loss: 2.868, accuracy: 0.800\n",
      "iteration 53 40\t loss: 2.964, accuracy: 0.826\n",
      "iteration 53 50\t loss: 2.715, accuracy: 0.828\n",
      "iteration 53 60\t loss: 2.884, accuracy: 0.820\n",
      "iteration 53 70\t loss: 3.113, accuracy: 0.809\n",
      "iteration 53 80\t loss: 3.156, accuracy: 0.815\n",
      "iteration 53 90\t loss: 3.181, accuracy: 0.828\n",
      "iteration 53 100\t loss: 3.440, accuracy: 0.816\n",
      "iteration 53 110\t loss: 3.045, accuracy: 0.820\n",
      "iteration 53 120\t loss: 3.203, accuracy: 0.813\n",
      "iteration 53 130\t loss: 3.159, accuracy: 0.826\n",
      "iteration 53 140\t loss: 3.012, accuracy: 0.817\n",
      "iteration 53 150\t loss: 2.860, accuracy: 0.811\n",
      "iteration 53 160\t loss: 2.800, accuracy: 0.827\n",
      "iteration 53 170\t loss: 2.615, accuracy: 0.819\n",
      "iteration 53 180\t loss: 2.872, accuracy: 0.822\n",
      "iteration 53 190\t loss: 2.979, accuracy: 0.822\n",
      "iteration 53 200\t loss: 3.222, accuracy: 0.821\n",
      "iteration 53 210\t loss: 2.901, accuracy: 0.808\n",
      "iteration 53 220\t loss: 3.022, accuracy: 0.823\n",
      "iteration 53 230\t loss: 3.091, accuracy: 0.824\n",
      "iteration 53 240\t loss: 3.000, accuracy: 0.820\n",
      "iteration 53 250\t loss: 2.625, accuracy: 0.822\n",
      "iteration 53 260\t loss: 2.831, accuracy: 0.829\n",
      "iteration 53 270\t loss: 2.837, accuracy: 0.828\n",
      "iteration 53 280\t loss: 3.414, accuracy: 0.808\n",
      "iteration 54 0\t loss: 2.959, accuracy: 0.811\n",
      "iteration 54 10\t loss: 2.740, accuracy: 0.811\n",
      "iteration 54 20\t loss: 3.143, accuracy: 0.819\n",
      "iteration 54 30\t loss: 2.978, accuracy: 0.789\n",
      "iteration 54 40\t loss: 2.982, accuracy: 0.814\n",
      "iteration 54 50\t loss: 2.998, accuracy: 0.823\n",
      "iteration 54 60\t loss: 2.997, accuracy: 0.817\n",
      "iteration 54 70\t loss: 3.087, accuracy: 0.828\n",
      "iteration 54 80\t loss: 3.144, accuracy: 0.823\n",
      "iteration 54 90\t loss: 3.147, accuracy: 0.824\n",
      "iteration 54 100\t loss: 3.816, accuracy: 0.813\n",
      "iteration 54 110\t loss: 3.226, accuracy: 0.820\n",
      "iteration 54 120\t loss: 3.765, accuracy: 0.802\n",
      "iteration 54 130\t loss: 3.411, accuracy: 0.826\n",
      "iteration 54 140\t loss: 3.207, accuracy: 0.819\n",
      "iteration 54 150\t loss: 2.995, accuracy: 0.803\n",
      "iteration 54 160\t loss: 3.028, accuracy: 0.829\n",
      "iteration 54 170\t loss: 2.575, accuracy: 0.811\n",
      "iteration 54 180\t loss: 3.005, accuracy: 0.830\n",
      "iteration 54 190\t loss: 3.243, accuracy: 0.821\n",
      "iteration 54 200\t loss: 3.124, accuracy: 0.817\n",
      "iteration 54 210\t loss: 2.906, accuracy: 0.806\n",
      "iteration 54 220\t loss: 3.358, accuracy: 0.820\n",
      "iteration 54 230\t loss: 3.286, accuracy: 0.826\n",
      "iteration 54 240\t loss: 3.054, accuracy: 0.822\n",
      "iteration 54 250\t loss: 2.691, accuracy: 0.824\n",
      "iteration 54 260\t loss: 2.939, accuracy: 0.833\n",
      "iteration 54 270\t loss: 3.079, accuracy: 0.830\n",
      "iteration 54 280\t loss: 3.316, accuracy: 0.816\n",
      "iteration 55 0\t loss: 2.997, accuracy: 0.820\n",
      "iteration 55 10\t loss: 2.889, accuracy: 0.815\n",
      "iteration 55 20\t loss: 3.069, accuracy: 0.812\n",
      "iteration 55 30\t loss: 3.078, accuracy: 0.790\n",
      "iteration 55 40\t loss: 3.242, accuracy: 0.813\n",
      "iteration 55 50\t loss: 3.162, accuracy: 0.816\n",
      "iteration 55 60\t loss: 2.988, accuracy: 0.809\n",
      "iteration 55 70\t loss: 3.010, accuracy: 0.821\n",
      "iteration 55 80\t loss: 2.950, accuracy: 0.824\n",
      "iteration 55 90\t loss: 3.473, accuracy: 0.826\n",
      "iteration 55 100\t loss: 3.503, accuracy: 0.803\n",
      "iteration 55 110\t loss: 3.034, accuracy: 0.821\n",
      "iteration 55 120\t loss: 4.017, accuracy: 0.800\n",
      "iteration 55 130\t loss: 3.402, accuracy: 0.823\n",
      "iteration 55 140\t loss: 3.225, accuracy: 0.826\n",
      "iteration 55 150\t loss: 3.028, accuracy: 0.806\n",
      "iteration 55 160\t loss: 3.103, accuracy: 0.828\n",
      "iteration 55 170\t loss: 2.818, accuracy: 0.824\n",
      "iteration 55 180\t loss: 2.895, accuracy: 0.824\n",
      "iteration 55 190\t loss: 3.181, accuracy: 0.823\n",
      "iteration 55 200\t loss: 3.227, accuracy: 0.813\n",
      "iteration 55 210\t loss: 3.167, accuracy: 0.807\n",
      "iteration 55 220\t loss: 3.269, accuracy: 0.819\n",
      "iteration 55 230\t loss: 3.489, accuracy: 0.823\n",
      "iteration 55 240\t loss: 3.268, accuracy: 0.821\n",
      "iteration 55 250\t loss: 2.921, accuracy: 0.814\n",
      "iteration 55 260\t loss: 3.005, accuracy: 0.830\n",
      "iteration 55 270\t loss: 3.290, accuracy: 0.824\n",
      "iteration 55 280\t loss: 3.470, accuracy: 0.805\n",
      "iteration 56 0\t loss: 3.349, accuracy: 0.812\n",
      "iteration 56 10\t loss: 2.885, accuracy: 0.813\n",
      "iteration 56 20\t loss: 3.088, accuracy: 0.822\n",
      "iteration 56 30\t loss: 3.250, accuracy: 0.804\n",
      "iteration 56 40\t loss: 3.215, accuracy: 0.825\n",
      "iteration 56 50\t loss: 3.316, accuracy: 0.818\n",
      "iteration 56 60\t loss: 3.112, accuracy: 0.807\n",
      "iteration 56 70\t loss: 3.239, accuracy: 0.825\n",
      "iteration 56 80\t loss: 3.073, accuracy: 0.826\n",
      "iteration 56 90\t loss: 2.976, accuracy: 0.832\n",
      "iteration 56 100\t loss: 3.723, accuracy: 0.804\n",
      "iteration 56 110\t loss: 3.006, accuracy: 0.819\n",
      "iteration 56 120\t loss: 3.859, accuracy: 0.811\n",
      "iteration 56 130\t loss: 3.498, accuracy: 0.831\n",
      "iteration 56 140\t loss: 3.280, accuracy: 0.821\n",
      "iteration 56 150\t loss: 2.920, accuracy: 0.813\n",
      "iteration 56 160\t loss: 2.948, accuracy: 0.826\n",
      "iteration 56 170\t loss: 2.607, accuracy: 0.819\n",
      "iteration 56 180\t loss: 2.898, accuracy: 0.826\n",
      "iteration 56 190\t loss: 3.379, accuracy: 0.820\n",
      "iteration 56 200\t loss: 3.185, accuracy: 0.820\n",
      "iteration 56 210\t loss: 2.816, accuracy: 0.816\n",
      "iteration 56 220\t loss: 3.258, accuracy: 0.816\n",
      "iteration 56 230\t loss: 3.060, accuracy: 0.824\n",
      "iteration 56 240\t loss: 3.076, accuracy: 0.821\n",
      "iteration 56 250\t loss: 3.167, accuracy: 0.803\n",
      "iteration 56 260\t loss: 2.789, accuracy: 0.829\n",
      "iteration 56 270\t loss: 3.018, accuracy: 0.831\n",
      "iteration 56 280\t loss: 3.427, accuracy: 0.820\n",
      "iteration 57 0\t loss: 3.346, accuracy: 0.806\n",
      "iteration 57 10\t loss: 2.910, accuracy: 0.812\n",
      "iteration 57 20\t loss: 3.072, accuracy: 0.807\n",
      "iteration 57 30\t loss: 3.459, accuracy: 0.803\n",
      "iteration 57 40\t loss: 3.149, accuracy: 0.823\n",
      "iteration 57 50\t loss: 3.455, accuracy: 0.819\n",
      "iteration 57 60\t loss: 3.293, accuracy: 0.816\n",
      "iteration 57 70\t loss: 3.302, accuracy: 0.822\n",
      "iteration 57 80\t loss: 3.330, accuracy: 0.821\n",
      "iteration 57 90\t loss: 3.422, accuracy: 0.830\n",
      "iteration 57 100\t loss: 3.410, accuracy: 0.807\n",
      "iteration 57 110\t loss: 3.356, accuracy: 0.821\n",
      "iteration 57 120\t loss: 3.602, accuracy: 0.816\n",
      "iteration 57 130\t loss: 3.758, accuracy: 0.832\n",
      "iteration 57 140\t loss: 3.296, accuracy: 0.821\n",
      "iteration 57 150\t loss: 3.113, accuracy: 0.805\n",
      "iteration 57 160\t loss: 3.232, accuracy: 0.823\n",
      "iteration 57 170\t loss: 2.723, accuracy: 0.820\n",
      "iteration 57 180\t loss: 2.985, accuracy: 0.825\n",
      "iteration 57 190\t loss: 3.192, accuracy: 0.823\n",
      "iteration 57 200\t loss: 3.151, accuracy: 0.818\n",
      "iteration 57 210\t loss: 2.886, accuracy: 0.818\n",
      "iteration 57 220\t loss: 3.528, accuracy: 0.816\n",
      "iteration 57 230\t loss: 3.102, accuracy: 0.827\n",
      "iteration 57 240\t loss: 3.154, accuracy: 0.828\n",
      "iteration 57 250\t loss: 2.995, accuracy: 0.813\n",
      "iteration 57 260\t loss: 3.088, accuracy: 0.836\n",
      "iteration 57 270\t loss: 3.222, accuracy: 0.836\n",
      "iteration 57 280\t loss: 3.125, accuracy: 0.825\n",
      "iteration 58 0\t loss: 3.370, accuracy: 0.818\n",
      "iteration 58 10\t loss: 2.989, accuracy: 0.794\n",
      "iteration 58 20\t loss: 3.322, accuracy: 0.813\n",
      "iteration 58 30\t loss: 3.151, accuracy: 0.805\n",
      "iteration 58 40\t loss: 3.601, accuracy: 0.822\n",
      "iteration 58 50\t loss: 3.402, accuracy: 0.819\n",
      "iteration 58 60\t loss: 3.207, accuracy: 0.817\n",
      "iteration 58 70\t loss: 3.750, accuracy: 0.816\n",
      "iteration 58 80\t loss: 3.019, accuracy: 0.804\n",
      "iteration 58 90\t loss: 2.964, accuracy: 0.822\n",
      "iteration 58 100\t loss: 3.383, accuracy: 0.814\n",
      "iteration 58 110\t loss: 3.060, accuracy: 0.816\n",
      "iteration 58 120\t loss: 3.505, accuracy: 0.812\n",
      "iteration 58 130\t loss: 3.808, accuracy: 0.821\n",
      "iteration 58 140\t loss: 3.164, accuracy: 0.825\n",
      "iteration 58 150\t loss: 3.064, accuracy: 0.814\n",
      "iteration 58 160\t loss: 3.023, accuracy: 0.829\n",
      "iteration 58 170\t loss: 2.896, accuracy: 0.829\n",
      "iteration 58 180\t loss: 2.847, accuracy: 0.828\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e82c7b4e1c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mnew_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_expanded_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_generators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtest_saving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-e82c7b4e1c3c>\u001b[0m in \u001b[0;36mtest_saving\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_saving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_classification_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mnew_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_generators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mcnn_expanded_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_classification_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_expanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mnew_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_expanded_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_generators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e82c7b4e1c3c>\u001b[0m in \u001b[0;36mnew_train_model\u001b[0;34m(model_dict, dataset_generators, epoch_n, print_every, save_model, load_model)\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mtest_feed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0mto_compute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                         \u001b[0mcollect_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_compute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0maverages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Modify this:\n",
    "def new_train_model(model_dict, dataset_generators, epoch_n, print_every,\n",
    "                    save_model=False, load_model=False):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        var_list = [x for x in tf.global_variables()]\n",
    "        saver = tf.train.Saver([i for i in var_list[:2]])\n",
    "        if load_model:\n",
    "            saver.restore(sess,\"/tmp/model.ckpt\")\n",
    "            sess.run(tf.variables_initializer(var_list[2:]))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "\n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('iteration {:d} {:d}\\t loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "        if save_model:\n",
    "            save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "def test_saving():\n",
    "    model_dict = apply_classification_loss(cnn_map)\n",
    "    new_train_model(model_dict, dataset_generators, epoch_n=100, print_every=10, save_model=True)\n",
    "    cnn_expanded_dict = apply_classification_loss(cnn_expanded)\n",
    "    new_train_model(cnn_expanded_dict, dataset_generators, epoch_n=10, print_every=1, load_model=True)\n",
    "    \n",
    "test_saving()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Fine-tuning a Pre-trained Network on CIFAR-10\n",
    "(20 points)\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is another popular benchmark for image classification.\n",
    "We provide you with modified verstion of the file cifar10.py from [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also provide a generator for the CIFAR-10 Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q3.1 Fine-tuning\n",
    "Let's fine-tune SVHN net on **1000 examples** from CIFAR-10. \n",
    "Compare test accuracies of the following scenarios: \n",
    "  - Training `cnn_map` from scratch on the 1000 CIFAR-10 examples\n",
    "  - Fine-tuning SVHN net (`cnn_map` trained on SVHN dataset) on 1000 exampes from CIFAR-10. Use `new_train_model()` defined above to load SVHN net weights, but train on the CIFAR-10 examples.\n",
    "  \n",
    "**Important:** please do not change the `restrict_size=1000` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## train a model from scratch\n",
    "cnn_expanded_dict = apply_classification_loss(cnn_expanded)\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, \n",
    "                print_every=10, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## fine-tuning SVHN Net using Cifar-10 weights saved in Q2\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, \n",
    "                print_every=10, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: TensorBoard\n",
    "(30 points)\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is a very helpful tool for visualization of neural networks. \n",
    "\n",
    "### Q4.1 Plotting\n",
    "Present at least one visualization for each of the following:\n",
    "  - Filters\n",
    "  - Loss\n",
    "  - Accuracy\n",
    "\n",
    "Modify code you have wrote above to also have summary writers. To  run tensorboard, the command is `tensorboard --logdir=path/to/your/log/directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter, loss, and accuracy visualizations\n",
    "def visualize():\n",
    "    raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    ####     ADD YOUR CODE HERE    ####\n",
    "    ###################################    \n",
    "    \n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 5: Bonus\n",
    "(20 points)\n",
    "\n",
    "### Q5.1 SVHN Net ++\n",
    "Improve the accuracy of SVHN Net beyond that of the provided demo: SVHN Net ++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def SVHN_plusplus():\n",
    "    raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    ####     ADD YOUR CODE HERE    ####\n",
    "    ###################################    \n",
    "    \n",
    "SVHN_plusplus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
